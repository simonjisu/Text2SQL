{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlike-algeria",
   "metadata": {},
   "source": [
    "# TEXT2SQL with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enhanced-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "flexible-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-kennedy",
   "metadata": {},
   "source": [
    "## Data to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stunning-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import records\n",
    "\n",
    "schema_re = re.compile(r'\\((.+)\\)')\n",
    "num_re = re.compile(r'[-+]?\\d*\\.\\d+|\\d+')\n",
    "\n",
    "db_path = Path(\"./private\")\n",
    "db = records.Database(f\"sqlite:///{db_path / 'samsung_new.db'}\")\n",
    "\n",
    "table_id = \"receipts\"\n",
    "table_info = db.query('SELECT sql from sqlite_master WHERE tbl_name = :name', name=table_id).all()[0].sql\n",
    "schema_str = schema_re.findall(table_info.replace(\"\\n\", \"\"))[0]\n",
    "schema = {}\n",
    "for tup in schema_str.split(', '):\n",
    "    c, t = tup.split()\n",
    "    schema[c.strip('\"')] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "purple-performance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 'INTEGER',\n",
       " 'rcept_no': 'TEXT',\n",
       " 'reprt_code': 'TEXT',\n",
       " 'bsns_year': 'INTEGER',\n",
       " 'corp_code': 'TEXT',\n",
       " 'stock_code': 'TEXT',\n",
       " 'fs_div': 'TEXT',\n",
       " 'fs_nm': 'TEXT',\n",
       " 'sj_div': 'TEXT',\n",
       " 'sj_nm': 'TEXT',\n",
       " 'account_nm': 'TEXT',\n",
       " 'thstrm_nm': 'TEXT',\n",
       " 'thstrm_dt': 'TEXT',\n",
       " 'thstrm_amount': 'INTEGER',\n",
       " 'frmtrm_nm': 'TEXT',\n",
       " 'frmtrm_dt': 'TEXT',\n",
       " 'frmtrm_amount': 'INTEGER',\n",
       " 'bfefrmtrm_nm': 'TEXT',\n",
       " 'bfefrmtrm_dt': 'TEXT',\n",
       " 'bfefrmtrm_amount': 'INTEGER'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sqlite can have NULL, INTEGER, REAL, TEXT, BLOB data type\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cultural-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INTEGER', 'TEXT'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(schema.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mexican-status",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 제 51 기에 삼성전자의 유동자산은 어떻게 돼? [T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"제 51 기에 삼성전자의 유동자산은 어떻게 돼?\"\n",
    "cls_token = \"[CLS]\"\n",
    "table_token = \"[T]\"\n",
    "column_token = \"[C]\"\n",
    "\n",
    "f\"{cls_token} {question} [T] {table_id} \" + \" \".join([f\"{column_token} {col} [{typ}]\" for col, typ in schema.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-dynamics",
   "metadata": {},
   "source": [
    "## Build a cumstom Tokenizer\n",
    "\n",
    "https://huggingface.co/docs/tokenizers/python/latest/pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-worcester",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "- `normalizers`는 raw text를 더 깨끗하게 만드는 과정이다.\n",
    "- `NFD` 사용하게 되면 한글은 자음 모음으로 분리된다.\n",
    "    - NFD(Normalization Form Canonical Decomposition) = 조합형\n",
    "    - NFC(Normalizaiton Form Canonical Compostion) = 완성형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beginning-private",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제 51 기에 삼성전자의 유동자산은 어떻게 돼?\n",
      "ᄌ/ᅦ/ /5/1/ /ᄀ/ᅵ/ᄋ/ᅦ/ /ᄉ/ᅡ/ᆷ/ᄉ/ᅥ/ᆼ/ᄌ/ᅥ/ᆫ/ᄌ/ᅡ/ᄋ/ᅴ/ /ᄋ/ᅲ/ᄃ/ᅩ/ᆼ/ᄌ/ᅡ/ᄉ/ᅡ/ᆫ/ᄋ/ᅳ/ᆫ/ /ᄋ/ᅥ/ᄄ/ᅥ/ᇂ/ᄀ/ᅦ/ /ᄃ/ᅫ/?\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "# 일반 출력시 합쳐져서 보이지만, for문을 사용하면 분리된다. \n",
    "print(normalizer.normalize_str(question))\n",
    "print(\"/\".join([x for x in normalizer.normalize_str(question)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-arrival",
   "metadata": {},
   "source": [
    "### Pre-Tokenization\n",
    "\n",
    "- `pre_tokenizers`는 텍스트를 더 작은 토큰으로 분리하는 과정이다.\n",
    "- `Whitespace`는 토큰을 공백을 기준으로 나누면 string의 위치를 같이 반환한다. (start + end) - 기준 regex: `\\w+|[^\\w\\s]+`\n",
    "- `Punctuation`은 문장 부호를 각각 분리한다. \n",
    "- `Digits`를 통해 숫자를 각각 분리할지 말지 결정할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stone-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace: \n",
      "[('제', (0, 1)), ('51', (2, 4)), ('기에', (5, 7)), ('삼성전자의', (8, 13)), ('유동자산은', (14, 19)), ('어떻게', (20, 23)), ('돼', (24, 25)), ('?', (25, 26))]\n",
      "Digits individual: False\n",
      "[('제', (0, 1)), ('51', (2, 4)), ('기에', (5, 7)), ('삼성전자의', (8, 13)), ('유동자산은', (14, 19)), ('어떻게', (20, 23)), ('돼', (24, 25)), ('?????', (25, 30))]\n",
      "Punctuation + Individual Digits\n",
      "[('제', (0, 1)), ('5', (2, 3)), ('1', (3, 4)), ('기에', (5, 7)), ('삼성전자의', (8, 13)), ('유동자산은', (14, 19)), ('어떻게', (20, 23)), ('돼', (24, 25)), ('?', (25, 26)), ('?', (26, 27)), ('?', (27, 28)), ('?', (28, 29)), ('?', (29, 30))]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits\n",
    "\n",
    "print(\"Whitespace: \")\n",
    "print(Whitespace().pre_tokenize_str(question))\n",
    "\n",
    "print(\"Digits individual: False\")\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False)])\n",
    "print(pre_tokenizer.pre_tokenize_str(question + \"????\"))\n",
    "\n",
    "print(\"Punctuation + Individual Digits\")\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True)])\n",
    "print(pre_tokenizer.pre_tokenize_str(question + \"????\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-basket",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "- `BPE`: Byte-Pair Encoding 토큰화, 자주 등장하는 character를 합쳐서 표현하는 알고리즘\n",
    "- `Unigram`: 확률적으로 최적의 subword 토큰을 결정\n",
    "- `WordLevel`: 단어 단위의 토큰화\n",
    "- `WordPiece`: Google WordPiece 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "central-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-suggestion",
   "metadata": {},
   "source": [
    "### Post-Processing\n",
    "\n",
    "- `processors` 에서 후처리를 할 수 있다.\n",
    "- `TemplateProcessing`을 이용해 원하는 형태로 토큰을 분리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "choice-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import PostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "great-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [T] $B [C] $B\",\n",
    "    pair=\"[CLS] $A [T] $B:1 [C] $B:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2), (\"[T]\", 3), (\"[C]\", 4), (\"[INTEGER]\", 5), (\"[REAL]\", 6), (\"[TEXT]\", 7), (\"[BLOB]\", 8)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-bouquet",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-image",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/tokenizers/python/latest/index.html\n",
    "- https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=pretrainedtokenizer#transformers.PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nuclear-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제 51 기에 삼성전자의 유동자산은 어떻게 돼? [T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = f\"{question} [T] {table_id} \" + \" \".join([f\"{column_token} {col} [{typ}]\" for col, typ in schema.items()])\n",
    "input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interracial-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_re = re.compile(r\"\\[T\\]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "close-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs = [\"제 51 기에 삼성전자의 유동자산은 어떻게 돼?\", \"2020년도 삼성전자의 유동자산은 얼마?\"]\n",
    "table_str = f\"[T]{table_id}\" + \"\".join([f\"{column_token}{col}[{typ}]\" for col, typ in schema.items()])\n",
    "batch_ts = [table_str] * len(batch_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earned-basement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "new_special_tokens = [\"[T]\", \"[C]\", \"[INTEGER]\", \"[REAL]\", \"[TEXT]\", \"[BLOB]\"]\n",
    "# new_special_tokens = list(map(lambda x: AddedToken(x, single_word=True, normalized=False), new_special_tokens))\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert', add_special_tokens=True, additional_special_tokens=new_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "champion-portuguese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8002, 8003]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_special_tokens_ids = tokenizer.all_special_ids[5:7]\n",
    "new_special_tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "driving-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_input = tokenizer(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    "    return_special_tokens_mask=False, \n",
    ")\n",
    "encode_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "loved-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "table_mask = torch.where(\n",
    "    (encode_input[\"input_ids\"] == 8002), \n",
    "    torch.ones_like(encode_input[\"input_ids\"], dtype=torch.bool), \n",
    "    torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool)\n",
    ")\n",
    "column_mask = torch.where(\n",
    "    (encode_input[\"input_ids\"] == 8003),\n",
    "    torch.ones_like(encode_input[\"input_ids\"], dtype=torch.bool), \n",
    "    torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lyric-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 제 51 기에 삼성전자의 유동자산은 어떻게 돼?[SEP] [T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER] [SEP]\n",
      "\n",
      "[CLS] 2020년도 삼성전자의 유동자산은 얼마?[SEP] [T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER] [SEP][PAD][PAD]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in encode_input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(x))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "banned-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- type_ids = 0\n",
      "[CLS] 제 51 기에 삼성전자의 유동자산은 어떻게 돼?[SEP]\n",
      "--- type_ids = 1\n",
      "[T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER] [SEP]\n",
      "\n",
      "--- type_ids = 0\n",
      "[CLS] 2020년도 삼성전자의 유동자산은 얼마?[SEP]\n",
      "--- type_ids = 1\n",
      "[T] receipts [C] index [INTEGER] [C] rcept_no [TEXT] [C] reprt_code [TEXT] [C] bsns_year [INTEGER] [C] corp_code [TEXT] [C] stock_code [TEXT] [C] fs_div [TEXT] [C] fs_nm [TEXT] [C] sj_div [TEXT] [C] sj_nm [TEXT] [C] account_nm [TEXT] [C] thstrm_nm [TEXT] [C] thstrm_dt [TEXT] [C] thstrm_amount [INTEGER] [C] frmtrm_nm [TEXT] [C] frmtrm_dt [TEXT] [C] frmtrm_amount [INTEGER] [C] bfefrmtrm_nm [TEXT] [C] bfefrmtrm_dt [TEXT] [C] bfefrmtrm_amount [INTEGER] [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inputs, type_ids in zip(encode_input[\"input_ids\"], encode_input[\"token_type_ids\"]):\n",
    "    print(\"--- type_ids = 0\")\n",
    "    print(tokenizer.decode(inputs[type_ids == 0]).replace(\"[PAD]\", \"\"))\n",
    "    print(\"--- type_ids = 1\")\n",
    "    print(tokenizer.decode(inputs[type_ids == 1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lesbian-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8008, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "devoted-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_output = model(**encode_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "joint-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to question tokens and column tokens\n",
    "question_lengths = question_mask.sum(1).tolist()\n",
    "question_tensors = encode_output.last_hidden_state[question_mask, :]\n",
    "batches = torch.split(question_tensors, question_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "departmental-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_questions(batches: Tuple[torch.Tensor], question_lengths: List[int], model: BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "    padded = []\n",
    "    max_length = max(question_lengths)\n",
    "    for x in batches:\n",
    "        if len(x) < max_length:\n",
    "            pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "            padded.append(torch.cat([x, pad_tensor]))\n",
    "        else:\n",
    "            padded.append(x)\n",
    "    return torch.stack(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "proprietary-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded = pad_questions(batches, question_lengths, model, pad_idx=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-idaho",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-terry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-manual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-southeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-minute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-directive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "exceptional-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 247, 768])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "respective-technician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_output.pooler_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-animal",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "derived-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "                     \"vocab_txt\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = u'▁'\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "    \"\"\"\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            vocab_txt,\n",
    "            do_lower_case=False,\n",
    "            remove_space=True,\n",
    "            keep_accents=False,\n",
    "            unk_token=\"[UNK]\",\n",
    "            sep_token=\"[SEP]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            cls_token=\"[CLS]\",\n",
    "            mask_token=\"[MASK]\",\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize('NFKD', outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index, return_unicode=True):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A KoBERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A KoBERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-chrome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
