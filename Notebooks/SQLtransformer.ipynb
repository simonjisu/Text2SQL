{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6b9a71",
   "metadata": {},
   "source": [
    "# TEXT2SQL with transformers\n",
    "\n",
    "Lee Woo Chul, Jang Ji Soo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f458d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d111096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os\n",
    "\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e4531",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "`NLSQL.jsonl` and `\"table.jsonl` contains the data like following format same with [WikiSQL](https://github.com/salesforce/WikiSQL), Please follow the [link](https://github.com/salesforce/WikiSQL#content-and-format) to see what are the keys mean.\n",
    "\n",
    "```json\n",
    "// example of 'NLSQL.jsonl'\n",
    "{\n",
    "    \"phase\": 1, \n",
    "    \"question\": \"2015 삼성전자 유동자산은 어떻게 돼?\", \n",
    "    \"table_id\": \"receipts\", \n",
    "    \"sql\": {\n",
    "        \"sel\": 16, \n",
    "        \"agg\": 0, \n",
    "        \"conds\": [[10, 0, \"유동자산\"], [3, 0, 2016]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e69b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sql_path, table_path):\n",
    "    path_sql = Path(sql_path)\n",
    "    path_table = Path(table_path)\n",
    "\n",
    "    dataset = []\n",
    "    table = {}\n",
    "    with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            dataset.append(x)\n",
    "\n",
    "    with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            table[x['id']] = x\n",
    "            \n",
    "    return dataset, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26136786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, table = load_data(\"NLSQL.jsonl\", \"table.jsonl\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    dataset=data,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: x # now dictionary values are not merged!\n",
    ")\n",
    "# Load DBEngine\n",
    "db_path = Path(\"./private\")\n",
    "dbengine = DBEngine(db_path / \"samsung_new.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0ab588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96a06348504490b808c894d16ad550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test with toy data:   0%|          | 0/21120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch_data in enumerate(tqdm(data_loader, desc=\"Test with toy data\")):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0d0ac",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Used BERT in hugging Face with KoBERT\n",
    "\n",
    "- https://github.com/SKTBrain/KoBERT\n",
    "- https://github.com/monologg/KoBERT-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "af6c714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "def get_bert(model_path: str, output_hidden_states: bool=False):\n",
    "    special_tokens = [\"[S]\", \"[E]\", \"[COL]\"] # sequence start, sequence end, column tokens\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    config = BertConfig.from_pretrained(model_path)\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc748d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"monologg/kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "model_bert, tokenizer_bert, config_bert = get_bert(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20e5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data, dbengine):\n",
    "    batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "    tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "    batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "    batch_ts = []\n",
    "    for table_id in tid:\n",
    "        dbengine.get_schema_info(table_id)\n",
    "        table_str = f\"{table_id}\" + \"\".join([\n",
    "            f\"[COL]{col}\" for col in dbengine.schema\n",
    "        ]) \n",
    "        batch_ts.append(table_str)\n",
    "    \n",
    "    return batch_qs, batch_sqls, batch_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs, batch_sqls, batch_ts = get_batch_data(batch_data, dbengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7b85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Input\n",
    "encode_input = tokenizer_bert(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    "    return_special_tokens_mask=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccc5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 삼성전자의 2015년도 영업이익이 몇이야?[SEP] receipts [COL] index [COL] rcept_no [COL] reprt_code [COL] bsns_year [COL] corp_code [COL] stock_code [COL] fs_div [COL] fs_nm [COL] sj_div [COL] sj_nm [COL] account_nm [COL] thstrm_nm [COL] thstrm_dt [COL] thstrm_amount [COL] frmtrm_nm [COL] frmtrm_dt [COL] frmtrm_amount [COL] bfefrmtrm_nm [COL] bfefrmtrm_dt [COL] bfefrmtrm_amount[SEP]\n"
     ]
    }
   ],
   "source": [
    "# Show an Example of Input\n",
    "print(tokenizer_bert.decode(encode_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0853d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoBertTokenizer.KoBertTokenizer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3b756",
   "metadata": {},
   "source": [
    "## Prepare for decoder Inputs: Createing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7e5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_input_mask(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    r\"\"\"\n",
    "    input should only contains word tokens:\n",
    "    \"\"\"\n",
    "    start_tkn_mask = input_ids == start_tkn_id\n",
    "    end_tkn_mask = input_ids == end_tkn_id\n",
    "    start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "    return mask.scatter(1, index, False)\n",
    "\n",
    "def get_input_mask_and_answer(encode_input, tokenizer):\n",
    "    r\"\"\"\n",
    "    table -> database table name(id)\n",
    "    header -> database header\n",
    "    \n",
    "    returns:\n",
    "        input_question_mask, input_table_mask, input_header_mask, answer_table_tkns, answer_header_tkns\n",
    "    \"\"\"\n",
    "    batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "    sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "    start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "    \n",
    "    input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "    input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "    input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "    db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "    db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "    col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "    db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "    # split table_mask and header_mask\n",
    "    input_idx = torch.arange(max_length).repeat(batch_size, 1)\n",
    "    db_idx = input_idx[db_mask]\n",
    "    table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "    table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "    start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "    table_end_sep_idx = start_idx[:, 0] - 1\n",
    "    split_size = torch.stack([\n",
    "        table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "    ]).transpose(0, 1)\n",
    "\n",
    "    # Token idx\n",
    "    table_tkn_idx, header_tkn_idx = map(\n",
    "        lambda x: torch.stack(x), \n",
    "        zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "    )\n",
    "\n",
    "    table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "    # Mask include [S] & [E] tokens\n",
    "    table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, table_tkn_idx, True)\n",
    "    header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "    # For Decoder Input, Maskout [S], [E] for table & header  \n",
    "    input_table_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    input_header_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    # [COL] token mask: this is for attention\n",
    "    col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "    input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "    return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41090417",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_mask, input_table_mask, input_header_mask, input_col_mask = get_input_mask_and_answer(encode_input, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6585265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tokens for Decoder\n",
      "-------------------------\n",
      "삼성전자의 2015년도 영업이익이 몇이야? 삼성전자 2019년의 매출액은 몇이야?\n",
      "\n",
      "Table Tokens for Decoder\n",
      "-------------------------\n",
      "receipts receipts\n",
      "\n",
      "Header Tokens for Decoder\n",
      "-------------------------\n",
      "index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount\n",
      "\n",
      "Column(Index of Headers) Tokens for Decoder\n",
      "-------------------------\n",
      "[COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, t in zip(\n",
    "        [input_question_mask, input_table_mask, input_header_mask, input_col_mask], \n",
    "        [\"Question Tokens for Decoder\", \"Table Tokens for Decoder\", \"Header Tokens for Decoder\", \"Column(Index of Headers) Tokens for Decoder\"]\n",
    "    ):\n",
    "    print(t)\n",
    "    print(\"-----\"*5)\n",
    "    print(tokenizer_bert.decode(encode_input[\"input_ids\"][m]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2353e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed to BERT Model\n",
    "encode_outputs = model_bert(**encode_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27bfc6",
   "metadata": {},
   "source": [
    "\n",
    "The `encode_outputs` will be selected by 4 types of masks\n",
    "```\n",
    "encode_outputs\n",
    "-> Question\n",
    "-> Table\n",
    "-> Header\n",
    "-> Column(Index of Headers)\n",
    "```\n",
    "\n",
    "And pad batches which has less tokens than max length with \"\\[PAD\\]\"  for Decoder Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44e35b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batches: Tuple[torch.Tensor], lengths: List[int], model: BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "    padded = []\n",
    "    max_length = max(lengths)\n",
    "    for x in batches:\n",
    "        if len(x) < max_length:\n",
    "            pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "            padded.append(torch.cat([x, pad_tensor]))\n",
    "        else:\n",
    "            padded.append(x)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "def get_decoder_batches(encode_output, mask, model, pad_idx):\n",
    "    lengths = mask.sum(1)\n",
    "    tensors = encode_output.last_hidden_state[mask, :]\n",
    "    batches = torch.split(tensors, lengths.tolist())\n",
    "    if lengths.ne(lengths.max()).sum().item() != 0:\n",
    "        # pad not same length tokens\n",
    "        tensors_padded = pad(batches, lengths.tolist(), model, pad_idx=pad_idx)\n",
    "    else:\n",
    "        # just stack the splitted tensors\n",
    "        tensors_padded = torch.stack(batches)\n",
    "    return tensors_padded, lengths.tolist()\n",
    "\n",
    "# def get_pad_mask(lengths):\n",
    "#     batch_size = len(lengths)\n",
    "#     max_len = max(lengths)\n",
    "#     mask = torch.ones(batch_size, max_len)\n",
    "#     for i, l in enumerate(lengths):\n",
    "#         mask[i, :l] = 0\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "660639b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded, question_lengths = get_decoder_batches(encode_outputs, input_question_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "table_padded, table_lengths = get_decoder_batches(encode_outputs, input_table_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "header_padded, header_lengths = get_decoder_batches(encode_outputs, input_header_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "col_padded, col_lengths = get_decoder_batches(encode_outputs, input_col_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf7dcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65547005",
   "metadata": {},
   "source": [
    "## Create the Answers for decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e342dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_answers(batch_sqls, tokenizer, end_tkn_idx=1):\n",
    "    \"\"\"\n",
    "    for backward compatibility, separated with get_g\n",
    "    \n",
    "    sc: select column\n",
    "    sa: select agg\n",
    "    wn: where number\n",
    "    wc: where column\n",
    "    wo: where operator\n",
    "    wv: where value\n",
    "    \"\"\"\n",
    "\n",
    "    get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, sql_dict in enumerate(batch_sqls):\n",
    "        g_sc.append( sql_dict[\"sel\"] )\n",
    "        g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "        conds = sql_dict[\"conds\"]\n",
    "        if not sql_dict[\"agg\"] < 0:\n",
    "            g_wn.append( len(conds) )\n",
    "            g_wc.append( get_ith_element(conds, 0) )\n",
    "            g_wo.append( get_ith_element(conds, 1) )\n",
    "            g_wv.append( get_ith_element(conds, 2) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    \n",
    "    # get where value tokenized \n",
    "    end_tkn = tokenizer.additional_special_tokens[end_tkn_idx]\n",
    "    pad_tkn_id = tokenizer.pad_token_id\n",
    "    g_wv_tkns = [[f\"{s}{end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "    g_wv_tkns = [tokenizer(batch_wv, add_special_tokens=False)[\"input_ids\"] for batch_wv in g_wv_tkns]\n",
    "    # add empty list if batch has different where column number\n",
    "    max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "    g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "    temp = []\n",
    "    for batch_wv in list(zip(*g_wv_tkns)):\n",
    "        batch_max_len = max(map(len, batch_wv))\n",
    "        batch_temp = []\n",
    "        for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "            if len(wv_tkns) < batch_max_len:\n",
    "                batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "            else:\n",
    "                batch_temp.append(wv_tkns)\n",
    "        temp.append(batch_temp)\n",
    "    g_wv_tkns = list(zip(*temp))\n",
    "\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5db5b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([13, 16],\n",
       " [0, 0],\n",
       " [2, 2],\n",
       " [[10, 3], [10, 3]],\n",
       " [[0, 0], [0, 0]],\n",
       " [['영업이익', 2015], ['매출액', 2020]],\n",
       " [([3383, 8003], [576, 8003, 1]), ([2002, 8003], [554, 127, 8003])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert, 1)\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5be36b",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Similar structure in SQLova but a little difference in here.\n",
    "\n",
    "- SQLova is a neural semantic parser translating natural language utterance to SQL query.\n",
    "- Official Github: [https://github.com/naver/sqlova](https://github.com/naver/sqlova)\n",
    "- Paper: [A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization](https://arxiv.org/abs/1902.01069)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PW9oAXfW-ZI-jxGn5q9O_gzUIZnNYaet\" alt=\"Sqlova Decoder Architecture \" width=\"50%\" height=\"auto\">\n",
    "\n",
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a1fdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def wipe_out_pad_tkn_score(self, score, lengths, dim=2):\n",
    "        max_len = max(lengths)\n",
    "        for batch_idx, length in enumerate(lengths):\n",
    "            if length < max_len:\n",
    "                if dim == 2:\n",
    "                    score[batch_idx, :, length:] = -10000000\n",
    "                elif dim == 1:\n",
    "                    score[batch_idx, length:, :] = 0.0\n",
    "                else:\n",
    "                    raise ValueError(f\"`dim` in wipe_out_pad_tkn_score should be 1 or 2\")\n",
    "        return score \n",
    "\n",
    "\n",
    "class C2QAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Column to Question Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, o_c, o_q, q_lengths, c_lengths=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each column tokens, How much related to question tokens?\n",
    "        \n",
    "        o_c: LSTM output of column\n",
    "        o_q: LSTM output of question \n",
    "        \n",
    "        c_lengths: wipe out row length\n",
    "        return context atttended to question tokens\n",
    "        \"\"\"\n",
    "        sqrt_H = torch.sqrt(torch.FloatTensor([o_c.size(-1)], device=o_c.device))  # Apply Attention is All you Need Technique\n",
    "        o_q_transform = self.linear(o_q)  # (B, T_q, H)\n",
    "        score_c2q = torch.bmm(o_c, o_q_transform.transpose(1, 2)) / sqrt_H  # (B, T_c, H) x (B, H, T_q) = (B, T_c, T_q)\n",
    "        score_c2q = self.wipe_out_pad_tkn_score(score_c2q, q_lengths, dim=2)\n",
    "        \n",
    "        prob_c2q = self.softmax(score_c2q)\n",
    "        if c_lengths is not None:\n",
    "            prob_c2q = self.wipe_out_pad_tkn_score(prob_c2q, c_lengths, dim=1)\n",
    "        # prob_c2q: (B, T_c, T_q) -> (B, T_c, T_q, 1)\n",
    "        # o_q: (B, 1, T_q, H)\n",
    "        # p_col2question \\odot o_q = (B, T_c, T_q, 1) \\odot (B, 1, T_q, H) = (B, T_c, T_q, H)\n",
    "        # -> reduce sum to T_q to get context for each column (B, T_c, H)\n",
    "        context = torch.mul(prob_c2q.unsqueeze(3), o_q.unsqueeze(1)).sum(dim=2)\n",
    "        if rt_attn:\n",
    "            attn = prob_c2q\n",
    "        else:\n",
    "            attn = None\n",
    "        return context, attn\n",
    "\n",
    "class SelfAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Self Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, o, lengths, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each o tokens, How much related to o tokens?\n",
    "        \n",
    "        return attended summary of o\n",
    "        \"\"\"\n",
    "        o_transform = self.linear(o)  # (B, T_o, H) -> (B, T_o, 1)\n",
    "        o_transform = self.wipe_out_pad_tkn_score(o_transform, lengths) \n",
    "        o_prob = self.softmax(o_transform)  # (B, T_o, 1)\n",
    "        \n",
    "        o_summary = torch.mul(o, o_prob).sum(1)  # (B, T_o, H) \\odot (B, T_o, 1) -> (B, H)\n",
    "\n",
    "        if rt_attn:\n",
    "            attn = o_prob\n",
    "        else:\n",
    "            attn = None\n",
    "        return o_summary, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46721b5a",
   "metadata": {},
   "source": [
    "## Decoder Sub Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9eeaaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectDecoder(nn.Module):\n",
    "    r\"\"\"SELECT Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "\n",
    "class AggDecoder(nn.Module):\n",
    "    r\"\"\"AGG Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], col_lengths: List[int], select_idxes: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        \n",
    "        col_selected = col_context[list(range(batch_size)), select_idxes].unsqueeze(1)  # col_selected: (B, 1, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_selected, o_q, question_lengths, col_lengths, rt_attn)  # (B, 1, H), (B, 1, T_q)\n",
    "        output = self.output_layer(col_q_context.squeeze(1))\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    \n",
    "class WhereNumDecoder(nn.Module):\n",
    "    r\"\"\"WHERE number Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        if self.output_size > self.max_where_conds+1:\n",
    "            # HERE output will be dilivered to cross-entropy loss, not guessing the real number of where clause\n",
    "            raise ValueError(f\"`WhereNumDecoder` only support maximum {max_where_conds} where clause\")\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_self_attn = SelfAttention(2*hidden_size, 1)\n",
    "        self.lstm_q_hidden_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.lstm_q_cell_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        self.context_self_attn = SelfAttention(hidden_size, 1)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "\n",
    "        col_self_attn, col_attn = self.col_self_attn(col_context, col_lengths, rt_attn)  # (B, 2H), (B, T_c)\n",
    "\n",
    "        h_0 = self.lstm_q_hidden_init_linear(col_self_attn)  # (B, 2H)\n",
    "        h_0 = h_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        c_0 = self.lstm_q_cell_init_linear(col_self_attn)  # (B, 2H)\n",
    "        c_0 = c_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded, (h_0, c_0))  # o_q: (B, T_q, H)\n",
    "        o_summary, o_attn = self.context_self_attn(o_q, question_lengths, rt_attn)  # (B, H), (B, T_q)\n",
    "        output = self.output_layer(o_summary)\n",
    "        \n",
    "        return output, (col_attn, o_attn)\n",
    "\n",
    "    \n",
    "class WhereColumnDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Column Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int=1, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "\n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "    \n",
    "class WhereOpDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Opperator Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is settled at WhereColumnDecoder, but it can be lower than or equal to `max_where_conds`\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context_padded], dim=2)  # (B, max_where_col_nums, 2H)\n",
    "        output = self.output_layer(vec)  # (B, max_where_col_nums, n_cond_ops)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        return output\n",
    "        \n",
    "    def get_context_padded(self, col_context, where_nums, where_col_idxes):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums) \n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size, device=col_context.device)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    \n",
    "class WhereValueDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Value Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4, n_cond_ops: int=4,\n",
    "                 start_tkn_id=8002, end_tkn_id=8003, embedding_layer=None) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        \n",
    "        self.start_tkn_id = start_tkn_id\n",
    "        self.end_tkn_id = end_tkn_id\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.where_op_linear = nn.Linear(n_cond_ops, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        if embedding_layer is None:\n",
    "            raise KeyError(\"Must initialize the embedding_layer to BertModel's word embedding layer\")\n",
    "        else:\n",
    "            if not isinstance(embedding_layer, torch.nn.modules.sparse.Embedding):\n",
    "                embedding_layer = embedding_layer.word_embeddings\n",
    "            self.embedding_layer = embedding_layer\n",
    "            vocab_size, bert_hidden_size = embedding_layer.weight.data.size()\n",
    "            self.output_lstm_hidden_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm_cell_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm = nn.LSTM(bert_hidden_size, bert_hidden_size, 1, batch_first=True)\n",
    "            self.output_linear = nn.Linear(bert_hidden_size, vocab_size)\n",
    "            self.output_linear.weight.data = embedding_layer.weight.data\n",
    "\n",
    "        \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], where_op_idxes: List[List[int]], value_tkn_max_len=None, g_wv_tkns=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is setted at WhereColumnDecoder\n",
    "        value_tkn_max_len = Train if None else Test\n",
    "        g_wv_tkns = When Train should not be None\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        where_op_one_hot_padded = self.get_where_op_one_hot_padded(where_op_idxes, where_nums, where_col_idxes, n_cond_ops=self.n_cond_ops)#.to(o_q.device)  # (B, max_where_col_nums, n_cond_ops)\n",
    "        where_op = self.where_op_linear(where_op_one_hot_padded)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        vec = torch.cat([col_q_context, col_context_padded, where_op], dim=2)  # (B, max_where_col_nums, 3H)\n",
    "        max_where_col_nums = vec.size(1)\n",
    "        # predict each where_col\n",
    "        total_scores = []\n",
    "        for i in range(max_where_col_nums):\n",
    "            g_wv_tkns_i = torch.LongTensor([g_wv_tkns[b_idx][i] for b_idx in range(batch_size)]) if g_wv_tkns is not None else None  # (B, T_d_i)\n",
    "            vec_i = vec[:, i, :]  # (B, 3H)\n",
    "            \n",
    "            h_0 = self.output_lstm_hidden_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            c_0 = self.output_lstm_cell_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            \n",
    "            scores = self.decode_single_where_col(batch_size, h_0, c_0, value_tkn_max_len=value_tkn_max_len, g_wv_tkns_i=g_wv_tkns_i)  # (B, T_d_i, vocab_size)\n",
    "            total_scores.append(scores)\n",
    "        \n",
    "        # total_scores: [(B, T_d_i, vocab_size)] x max_where_col_nums\n",
    "        return total_scores\n",
    "    \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.start_tkn_id]*batch_size).unsqueeze(1)  # (B, 1)\n",
    "        return sos\n",
    "    \n",
    "    def decode_single_where_col(self, batch_size, h_0, c_0, value_tkn_max_len=None, g_wv_tkns_i=None):\n",
    "        if value_tkn_max_len is None:\n",
    "            # [Training] set the max length to gold token max length (already padded)\n",
    "            max_len = len(g_wv_tkns_i[0])\n",
    "        else:\n",
    "            # [Testing]  don't know the max length\n",
    "            max_len = value_tkn_max_len\n",
    "            \n",
    "        sos = self.start_token(batch_size)  # (B, 1)\n",
    "        emb = self.embedding_layer(sos)  # (B, 1, bert_H)\n",
    "        scores = [] \n",
    "        for i in range(max_len):\n",
    "            o, (h, c) = self.output_lstm(emb, (h_0, c_0))  # h: (1, B, bert_H)  \n",
    "            s = self.output_linear(h[-1, :]) # select last layer if use multiple rnn layers, h: (1, B, bert_H) -> (B, bert_H) -> s: (B, vocab_size)\n",
    "            scores.append(s)\n",
    "            \n",
    "            if g_wv_tkns_i is not None:\n",
    "                # [Training] Teacher Force model\n",
    "                pred = g_wv_tkns_i[:, i]  # (B, )\n",
    "            else:\n",
    "                # [Testing]\n",
    "                pred = s.argmax(1)  # (B, )\n",
    "                if (pred == self.end_tkn_id).sum() == batch_size:\n",
    "                    break\n",
    "                    \n",
    "            emb = self.embedding_layer(pred.unsqueeze(1))  # (B, 1, bert_H)\n",
    "        \n",
    "        return torch.stack(scores).transpose(0, 1).contiguous() # (T_d_i, B, vocab_size) -> (B, T_d_i, vocab_size)\n",
    "        \n",
    "    def get_context_padded(self, col_context: torch.Tensor, where_nums: List[int], where_col_idxes: List[List[int]]):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    def get_where_op_one_hot_padded(self, where_op_idxes: List[List[int]], where_nums: List[int], where_col_idxes: List[List[int]], n_cond_ops: int):\n",
    "        r\"\"\"\n",
    "        Turn where operation indexs into one hot encoded vectors\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [torch.zeros(where_num, n_cond_ops).scatter(1, torch.LongTensor(batch_col).unsqueeze(1), 1) for where_num, batch_col in zip(where_nums, where_op_idxes)]  \n",
    "        # batches = [(where_col_nums, n_cond_ops), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), n_cond_ops)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc868d9",
   "metadata": {},
   "source": [
    "## Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8d4e8",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e297ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_bert.hidden_size\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "dropout_ratio = 0.3\n",
    "max_where_conds = 4\n",
    "n_agg_ops = len(dbengine.agg_ops)\n",
    "n_cond_ops = len(dbengine.cond_ops)\n",
    "start_tkn_id = tokenizer_bert.additional_special_tokens_ids[0]\n",
    "end_tkn_id = tokenizer_bert.additional_special_tokens_ids[1]\n",
    "embedding_layer = model_bert.embeddings.word_embeddings\n",
    "train = True\n",
    "if train:\n",
    "    value_tkn_max_len = None\n",
    "else:\n",
    "    value_tkn_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0ac9a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.select_decoder = SelectDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.agg_decoder = AggDecoder(\n",
    "            input_size, hidden_size, output_size=n_agg_ops, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_num_decoder = WhereNumDecoder(\n",
    "            input_size, hidden_size, output_size=(max_where_conds+1), num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_col_decoder = WhereColumnDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_op_decoder = WhereOpDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_value_decoder = WhereValueDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds, \n",
    "            n_cond_ops=n_cond_ops, start_tkn_id=start_tkn_id, end_tkn_id=end_tkn_id, embedding_layer=embedding_layer\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=None, gold=None):\n",
    "        if gold is None:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = [None] * 6\n",
    "        else:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "        decoder_outputs = {}\n",
    "\n",
    "        select_outputs, _ = self.select_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        select_idxes = g_sc if g_sc else self.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "        agg_outputs, _ = self.agg_decoder(question_padded, col_padded, question_lengths, col_lengths, select_idxes)\n",
    "\n",
    "        where_num_outputs, _  = self.where_num_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_nums = g_wn if g_wn else self.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "        where_col_outputs, _ = self.where_col_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_col_idxes = g_wc if g_wc else self.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_op_outputs = self.where_op_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes)\n",
    "        where_op_idxes = g_wo if g_wo else self.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_value_outputs = self.where_value_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns)\n",
    "\n",
    "        decoder_outputs = {\n",
    "            \"sc\": select_outputs,  # cross entropy\n",
    "            \"sa\": agg_outputs,  # cross entropy\n",
    "            \"wn\": where_num_outputs,  # cross entropy\n",
    "            \"wc\": where_col_outputs,  # binary cross entropy\n",
    "            \"wo\": where_op_outputs,  # cross entropy\n",
    "            \"wv\": where_value_outputs  # cross entropy\n",
    "        }\n",
    "        \n",
    "        return decoder_outputs\n",
    "        \n",
    "    def predict_decoder(self, typ, **kwargs):\n",
    "        r\"\"\"\n",
    "        if not using teacher force model will use this function to predict answer\n",
    "        \"\"\"\n",
    "        if typ == \"sc\":  # SELECT column\n",
    "            select_outputs = kwargs[\"select_outputs\"]\n",
    "            return select_outputs.argmax(1).tolist()\n",
    "        elif typ == \"sa\":  # SELECT aggregation operator\n",
    "            # not need actually\n",
    "            agg_outputs = kwargs[\"agg_outputs\"]\n",
    "            return agg_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wn\":  # WHERE number\n",
    "            where_num_outputs = kwargs[\"where_num_outputs\"]\n",
    "            return where_num_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wc\":  # WHERE clause column\n",
    "            where_col_outputs = kwargs[\"where_col_outputs\"]\n",
    "            where_col_argsort = torch.sigmoid(where_col_outputs).argsort(1)\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_col_idxes = [where_col_argsort[b_idx, :w_num].tolist() for b_idx, w_num in enumerate(where_nums)]\n",
    "            return where_col_idxes\n",
    "        elif typ == \"wo\":  # WHERE clause operator\n",
    "            where_op_outputs = kwargs[\"where_op_outputs\"]\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_op_idxes = []\n",
    "            for b_idx, w_num in enumerate(where_nums):\n",
    "                if w_num == 0:  # means no where number\n",
    "                    where_op_idxes.append([])\n",
    "                else:\n",
    "                    where_op_idxes.append(where_op_outputs.argmax(2)[b_idx, :w_num].tolist())\n",
    "            return where_op_idxes\n",
    "        elif typ == \"wv\":  # WHERE clause value\n",
    "            # not need actually\n",
    "            where_value_outputs = kwargs[\"where_value_outputs\"]\n",
    "            return [o.argmax(2).tolist() for o in where_value_outputs]\n",
    "        else:\n",
    "            raise KeyError(\"`typ` must be in ['sc', 'sa', 'wn', 'wc', 'wo', 'wv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "701ac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = {}\n",
    "predicts[\"sc\"] = model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "predicts[\"sa\"] = model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "predicts[\"wn\"] = model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "predicts[\"wc\"] = model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "predicts[\"wo\"] = model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "predicts[\"wv\"] = model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ab78b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1b5c55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decoder = Decoder(input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "554acd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([13, 16],\n",
       " [0, 0],\n",
       " [3, 1],\n",
       " [[10, 3, 3], [10]],\n",
       " [[0, 0, 0], [0]],\n",
       " [['유동부채', 2018, 2018], ['유동자산']],\n",
       " [([3574, 5872, 6398, 7405, 8003], [554, 115, 8003], [554, 115, 8003]),\n",
       "  ([3574, 5872, 7162, 8003, 1], [1, 1, 1], [1, 1, 1])])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\n",
    "    {'sel': 13, 'agg': 0, 'conds': [[10, 0, '유동부채'], [3, 0, 2018], [3, 0, 2018]]},\n",
    "    {'sel': 16, 'agg': 0, 'conds': [[10, 0, '유동자산']]}\n",
    "]\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(x, tokenizer_bert, 1)\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN\n",
    "NL + TABLE -> 6\n",
    "WHERE 개수 -> WHERE C, O, V\n",
    "3 X, 2 O  -> 2\n",
    "\n",
    "TEST (TEST 정답)\n",
    "NL + TABLE -> 6 \n",
    "WHERE 개수 -> WHERE C, O, V\n",
    "모델 예측: 3 -> 3\n",
    "테스트 정답: 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 예측: [3, 1] / [0, 0] / [3574, 5872, 6398, 7405, 8003], [34, 55, 8003]\n",
    "훈련 정답:  [10, 3] / [0, 0] / [3574, 5872, 6398, 7405, 8003], [554, 115, 8003],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "예측하는건 최대 4개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a46ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 예측: [4, 4] / [10, 3, 3, 4] / [0, 0, 0, 5] / [3574, 5872, 6398, 7405, 8003], [554, 115, 8003], [554, 115, 8003]\n",
    "테스트 정답: [2, 2] / [10, 3] / [0, 0] / [3574, 5872, 6398, 7405, 8003], [554, 115, 8003],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b74b0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = (\n",
    "    [13, 16],\n",
    "    [0, 0],\n",
    "    [3, 0],\n",
    "    [[10, 3, 3], []],\n",
    "    [[0, 0, 0], []],\n",
    "    [['유동부채', 2018, 2018], []],\n",
    "    [\n",
    "        ([3574, 5872, 6398, 7405, 8003], [554, 115, 8003], [554, 115, 8003]),\n",
    "        ([])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2e58fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = (\n",
    "    [13, 16],\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [[], []],\n",
    "    [[], []],\n",
    "    [[], []],\n",
    "    [([]),([])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b66a3015",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-89f11a770b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-294-3f80882dda20>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len, gold)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwhere_op_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_wo\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg_wo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_op_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere_op_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_nums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere_nums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mwhere_value_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere_value_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_col_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_op_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wv_tkns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         decoder_outputs = {\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-bba51efef0f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, question_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns, rt_attn)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lstm_cell_init_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single_where_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, T_d_i, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mtotal_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-bba51efef0f6>\u001b[0m in \u001b[0;36mdecode_single_where_col\u001b[0;34m(self, batch_size, h_0, c_0, value_tkn_max_len, g_wv_tkns_i)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# [Testing]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,) only for single batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_tkn_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "decoder_outputs = model_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=20, gold=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "3738535c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-6d0b7a1ee2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwhere_op_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_wo\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg_wo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_op_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere_op_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_nums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere_nums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwhere_value_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere_value_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_col_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_op_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wv_tkns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-bba51efef0f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, question_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns, rt_attn)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lstm_cell_init_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single_where_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, T_d_i, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mtotal_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-bba51efef0f6>\u001b[0m in \u001b[0;36mdecode_single_where_col\u001b[0;34m(self, batch_size, h_0, c_0, value_tkn_max_len, g_wv_tkns_i)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# [Testing]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,) only for single batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_tkn_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "select_outputs, _ = model_decoder.select_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "select_idxes = g_sc if g_sc else model_decoder.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "agg_outputs, _ = model_decoder.agg_decoder(question_padded, col_padded, question_lengths, col_lengths, select_idxes)\n",
    "\n",
    "where_num_outputs, _  = model_decoder.where_num_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "where_nums = g_wn if g_wn else model_decoder.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "where_col_outputs, _ = model_decoder.where_col_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "where_col_idxes = g_wc if g_wc else model_decoder.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "\n",
    "where_op_outputs = model_decoder.where_op_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes)\n",
    "where_op_idxes = g_wo if g_wo else model_decoder.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "\n",
    "where_value_outputs = model_decoder.where_value_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "0efd2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = model_decoder.where_value_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "6d63fe3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-340-527775fc7eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lstm_cell_init_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single_where_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_tkn_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_wv_tkns_i\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, T_d_i, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtotal_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-bba51efef0f6>\u001b[0m in \u001b[0;36mdecode_single_where_col\u001b[0;34m(self, batch_size, h_0, c_0, value_tkn_max_len, g_wv_tkns_i)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# [Testing]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,) only for single batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_tkn_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "batch_size, n_col, _ = col_padded.size()\n",
    "o_q, (h_q, c_q) = dd.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "o_c, (h_c, c_c) = dd.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "o_h, (h_h, c_h) = dd.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "\n",
    "header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "col_context = dd.col_context_linear(col_context)  # (B, T_c, H)\n",
    "col_context_padded = dd.get_context_padded(col_context, where_nums, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "\n",
    "col_q_context, attn = dd.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, False)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "where_op_one_hot_padded = dd.get_where_op_one_hot_padded(where_op_idxes, where_nums, where_col_idxes, n_cond_ops=dd.n_cond_ops)#.to(o_q.device)  # (B, max_where_col_nums, n_cond_ops)\n",
    "where_op = dd.where_op_linear(where_op_one_hot_padded)  # (B, max_where_col_nums, H)\n",
    "\n",
    "vec = torch.cat([col_q_context, col_context_padded, where_op], dim=2)  # (B, max_where_col_nums, 3H)\n",
    "max_where_col_nums = vec.size(1)\n",
    "# predict each where_col\n",
    "total_scores = []\n",
    "for i in range(max_where_col_nums):\n",
    "    g_wv_tkns_i = torch.LongTensor([g_wv_tkns[b_idx][i] for b_idx in range(batch_size)]) if g_wv_tkns is not None else None  # (B, T_d_i)\n",
    "    vec_i = vec[:, i, :]  # (B, 3H)\n",
    "\n",
    "    h_0 = dd.output_lstm_hidden_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "    c_0 = dd.output_lstm_cell_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "    \n",
    "    scores = dd.decode_single_where_col(batch_size, h_0, c_0, value_tkn_max_len=value_tkn_max_len, g_wv_tkns_i=g_wv_tkns_i)  # (B, T_d_i, vocab_size)\n",
    "    total_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "b3602605",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "(h_0, c_0) = torch.randn(1, batch_size, 768),torch.randn(1, batch_size, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "12b888af",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_tkn_max_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "62f9f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if value_tkn_max_len is None:\n",
    "    # [Training] set the max length to gold token max length (already padded)\n",
    "    max_len = len(g_wv_tkns_i[0])\n",
    "else:\n",
    "    # [Testing]  don't know the max length\n",
    "    max_len = value_tkn_max_len\n",
    "\n",
    "sos = dd.start_token(batch_size)  # (B, 1)\n",
    "emb = dd.embedding_layer(sos)  # (B, 1, bert_H)\n",
    "scores = []\n",
    "for i in range(max_len):\n",
    "    temp_score = []\n",
    "    o, (h, c) = dd.output_lstm(emb, (h_0, c_0))  # h: (1, B, bert_H)  \n",
    "    s = dd.output_linear(h[-1, :]) # select last layer if use multiple rnn layers, h: (1, B, bert_H) -> (B, bert_H) -> s: (B, vocab_size)\n",
    "    \n",
    "\n",
    "    if g_wv_tkns_i is not None:\n",
    "        # [Training] Teacher Force model\n",
    "        pred = g_wv_tkns_i[:, i]  # (B, )\n",
    "        scores.append(s)\n",
    "    else:\n",
    "        # [Testing]\n",
    "        pred = s.argmax(1)\n",
    "        if (pred == dd.end_tkn_id).sum() == batch_size:  # all stop\n",
    "            break\n",
    "        else:\n",
    "            stop_mask = pred == dd.end_tkn_id\n",
    "            temp_score.append(s[stop_mask])\n",
    "            pred = pred[~stop_mask]\n",
    "    emb = dd.embedding_layer(pred.unsqueeze(1))  # (B, 1, bert_H)\n",
    "\n",
    "# torch.stack(scores).transpose(0, 1).contiguous() # (T_d_i, B, vocab_size) -> (B, T_d_i, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "9f0c1054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], size=(0, 8005), grad_fn=<IndexBackward>)]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e1ebebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8005])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6745d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.LongTensor([8003, 7658, 4334])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "730a14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == dd.end_tkn_id).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "27144e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_mask = pred == dd.end_tkn_id # continue decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e8b212f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1945,  0.2386,  0.0912,  ...,  0.1049,  0.1218, -0.1526]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[stop_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a55f4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "83a7b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_tkn_max_len=20\n",
    "gold=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f5d4a8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[-0.1163, -0.1109, -0.1078, -0.1030, -0.1065, -0.0998, -0.0895, -0.1010,\n",
       "          -0.1024, -0.1002, -0.1019, -0.1069, -0.1043, -0.1075, -0.1036, -0.1041,\n",
       "          -0.1023, -0.1103, -0.1118, -0.1079],\n",
       "         [-0.1090, -0.1078, -0.1040, -0.1038, -0.1030, -0.0990, -0.0979, -0.0973,\n",
       "          -0.0993, -0.1048, -0.1039, -0.0977, -0.1025, -0.0991, -0.1010, -0.1044,\n",
       "          -0.1000, -0.1056, -0.1075, -0.1071]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[ 0.0106, -0.0421, -0.0010,  0.0178, -0.1228, -0.0558],\n",
       "         [ 0.0046, -0.0488, -0.0038,  0.0123, -0.1135, -0.0388]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[ 0.0669, -0.0519, -0.0237, -0.0665,  0.0456],\n",
       "         [ 0.0634, -0.0608, -0.0213, -0.0428,  0.0621]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[-0.0556, -0.0530, -0.0546, -0.0559, -0.0581, -0.0510, -0.0543, -0.0517,\n",
       "          -0.0571, -0.0567, -0.0574, -0.0640, -0.0641, -0.0640, -0.0605, -0.0559,\n",
       "          -0.0615, -0.0614, -0.0541, -0.0542],\n",
       "         [-0.0593, -0.0583, -0.0557, -0.0493, -0.0513, -0.0555, -0.0566, -0.0592,\n",
       "          -0.0556, -0.0589, -0.0649, -0.0669, -0.0691, -0.0692, -0.0649, -0.0658,\n",
       "          -0.0649, -0.0648, -0.0591, -0.0573]], grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[-0.0276,  0.0138,  0.0601,  0.0230],\n",
       "          [-0.0294,  0.0176,  0.0546,  0.0134]],\n",
       " \n",
       "         [[-0.0290,  0.0214,  0.0627,  0.0054],\n",
       "          [-0.0324,  0.0243,  0.0588, -0.0031]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[ 0.0399,  0.0598,  0.0037,  ..., -0.0126,  0.0027,  0.0270],\n",
       "           [ 0.0258,  0.0659,  0.0013,  ..., -0.0041,  0.0051,  0.0306]],\n",
       "  \n",
       "          [[ 0.0322,  0.0586,  0.0007,  ..., -0.0149,  0.0035,  0.0242],\n",
       "           [ 0.0240,  0.0918,  0.0056,  ..., -0.0130,  0.0044,  0.0294]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[ 0.0352,  0.0573,  0.0058,  ..., -0.0108,  0.0022,  0.0254],\n",
       "           [ 0.0344,  0.0501, -0.0024,  ..., -0.0087,  0.0072,  0.0279],\n",
       "           [ 0.0255,  0.0554,  0.0011,  ..., -0.0091,  0.0046,  0.0251]],\n",
       "  \n",
       "          [[ 0.0252,  0.0622,  0.0021,  ..., -0.0142,  0.0036,  0.0234],\n",
       "           [ 0.0115,  0.0509, -0.0087,  ..., -0.0106,  0.0056,  0.0205],\n",
       "           [ 0.0273,  0.0632, -0.0019,  ..., -0.0051,  0.0138,  0.0240]]],\n",
       "         grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f39fe",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8a5fafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "606a8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "binary_cross_entropy = nn.BCEWithLogitsLoss()\n",
    "vocab_size = len(tokenizer_bert)\n",
    "\n",
    "loss_sc = cross_entropy(decoder_outputs[\"sc\"], torch.LongTensor(g_sc))\n",
    "loss_sa = cross_entropy(decoder_outputs[\"sa\"], torch.LongTensor(g_sa))\n",
    "loss_wn = cross_entropy(decoder_outputs[\"wn\"], torch.LongTensor(g_wn))\n",
    "\n",
    "one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"]).scatter(1, torch.LongTensor(g_wc), 1.0)\n",
    "loss_wc = binary_cross_entropy(decoder_outputs[\"wc\"], one_hot_dist)\n",
    "\n",
    "# add loss by where numbers\n",
    "loss_wo = 0\n",
    "for wo_i, batch_wo in enumerate(decoder_outputs[\"wo\"].transpose(0, 1)):\n",
    "    loss_wo += cross_entropy(batch_wo, torch.LongTensor(g_wo[wo_i]))\n",
    "\n",
    "loss_wv = 0\n",
    "g_wv_tkns_by_wn = list(zip(*g_wv_tkns))\n",
    "for wn_i, batch_wv in enumerate(decoder_outputs[\"wv\"]):\n",
    "    loss_wv += cross_entropy(batch_wv.view(-1, vocab_size), torch.LongTensor(g_wv_tkns_by_wn[wn_i]).view(-1))\n",
    "    \n",
    "total_loss = loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b74d525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.0726, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963ffed2",
   "metadata": {},
   "source": [
    "# Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "67e16fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_bert, \n",
    "        tokenizer_bert,\n",
    "        input_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        dropout_ratio, \n",
    "        max_where_conds, \n",
    "        n_agg_ops, \n",
    "        n_cond_ops, \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.model_bert = model_bert\n",
    "        self.tokenizer_bert = tokenizer_bert\n",
    "        # Decoder\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        self.n_agg_ops = n_agg_ops\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        \n",
    "        self.model_decoder = Decoder(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            dropout_ratio,\n",
    "            max_where_conds, \n",
    "            n_agg_ops, \n",
    "            n_cond_ops, \n",
    "            start_tkn_id = tokenizer_bert.additional_special_tokens_ids[0],\n",
    "            end_tkn_id = tokenizer_bert.additional_special_tokens_ids[1],\n",
    "            embedding_layer = model_bert.embeddings.word_embeddings\n",
    "        )\n",
    "        # Loss\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.binary_cross_entropy = nn.BCEWithLogitsLoss()\n",
    "        self.vocab_size = len(self.tokenizer_bert)\n",
    "        \n",
    "    def forward_outputs(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        # --- Get Answer & Variables ---\n",
    "        if train:\n",
    "            assert value_tkn_max_len is None, \"In Training Phase, `value_tkn_max_len` must be None\"\n",
    "            assert batch_sqls is not None, \"In Training Phase, `batch_sqls` must not be None\"\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls, self.tokenizer_bert, end_tkn_in_tokenzier_idx=1)\n",
    "            gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "            \n",
    "        else:\n",
    "            gold = None\n",
    "            value_tkn_max_len = value_tkn_max_len\n",
    "            \n",
    "        # --- Get Inputs for Encoder --- \n",
    "        encode_inputs = tokenizer_bert(\n",
    "            batch_qs, batch_ts, \n",
    "            max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "            return_attention_mask=True, \n",
    "            return_special_tokens_mask=False, \n",
    "        )\n",
    "        \n",
    "        # --- Forward Encoder ---\n",
    "        encode_outputs = self.model_bert(**encode_input)\n",
    "        \n",
    "        # --- Get Inputs for Decoder ---\n",
    "        input_question_mask, input_table_mask, input_header_mask, input_col_mask = self.get_input_mask_and_answer(encode_inputs, self.tokenizer_bert)\n",
    "        question_padded, question_lengths = get_decoder_batches(encode_outputs, input_question_mask, self.model_bert, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        # table_padded, table_lengths = get_decoder_batches(encode_outputs, input_table_mask, self.model_bert, pad_idx=self.tokenizer_bert.pad_token_id)  # Not used yet\n",
    "        header_padded, header_lengths = get_decoder_batches(encode_outputs, input_header_mask, self.model_bert, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        col_padded, col_lengths = get_decoder_batches(encode_outputs, input_col_mask, self.model_bert, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        \n",
    "        # --- Forward Decoder ---\n",
    "        decoder_outputs = self.model_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len, gold)\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def predict(self, qs, ts, sqls, value_tkn_max_len):\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=value_tkn_max_len, train=False)\n",
    "        \n",
    "        predicts = {}\n",
    "        predicts[\"sc\"] = self.model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "        predicts[\"sa\"] = self.model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "        predicts[\"wn\"] = self.model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "        predicts[\"wc\"] = self.model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wo\"] = self.model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wv\"] = self.model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, _ = self.get_sql_answers(batch_sqls, self.tokenizer_bert, end_tkn_in_tokenzier_idx=1)\n",
    "        golds = dict(sc=g_sc, sa=g_sa, wn=g_wn, wc=g_wc, wo=g_wo, wv=g_wv)\n",
    "        \n",
    "        return predicts, golds\n",
    "    \n",
    "    def forward(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls, value_tkn_max_len, train)\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls, self.tokenizer_bert, end_tkn_in_tokenzier_idx=1)\n",
    "        gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "        loss = self.calculate_loss(outputs, gold)\n",
    "        return loss, outputs\n",
    "    \n",
    "    def calculate_loss(self, decoder_outputs, gold, train=True):\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "            \n",
    "        loss_sc = self.cross_entropy(decoder_outputs[\"sc\"], torch.LongTensor(g_sc))\n",
    "        loss_sa = self.cross_entropy(decoder_outputs[\"sa\"], torch.LongTensor(g_sa))\n",
    "        loss_wn = self.cross_entropy(decoder_outputs[\"wn\"], torch.LongTensor(g_wn))\n",
    "\n",
    "        one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"]).scatter(1, torch.LongTensor(g_wc), 1.0)\n",
    "        loss_wc = self.binary_cross_entropy(decoder_outputs[\"wc\"], one_hot_dist)\n",
    "\n",
    "        # add loss by where numbers\n",
    "        loss_wo = 0\n",
    "        for wo_i, batch_wo in enumerate(decoder_outputs[\"wo\"].transpose(0, 1)):\n",
    "            loss_wo += self.cross_entropy(batch_wo, torch.LongTensor(g_wo[wo_i]))\n",
    "\n",
    "        loss_wv = 0\n",
    "        g_wv_tkns_by_wn = list(zip(*g_wv_tkns))\n",
    "        for wn_i, batch_wv in enumerate(decoder_outputs[\"wv\"]):\n",
    "            loss_wv += self.cross_entropy(batch_wv.view(-1, self.vocab_size), torch.LongTensor(g_wv_tkns_by_wn[wn_i]).view(-1))\n",
    "\n",
    "        loss = loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv\n",
    "        return loss\n",
    "    \n",
    "    def get_sql_answers(self, batch_sqls: List[Dict[str, Any]], tokenizer: KoBertTokenizer, end_tkn_in_tokenzier_idx:int=1):\n",
    "        \"\"\"[summary]\n",
    "        sc: select column\n",
    "        sa: select agg\n",
    "        wn: where number\n",
    "        wc: where column\n",
    "        wo: where operator\n",
    "        wv: where value\n",
    "\n",
    "        Args:\n",
    "            batch_sqls (List[Dict[str, Any]]): [description]\n",
    "            tokenizer (KoBertTokenizer): [description]\n",
    "            end_tkn_in_tokenzier_idx (int, optional): [description]. Defaults to 1.\n",
    "\n",
    "        Raises:\n",
    "            EnvironmentError: [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "        g_sc = []\n",
    "        g_sa = []\n",
    "        g_wn = []\n",
    "        g_wc = []\n",
    "        g_wo = []\n",
    "        g_wv = []\n",
    "        for b, sql_dict in enumerate(batch_sqls):\n",
    "            g_sc.append( sql_dict[\"sel\"] )\n",
    "            g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "            conds = sql_dict[\"conds\"]\n",
    "            if not sql_dict[\"agg\"] < 0:\n",
    "                g_wn.append( len(conds) )\n",
    "                g_wc.append( get_ith_element(conds, 0) )\n",
    "                g_wo.append( get_ith_element(conds, 1) )\n",
    "                g_wv.append( get_ith_element(conds, 2) )\n",
    "            else:\n",
    "                raise EnvironmentError\n",
    "\n",
    "        # get where value tokenized \n",
    "        end_tkn = tokenizer.additional_special_tokens[end_tkn_in_tokenzier_idx]\n",
    "        pad_tkn_id = tokenizer.pad_token_id\n",
    "        g_wv_tkns = [[f\"{s}{end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "        g_wv_tkns = [tokenizer(batch_wv, add_special_tokens=False)[\"input_ids\"] for batch_wv in g_wv_tkns]\n",
    "        # add empty list if batch has different where column number\n",
    "        max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "        g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "        temp = []\n",
    "        for batch_wv in list(zip(*g_wv_tkns)):\n",
    "            batch_max_len = max(map(len, batch_wv))\n",
    "            batch_temp = []\n",
    "            for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "                if len(wv_tkns) < batch_max_len:\n",
    "                    batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "                else:\n",
    "                    batch_temp.append(wv_tkns)\n",
    "            temp.append(batch_temp)\n",
    "        g_wv_tkns = list(zip(*temp))\n",
    "\n",
    "        return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns\n",
    "    \n",
    "    \n",
    "    ## Masks\n",
    "    # TODO: [EXP] Experiment for generate column directly\n",
    "    # def get_answer(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    #     r\"\"\"\n",
    "    #     answer should include end token: [E]\n",
    "    #     \"\"\"\n",
    "    #     masked_input_ids = input_ids[mask]\n",
    "    #     start_tkn_mask = masked_input_ids == start_tkn_id\n",
    "    #     end_tkn_mask = masked_input_ids == end_tkn_id\n",
    "    #     table_col_length = masked_input_ids.view(batch_size, -1).size(1)\n",
    "    #     start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    #     index = torch.arange(table_col_length).repeat(batch_size)[start_end_mask].view(batch_size, -1, 2)\n",
    "    #     tkn_lengths = index[:, :, 1] - index[:, :, 0]\n",
    "    #     answer_col_tkns = [x.split(tkn_length.tolist()) for x, tkn_length in zip(\n",
    "    #         masked_input_ids[~start_tkn_mask].view(batch_size, -1), tkn_lengths)]\n",
    "    #     return answer_col_tkns\n",
    "\n",
    "\n",
    "    def get_decoder_input_mask(self, input_ids: torch.Tensor, mask: torch.BoolTensor, batch_size: int, start_tkn_id: int, end_tkn_id: int) -> torch.BoolTensor:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): [description]\n",
    "            mask (torch.BoolTensor): [description]\n",
    "            batch_size (int): [description]\n",
    "            start_tkn_id (int): [description]\n",
    "            end_tkn_id (int): [description]\n",
    "\n",
    "        Returns:\n",
    "            torch.BoolTensor: [description]\n",
    "        \"\"\"    \n",
    "        start_tkn_mask = input_ids == start_tkn_id\n",
    "        end_tkn_mask = input_ids == end_tkn_id\n",
    "        start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "        index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "        return mask.scatter(1, index, False)\n",
    "\n",
    "\n",
    "    def get_input_mask_and_answer(self, encode_input: transformers.tokenization_utils_base.BatchEncoding, tokenizer: KoBertTokenizer) -> Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        In this code 'table' means database table name(id), 'header' means database header, 'col' means index of header \n",
    "\n",
    "        Args:\n",
    "            encode_input (transformers.tokenization_utils_base.BatchEncoding): [description]\n",
    "            tokenizer (KoBertTokenizer): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "        sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "        start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "\n",
    "        input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "        input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "        input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "        db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "        db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "        col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "        db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "        # split table_mask and header_mask\n",
    "        input_idx = torch.arange(max_length).repeat(batch_size, 1)\n",
    "        db_idx = input_idx[db_mask]\n",
    "        table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "        table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "        start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "        table_end_sep_idx = start_idx[:, 0] - 1\n",
    "        split_size = torch.stack([\n",
    "            table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "        ]).transpose(0, 1)\n",
    "\n",
    "        # Token idx\n",
    "        table_tkn_idx, header_tkn_idx = map(\n",
    "            lambda x: torch.stack(x), \n",
    "            zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "        )\n",
    "\n",
    "        table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # If [EXP], `table_tkn_mask` and `header_tkn_mask` should include [S] & [E] tokens\n",
    "        table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, table_tkn_idx, True)\n",
    "        header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # For Decoder Input, Maskout [S], [E] for table & header -> will be done automatically\n",
    "        input_table_mask = self.get_decoder_input_mask(\n",
    "            encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        )\n",
    "        input_header_mask = self.get_decoder_input_mask(\n",
    "            encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        )\n",
    "\n",
    "        # [COL] token mask: this is for attention\n",
    "        col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "        input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # For Answer, Maskout [S] for table & header \n",
    "        # answer_table_tkns = get_answer(\n",
    "        #     encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        # )\n",
    "        # answer_header_tkns = get_answer(\n",
    "        #     encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        # )\n",
    "\n",
    "        return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns\n",
    "\n",
    "\n",
    "    ## Pad for decoder inputs\n",
    "    def pad(self, batches: Tuple[torch.Tensor], lengths: List[int], model: transformers.models.bert.modeling_bert.BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            batches (Tuple[torch.Tensor]): [description]\n",
    "            lengths (List[int]): [description]\n",
    "            model (transformers.models.bert.modeling_bert.BertModel): [description]\n",
    "            pad_idx (int, optional): [description]. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [description]\n",
    "        \"\"\"       \n",
    "        padded = []\n",
    "        max_length = max(lengths)\n",
    "        for x in batches:\n",
    "            if len(x) < max_length:\n",
    "                pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "                padded.append(torch.cat([x, pad_tensor]))\n",
    "            else:\n",
    "                padded.append(x)\n",
    "        return torch.stack(padded)\n",
    "\n",
    "    def get_decoder_batches(self, encode_output: transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions, mask: torch.BoolTensor, model: BertModel, pad_idx: int) -> Tuple[torch.Tensor, List[int]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            encode_output (transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions): [description]\n",
    "            mask (torch.BoolTensor): [description]\n",
    "            model (BertModel): [description]\n",
    "            pad_idx (int): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, List[int]]: [description]\n",
    "        \"\"\"    \n",
    "        lengths = mask.sum(1)\n",
    "        tensors = encode_output.last_hidden_state[mask, :]\n",
    "        batches = torch.split(tensors, lengths.tolist())\n",
    "        if lengths.ne(lengths.max()).sum().item() != 0:\n",
    "            # pad not same length tokens\n",
    "            tensors_padded = self.pad(batches, lengths.tolist(), model, pad_idx=pad_idx)\n",
    "        else:\n",
    "            # just stack the splitted tensors\n",
    "            tensors_padded = torch.stack(batches)\n",
    "        return tensors_padded, lengths.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "41c10e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    value_tkn_max_len = None\n",
    "else:\n",
    "    value_tkn_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "627508da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ARGS at 0x7fb2b01b2710>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a03d38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "592cb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"monologg/kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "def create_model(args, dbengine):\n",
    "    model_bert, tokenizer_bert, config_bert = get_bert(model_path=model_path)\n",
    "    model = Text2SQL(\n",
    "        model_bert=model_bert,\n",
    "        tokenizer_bert=tokenizer_bert,\n",
    "        input_size=config_bert.hidden_size,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers,\n",
    "        dropout_ratio=args.dropout_ratio,\n",
    "        max_where_conds=args.max_where_conds,\n",
    "        n_agg_ops=len(dbengine.agg_ops),\n",
    "        n_cond_ops=len(dbengine.cond_ops)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d5efdda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(args, dbengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c3464622",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, outputs = model(batch_qs, batch_ts, batch_sqls, value_tkn_max_len=None, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b927a63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.8144, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9dbd7dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[-0.0159, -0.0185, -0.0185, -0.0168, -0.0154, -0.0169, -0.0148, -0.0150,\n",
       "          -0.0149, -0.0084, -0.0031, -0.0086, -0.0085, -0.0047, -0.0110, -0.0132,\n",
       "          -0.0090, -0.0135, -0.0190, -0.0227],\n",
       "         [-0.0279, -0.0242, -0.0286, -0.0245, -0.0289, -0.0345, -0.0240, -0.0223,\n",
       "          -0.0274, -0.0257, -0.0198, -0.0214, -0.0198, -0.0223, -0.0207, -0.0270,\n",
       "          -0.0323, -0.0301, -0.0289, -0.0264]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[-0.0337, -0.0559, -0.0980, -0.0553,  0.0814,  0.0067],\n",
       "         [-0.0407, -0.0549, -0.1032, -0.0574,  0.0700,  0.0102]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[-0.0234,  0.0533,  0.0091, -0.0143, -0.0510],\n",
       "         [-0.0045,  0.0483,  0.0057, -0.0051, -0.0425]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[0.0371, 0.0337, 0.0299, 0.0279, 0.0292, 0.0283, 0.0312, 0.0295, 0.0344,\n",
       "          0.0313, 0.0277, 0.0325, 0.0310, 0.0280, 0.0269, 0.0248, 0.0253, 0.0268,\n",
       "          0.0277, 0.0275],\n",
       "         [0.0248, 0.0211, 0.0238, 0.0210, 0.0207, 0.0207, 0.0198, 0.0241, 0.0205,\n",
       "          0.0229, 0.0247, 0.0260, 0.0319, 0.0264, 0.0229, 0.0182, 0.0169, 0.0152,\n",
       "          0.0163, 0.0158]], grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[ 0.0550, -0.0148,  0.0687, -0.0362],\n",
       "          [ 0.0547, -0.0218,  0.0672, -0.0387]],\n",
       " \n",
       "         [[ 0.0481, -0.0316,  0.0596, -0.0504],\n",
       "          [ 0.0514, -0.0337,  0.0604, -0.0537]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[ 0.0376, -0.0380,  0.0383,  ...,  0.0451, -0.0048, -0.0304],\n",
       "           [ 0.0299, -0.0256,  0.0430,  ...,  0.0439, -0.0111, -0.0301]],\n",
       "  \n",
       "          [[ 0.0432, -0.0309,  0.0366,  ...,  0.0443, -0.0008, -0.0316],\n",
       "           [ 0.0434, -0.0254,  0.0334,  ...,  0.0464, -0.0049, -0.0295]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[ 0.0376, -0.0369,  0.0378,  ...,  0.0454, -0.0059, -0.0321],\n",
       "           [ 0.0417, -0.0373,  0.0364,  ...,  0.0478, -0.0033, -0.0265],\n",
       "           [ 0.0348, -0.0245,  0.0380,  ...,  0.0457, -0.0051, -0.0301]],\n",
       "  \n",
       "          [[ 0.0420, -0.0357,  0.0367,  ...,  0.0453, -0.0020, -0.0319],\n",
       "           [ 0.0508, -0.0395,  0.0371,  ...,  0.0429, -0.0059, -0.0340],\n",
       "           [ 0.0554, -0.0538,  0.0383,  ...,  0.0406, -0.0023, -0.0316]]],\n",
       "         grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10b6c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61129cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Load DBEngine\n",
    "        self.dbengine = DBEngine(Path(self.hparams.db_path))        \n",
    "        self.model = self.create_model(dbengine=self.dbengine)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table, self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=None, \n",
    "            train=True\n",
    "        )\n",
    "\n",
    "        return  {'loss': loss}  \n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table, self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=None, \n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        return  {'loss': loss}  \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"eval\")    \n",
    "  \n",
    "    \n",
    "    def load_data(self, sql_path: Union[Path, str], table_path: Union[Path, str]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        \"\"\"Load data from path\n",
    "\n",
    "        Args:\n",
    "            sql_path (Union[Path, str]): dataset path which contains NL with SQL queries (+answers)\n",
    "            table_path (Union[Path, str]): table information contains table name, header and values\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[Dict[str, Any]], Dict[str, Any]]: [description]\n",
    "        \"\"\"    \n",
    "        path_sql = Path(sql_path)\n",
    "        path_table = Path(table_path)\n",
    "\n",
    "        dataset = []\n",
    "        table = {}\n",
    "        with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                x = json.loads(line.strip())\n",
    "                dataset.append(x)\n",
    "\n",
    "        with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                x = json.loads(line.strip())\n",
    "                table[x['id']] = x\n",
    "\n",
    "        return dataset, table\n",
    "    \n",
    "    def create_data_loader(self, mode):\n",
    "        num_workers = 0 if os.name == \"nt\" else self.hparams.num_workers\n",
    "        if mode == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = self.hparams.train_batch_size\n",
    "            sql_file = self.hparams.train_sql_file\n",
    "            table_file = self.hparams.train_table_file\n",
    "        else:\n",
    "            shuffle = False\n",
    "            batch_size = self.hparams.eval_batch_size\n",
    "            sql_file = self.hparams.eval_sql_file\n",
    "            table_file = self.hparams.eval_table_file\n",
    "        \n",
    "        dataset, self.table = self.load_data(sql_file, table_file)\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            batch_size=batch_size,\n",
    "            dataset=dataset,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=lambda x: x # now dictionary values are not merged!\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def get_batch_data(data: List[Dict[str, Any]], table: Dict[str, Dict[str, List[Any]]] start_tkn=\"[S]\", end_tkn=\"[E]\") -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict[str, Any]]): [description]\n",
    "            dbengine (DBEngine): [description]\n",
    "            start_tkn (str, optional): [description]. Defaults to \"[S]\".\n",
    "            end_tkn (str, optional): [description]. Defaults to \"[E]\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[Dict[str, Any]]]: [description]\n",
    "        \"\"\"    \n",
    "        batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "        tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "        batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "        batch_ts = []\n",
    "        for table_id in tid:\n",
    "            table_str = f\"{table_id}\" + \"\".join([\n",
    "                f\"{self.hparams.special_col_tkn}{col}\" for col in table[table_id][\"header\"]\n",
    "            ])\n",
    "            # TODO: [EXP] Experiment for generate column directly\n",
    "            # table_str = f\"{start_tkn}{table_id}{end_tkn}\" + \"\".join([\n",
    "            #     f\"{col_tkn}{start_tkn}{col}{end_tkn}\" for col in dbengine.schema\n",
    "            # ]) \n",
    "            batch_ts.append(table_str)\n",
    "\n",
    "        return batch_qs, batch_ts, batch_sqls\n",
    "    \n",
    "    def create_model(self, dbengine):\n",
    "        model_bert, tokenizer_bert, config_bert = self.get_bert(model_path=self.hparams.model_bert_path)\n",
    "        model = Text2SQL(\n",
    "            model_bert=model_bert,\n",
    "            tokenizer_bert=tokenizer_bert,\n",
    "            input_size=config_bert.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            dropout_ratio=self.hparams.dropout_ratio,\n",
    "            max_where_conds=self.hparams.max_where_conds,\n",
    "            n_agg_ops=len(dbengine.agg_ops),\n",
    "            n_cond_ops=len(dbengine.cond_ops)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def get_bert(self, model_path: str, output_hidden_states: bool=False):\n",
    "        self.special_tokens = [self.hparams.special_start_tkn, self.hparams.special_end_tkn, self.hparams.special_col_tkn] # sequence start, sequence end, column tokens\n",
    "        tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        model = BertModel.from_pretrained(model_path)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d4736c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'rcept_no',\n",
       " 'reprt_code',\n",
       " 'bsns_year',\n",
       " 'corp_code',\n",
       " 'stock_code',\n",
       " 'fs_div',\n",
       " 'fs_nm',\n",
       " 'sj_div',\n",
       " 'sj_nm',\n",
       " 'account_nm',\n",
       " 'thstrm_nm',\n",
       " 'thstrm_dt',\n",
       " 'thstrm_amount',\n",
       " 'frmtrm_nm',\n",
       " 'frmtrm_dt',\n",
       " 'frmtrm_amount',\n",
       " 'bfefrmtrm_nm',\n",
       " 'bfefrmtrm_dt',\n",
       " 'bfefrmtrm_amount']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[\"receipts\"][\"header\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de669bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    db_path = \"./private/samsung_new.db\",\n",
    "    model_bert_path = \"monologg/kobert\",\n",
    "    # Dataloader\n",
    "    train_sql_file = \"./NLSQL_train.jsonl\",\n",
    "    train_table_file = \"./table_train.jsonl\",\n",
    "    train_batch_size = 16,\n",
    "    eval_file = \"./NLSQL_test.jsonl\",\n",
    "    eval_table_file = \"./table_test.jsonl\",\n",
    "    eval_batch_size = 16,\n",
    "    num_workers = 4,\n",
    "    # Model-decoder\n",
    "    hidden_size = 100,\n",
    "    num_layers = 2,\n",
    "    dropout_ratio = 0.3,\n",
    "    max_where_conds = 4,\n",
    "    # Tokenizer\n",
    "    special_start_tkn = \"[S]\", \n",
    "    special_end_tkn = \"[E]\",\n",
    "    special_col_tkn = \"[COL]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c3d0000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    db_path = \"./data/samsung_new.db\",\n",
    "    model_bert_path = \"monologg/kobert\",\n",
    "    # Dataloader\n",
    "    train_sql_file = \"./data/NLSQL_train.jsonl\",\n",
    "    train_table_file = \"./data/table_train.jsonl\",\n",
    "    train_batch_size = 16,\n",
    "    eval_file = \"./data/NLSQL_test.jsonl\",\n",
    "    eval_table_file = \"./data/table_test.jsonl\",\n",
    "    eval_batch_size = 16,\n",
    "    num_workers = 4,\n",
    "    # Model-decoder\n",
    "    hidden_size = 100,\n",
    "    num_layers = 2,\n",
    "    dropout_ratio = 0.3,\n",
    "    max_where_conds = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af87c70",
   "metadata": {},
   "source": [
    "## Traning\n",
    "\n",
    "Stil Working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr_bert = 1e-5\n",
    "\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=lr, weight_decay=0)\n",
    "opt_bert = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_bert.parameters()),\n",
    "                            lr=lr_bert, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d471166",
   "metadata": {},
   "source": [
    "## Testing: Execution-guided beam decoding\n",
    "\n",
    "Stil Working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba775307",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a3e92",
   "metadata": {},
   "source": [
    "select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38b70b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_output, _ = select_decoder(question_padded, header_padded, col_padded, question_lengths)\n",
    "select_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fcdfc2",
   "metadata": {},
   "source": [
    "construct all possible select + (agg) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03613ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_size, n_col = select_output.size()\n",
    "\n",
    "select_prob = torch.softmax(select_output, 1)  # prob_sc\n",
    "if n_col < beam_size:\n",
    "    beam_size_max_col = n_col\n",
    "else:\n",
    "    beam_size_max_col = beam_size\n",
    "\n",
    "prob_sc_sa = torch.zeros([batch_size, beam_size_max_col, n_agg_ops])\n",
    "prob_sca = torch.zeros_like(prob_sc_sa)\n",
    "print(prob_sca.size())  # (B, beam-size, n_agg_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2c403d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc top k: [[19, 18, 0, 14], [3, 4, 6, 5]]\n"
     ]
    }
   ],
   "source": [
    "# beamseacrh\n",
    "_, pr_sc_beam = select_output.topk(k=beam_size_max_col)\n",
    "print(f\"sc top k: {pr_sc_beam.tolist()}\")\n",
    "\n",
    "for i_beam in range(beam_size_max_col):\n",
    "    select_idx = pr_sc_beam[:, i_beam].tolist() # pr_sc\n",
    "    agg_output, _ = agg_decoder(question_padded, col_padded, question_lengths, select_idx)\n",
    "    agg_prob = torch.softmax(agg_output, dim=-1)  # prob_sa: (B, n_agg_ops)\n",
    "    prob_sc_sa[:, i_beam, :] = agg_prob\n",
    "    \n",
    "    prob_sc_selected = select_prob[range(batch_size), select_idx]  # (B,)\n",
    "    prob_sca[:, i_beam, :] = (agg_prob.t() * prob_sc_selected).t()  # (n_agg_ops, B) \\odot (1, B) (broadcast) -> (B, max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "906b8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1765, 0.1692, 0.1639, 0.1756, 0.1588, 0.1561],\n",
      "         [0.1777, 0.1687, 0.1647, 0.1742, 0.1588, 0.1558],\n",
      "         [0.1764, 0.1690, 0.1643, 0.1751, 0.1593, 0.1558],\n",
      "         [0.1779, 0.1693, 0.1647, 0.1732, 0.1581, 0.1568]],\n",
      "\n",
      "        [[0.1778, 0.1697, 0.1638, 0.1724, 0.1561, 0.1601],\n",
      "         [0.1778, 0.1712, 0.1638, 0.1742, 0.1554, 0.1577],\n",
      "         [0.1789, 0.1702, 0.1634, 0.1741, 0.1557, 0.1578],\n",
      "         [0.1774, 0.1712, 0.1640, 0.1729, 0.1564, 0.1581]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sc_sa.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79e013c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n",
      "tensor([[[0.0088, 0.0085, 0.0082, 0.0088, 0.0079, 0.0078],\n",
      "         [0.0089, 0.0084, 0.0082, 0.0087, 0.0079, 0.0078],\n",
      "         [0.0088, 0.0084, 0.0082, 0.0088, 0.0080, 0.0078],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0079, 0.0078]],\n",
      "\n",
      "        [[0.0089, 0.0085, 0.0082, 0.0086, 0.0078, 0.0080],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0086, 0.0078, 0.0079]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sca.size())  # (B, beam_size, prob_sc(beam size selected) * prob_agg)\n",
    "print(prob_sca.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f0c2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_multi_dim(tensor, n_topk):\n",
    "    batch_size = tensor.size(0)\n",
    "    values_1d, idxes_1d = tensor.view(batch_size, -1).topk(n_topk)\n",
    "    idxes = np.stack(np.unravel_index(idxes_1d, tensor.size()[1:])).transpose(1, 2, 0)\n",
    "    values = tensor.view(batch_size, -1).gather(1, idxes_1d).numpy()\n",
    "    return idxes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f639f7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First flatten to 1-d\n",
    "if np.prod(prob_sca.shape[1:]) < beam_size:\n",
    "    beam_size_sca = np.prod(prob_sca.shape[1:])\n",
    "else:\n",
    "    beam_size_sca = beam_size\n",
    "# Now as sc_idx is already sorted, re-map them properly.\n",
    "# idxes: [sc_beam_idx, sa_idx] -> sca_idxes: [sc_idx, sa_idx]\n",
    "idxes, values = topk_multi_dim(prob_sca.detach().cpu(), n_topk=beam_size_sca)\n",
    "sc_beam_idxes = idxes[:, :, 0]\n",
    "sc_idxes = np.stack([pr_sc_beam.numpy()[i, sc_beam_idx] for i, sc_beam_idx in enumerate(sc_beam_idxes)])\n",
    "sca_idxes = np.stack([sc_idxes, idxes[:, :, 1]]).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7756be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[14,  0],\n",
       "        [18,  0],\n",
       "        [19,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 6,  0],\n",
       "        [ 3,  0],\n",
       "        [ 4,  0],\n",
       "        [ 5,  0]]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca_idxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a2f54",
   "metadata": {},
   "source": [
    "writing ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b900b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
