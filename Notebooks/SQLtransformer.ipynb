{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ce73f2",
   "metadata": {},
   "source": [
    "# TEXT2SQL with transformers\n",
    "\n",
    "Lee Woo Chul, Jang Ji Soo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240a0f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e116ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os\n",
    "\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")\n",
    "\n",
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9eafa",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "`NLSQL.jsonl` and `\"table.jsonl` contains the data like following format same with [WikiSQL](https://github.com/salesforce/WikiSQL), Please follow the [link](https://github.com/salesforce/WikiSQL#content-and-format) to see what are the keys mean.\n",
    "\n",
    "```json\n",
    "// example of 'NLSQL.jsonl'\n",
    "{\n",
    "    \"phase\": 1, \n",
    "    \"question\": \"2015 삼성전자 유동자산은 어떻게 돼?\", \n",
    "    \"table_id\": \"receipts\", \n",
    "    \"sql\": {\n",
    "        \"sel\": 16, \n",
    "        \"agg\": 0, \n",
    "        \"conds\": [[10, 0, \"유동자산\"], [3, 0, 2016]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9756437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sql_path, table_path):\n",
    "    path_sql = Path(sql_path)\n",
    "    path_table = Path(table_path)\n",
    "\n",
    "    dataset = []\n",
    "    table = {}\n",
    "    with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            dataset.append(x)\n",
    "\n",
    "    with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            table[x['id']] = x\n",
    "            \n",
    "    return dataset, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1effb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, table = load_data(\"./olddata/NLSQL.jsonl\", \"./olddata/table.jsonl\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    dataset=data,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: x # now dictionary values are not merged!\n",
    ")\n",
    "# Load DBEngine\n",
    "db_path = Path(\"./private\")\n",
    "dbengine = DBEngine(db_path / \"samsung_new.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf9bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, table = load_data(\"./newdata/train/train.jsonl\", \"./newdata/train/train_table.jsonl\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    dataset=data,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: x # now dictionary values are not merged!\n",
    ")\n",
    "# Load DBEngine\n",
    "db_path = Path(\"./private\")\n",
    "dbengine = DBEngine(db_path / \"kospi.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba7bae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b354f3974e46d9986423a941ec6cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test with toy data:   0%|          | 0/231432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch_data in enumerate(tqdm(data_loader, desc=\"Test with toy data\")):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2b5e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phase': 1,\n",
       "  'question': '제 54 기 004140의 자본총계는 어떻게 돼?',\n",
       "  'table_id': '004140',\n",
       "  'sql': {'sel': 13, 'agg': 0, 'conds': [[11, 0, '제 54 기'], [10, 0, '자본총계']]}},\n",
       " {'phase': 1,\n",
       "  'question': '제 47 기 009580의 유동부채는 얼마야?',\n",
       "  'table_id': '009580',\n",
       "  'sql': {'sel': 13, 'agg': 0, 'conds': [[10, 0, '유동부채'], [11, 0, '제 47 기']]}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62003235-e37c-4672-84e4-97f4a10aa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = [\n",
    "    {\"phase\": 1, \"question\": \"000020의 제 5 기는 언제를 말하는 거야?\", \n",
    "     \"table_id\": \"000020\", \"sql\": {\"sel\": 12, \"agg\": 0, \"conds\": [[11, 0, \"제 5 기\"]]}},\n",
    "    {\"phase\": 1, \"question\": \"000210 회사의 CEO는 누구야?\", \n",
    "     \"table_id\": \"company\", \"sql\": {\"sel\": 5, \"agg\": 0, \"conds\": [[4, 0, \"000210\"]]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab7f6e",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Used BERT in hugging Face with KoBERT\n",
    "\n",
    "- https://github.com/SKTBrain/KoBERT\n",
    "- https://github.com/monologg/KoBERT-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6496b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert(model_path: str, output_hidden_states: bool=False):\n",
    "    special_tokens = [\"[S]\", \"[E]\", \"[COL]\"] # sequence start, sequence end, column tokens\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    config = BertConfig.from_pretrained(model_path)\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab52e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"monologg/kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "model_bert, tokenizer_bert, config_bert = get_bert(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7586970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch_data(data, dbengine):\n",
    "#     batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "#     tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "#     batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "#     batch_ts = []\n",
    "#     for table_id in tid:\n",
    "#         dbengine.get_schema_info(table_id)\n",
    "#         table_str = f\"{table_id}\" + \"\".join([\n",
    "#             f\"[COL]{col}\" for col in dbengine.schema\n",
    "#         ]) \n",
    "#         batch_ts.append(table_str)\n",
    "    \n",
    "#     return batch_qs, batch_sqls, batch_ts\n",
    "\n",
    "def get_batch_data(data: List[Dict[str, Any]], table: Dict[str, Dict[str, List[Any]]], start_tkn=\"[S]\", end_tkn=\"[E]\") -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
    "    batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "    tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "    batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "    batch_ts = []\n",
    "    for table_id in tid:\n",
    "        table_str = f\"{table_id}\" + \"\".join([\n",
    "            f\"[COL]{col}\" for col in table[table_id][\"header\"]\n",
    "        ])\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # table_str = f\"{start_tkn}{table_id}{end_tkn}\" + \"\".join([\n",
    "        #     f\"{col_tkn}{start_tkn}{col}{end_tkn}\" for col in dbengine.schema\n",
    "        # ]) \n",
    "        batch_ts.append(table_str)\n",
    "\n",
    "    return batch_qs, batch_ts, batch_sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15e4d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs, batch_ts, batch_sqls = get_batch_data(batch_data, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "568b265e-05f0-45d3-96c0-0a0e03322299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000020[COL]index[COL]rcept_no[COL]reprt_code[COL]bsns_year[COL]corp_code[COL]stock_code[COL]fs_div[COL]fs_nm[COL]sj_div[COL]sj_nm[COL]account_nm[COL]thstrm_nm[COL]thstrm_dt[COL]thstrm_amount',\n",
       " 'company[COL]index[COL]corp_name[COL]corp_name_eng[COL]stock_name[COL]stock_code[COL]ceo_nm[COL]corp_cls[COL]jurir_no[COL]bizr_no[COL]adres[COL]hm_url[COL]ir_url[COL]phn_no[COL]fax_no[COL]induty_code[COL]est_dt[COL]acc_mt']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86664de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Input\n",
    "encode_inputs = tokenizer_bert(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=256, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "072a927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 000020의 제 5 기는 언제를 말하는 거야?[SEP] 000020 [COL] index [COL] rcept_no [COL] reprt_code [COL] bsns_year [COL] corp_code [COL] stock_code [COL] fs_div [COL] fs_nm [COL] sj_div [COL] sj_nm [COL] account_nm [COL] thstrm_nm [COL] thstrm_dt [COL] thstrm_amount[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "# Show an Example of Input\n",
    "print(tokenizer_bert.decode(encode_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77b5b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encode_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e62582",
   "metadata": {},
   "source": [
    "## Prepare for decoder Inputs: Createing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e795b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_length = encode_inputs[\"input_ids\"].size()\n",
    "sep_tkn_mask = encode_inputs[\"input_ids\"] == tokenizer_bert.sep_token_id\n",
    "start_tkn_id, end_tkn_id, col_tkn_id = tokenizer_bert.additional_special_tokens_ids\n",
    "\n",
    "input_question_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 0, encode_inputs[\"attention_mask\"].bool())\n",
    "input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "db_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 1, encode_inputs[\"attention_mask\"].bool())\n",
    "col_tkn_mask = encode_inputs[\"input_ids\"] == col_tkn_id\n",
    "db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "db_mask = torch.bitwise_xor(db_mask, torch.bitwise_and(sep_tkn_mask, encode_inputs[\"token_type_ids\"] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd442253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000020의 제 5 기는 언제를 말하는 거야? 000210 회사의 CEO는 누구야?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode(x[input_question_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "748943db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000020 index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount company index corp_name corp_name_eng stock_name stock_code ceo_nm corp_cls jurir_no bizr_no adres hm_url ir_url phn_no fax_no induty_code est_dt acc_mt'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode(x[db_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0b435db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode(x[col_tkn_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_mask(encode_inputs):\n",
    "    sep_tkn_mask = encode_inputs[\"input_ids\"] == tokenizer_bert.sep_token_id\n",
    "    col_tkn_id = tokenizer_bert.additional_special_tokens_ids[2]\n",
    "\n",
    "    input_question_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 0, encode_inputs[\"attention_mask\"].bool())\n",
    "    input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "    input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "    db_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 1, encode_inputs[\"attention_mask\"].bool())\n",
    "    col_tkn_mask = encode_inputs[\"input_ids\"] == col_tkn_id\n",
    "    db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "    db_mask = torch.bitwise_xor(db_mask, torch.bitwise_and(sep_tkn_mask, encode_inputs[\"token_type_ids\"] == 1))\n",
    "    \n",
    "    return input_question_mask, db_mask, col_tkn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23b2e99c-0e1a-43db-aa2c-d8dfc2f099fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  517,   79,   79,  127, 7095, 4128,  611, 1258, 5760, 3245, 6116,\n",
       "         1958, 7794,  862, 6844,  258,    3,  517,   79,   79,  127, 8004,  517,\n",
       "          409,  388,  389,  457, 8004,  517,  435,  382,  389,  432,  442,  365,\n",
       "          425,  427, 8004,  517,  437,  432,  435,  442,  365,  385,  388,  389,\n",
       "         8004,  517,  380,  440,  425,  440,  365,  458,  389,  375, 8004,  705,\n",
       "          430,  432,  365,  385,  388,  389, 8004,  517,  441,  427,  382,  415,\n",
       "          365,  385,  388,  389, 8004,  517,  398,  440,  365,  388,  405,  453,\n",
       "         8004,  517,  398,  440,  365,  425,  423, 8004,  517,  440,  414,  365,\n",
       "          388,  405,  453, 8004,  517,  440,  414,  365,  425,  423, 8004,  517,\n",
       "          369,  385,  449,  442,  365,  425,  423, 8004,  517,  444,  441,  435,\n",
       "          423,  365,  425,  423, 8004,  517,  444,  441,  435,  423,  365,  388,\n",
       "          442, 8004,  517,  444,  441,  435,  423,  365,  373,  427,  449,  442,\n",
       "            3,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        [   2,  517,   79,   83,  105, 5156, 7095,  640, 5760, 1528, 6844,  258,\n",
       "            3,  517,  386,  432,  374,  458, 8004,  517,  409,  388,  389,  457,\n",
       "         8004,  705,  430,  432,  365,  425,  373,  389, 8004,  705,  430,  432,\n",
       "          365,  425,  373,  389,  365,  392,  399, 8004,  517,  441,  427,  382,\n",
       "          415,  365,  425,  373,  389, 8004,  517,  441,  427,  382,  415,  365,\n",
       "          385,  388,  389, 8004,  705,  389,  427,  365,  425,  423, 8004,  705,\n",
       "          430,  432,  365,  382,  420,  440, 8004,  517,  414,  450,  411,  365,\n",
       "          425,  427, 8004,  517,  380,  405,  459,  435,  365,  425,  427, 8004,\n",
       "          517,  370,  437,  440, 8004,  517,  401,  423,  365,  450,  420, 8004,\n",
       "          517,  411,  365,  450,  420, 8004,  517,  432,  401,  425,  365,  425,\n",
       "          427, 8004,  517,  398,  367,  457,  365,  425,  427, 8004,  517,  409,\n",
       "          388,  452,  458,  365,  385,  388,  389, 8004,  517,  396,  365,  388,\n",
       "          442, 8004,  517,  369,  382,  365,  423,  442,    3]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_decoder_input_mask(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "#     r\"\"\"\n",
    "#     input should only contains word tokens:\n",
    "#     \"\"\"\n",
    "#     start_tkn_mask = input_ids == start_tkn_id\n",
    "#     end_tkn_mask = input_ids == end_tkn_id\n",
    "#     start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "#     index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "#     return mask.scatter(1, index, False)\n",
    "\n",
    "# def get_input_mask_and_answer(encode_input, tokenizer):\n",
    "#     r\"\"\"\n",
    "#     table -> database table name(id)\n",
    "#     header -> database header\n",
    "    \n",
    "#     returns:\n",
    "#         input_question_mask, input_table_mask, input_header_mask, answer_table_tkns, answer_header_tkns\n",
    "#     \"\"\"\n",
    "#     batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "#     sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "#     start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "    \n",
    "#     input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "#     input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "#     input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "#     db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "#     db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "#     col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "#     db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "#     # split table_mask and header_mask\n",
    "#     input_idx = torch.arange(max_length).repeat(batch_size, 1)\n",
    "#     db_idx = input_idx[db_mask]\n",
    "#     table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "#     table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "#     start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "#     table_end_sep_idx = start_idx[:, 0] - 1\n",
    "#     split_size = torch.stack([\n",
    "#         table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "#     ]).transpose(0, 1)\n",
    "\n",
    "#     # Token idx\n",
    "#     table_tkn_idx, header_tkn_idx = map(\n",
    "#         lambda x: torch.stack(x), \n",
    "#         zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "#     )\n",
    "\n",
    "#     table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "#     # Mask include [S] & [E] tokens\n",
    "#     table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, table_tkn_idx, True)\n",
    "#     header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "#     # For Decoder Input, Maskout [S], [E] for table & header  \n",
    "#     input_table_mask = get_decoder_input_mask(\n",
    "#         encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "#     )\n",
    "#     input_header_mask = get_decoder_input_mask(\n",
    "#         encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "#     )\n",
    "#     # [COL] token mask: this is for attention\n",
    "#     col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "#     input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "#     return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns\n",
    "\n",
    "# input_question_mask, input_table_mask, input_header_mask, input_col_mask = get_input_mask_and_answer(encode_input, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92d497b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_mask, input_db_mask, input_col_mask = get_input_mask(encode_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eaf09900-e0a1-40e0-b8d5-18e6216cb20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 17])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_col_mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b886d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tokens for Decoder\n",
      "-------------------------\n",
      "000020의 제 5 기는 언제를 말하는 거야? 000210 회사의 CEO는 누구야?\n",
      "\n",
      "DB Tokens for Decoder\n",
      "-------------------------\n",
      "000020 index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount company index corp_name corp_name_eng stock_name stock_code ceo_nm corp_cls jurir_no bizr_no adres hm_url ir_url phn_no fax_no induty_code est_dt acc_mt\n",
      "\n",
      "Column(Index of Headers) Tokens for Decoder\n",
      "-------------------------\n",
      "[COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, t in zip(\n",
    "        [input_question_mask, input_db_mask, input_col_mask], \n",
    "        [\"Question Tokens for Decoder\", \"DB Tokens for Decoder\", \"Column(Index of Headers) Tokens for Decoder\"]\n",
    "    ):\n",
    "    print(t)\n",
    "    print(\"-----\"*5)\n",
    "    print(tokenizer_bert.decode(encode_inputs[\"input_ids\"][m]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "461e2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed to BERT Model\n",
    "encode_outputs = model_bert(**encode_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfd750",
   "metadata": {},
   "source": [
    "\n",
    "The `encode_outputs` will be selected by 4 types of masks\n",
    "```\n",
    "encode_outputs\n",
    "-> Question\n",
    "-> Table\n",
    "-> Header\n",
    "-> Column(Index of Headers)\n",
    "```\n",
    "\n",
    "And pad batches which has less tokens than max length with \"\\[PAD\\]\"  for Decoder Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bce1c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batches: Tuple[torch.Tensor], lengths: List[int], model: BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "    padded = []\n",
    "    max_length = max(lengths)\n",
    "    for x in batches:\n",
    "        if len(x) < max_length:\n",
    "            pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "            padded.append(torch.cat([x, pad_tensor]))\n",
    "        else:\n",
    "            padded.append(x)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "def get_decoder_batches(encode_outputs, mask, model, pad_idx):\n",
    "    lengths = mask.sum(1)\n",
    "    tensors = encode_outputs.last_hidden_state[mask, :]\n",
    "    batches = torch.split(tensors, lengths.tolist())\n",
    "    if lengths.ne(lengths.max()).sum() != 0:\n",
    "        # pad not same length tokens\n",
    "        tensors_padded = pad(batches, lengths.tolist(), model, pad_idx=pad_idx)\n",
    "    else:\n",
    "        # just stack the splitted tensors\n",
    "        tensors_padded = torch.stack(batches)\n",
    "    return tensors_padded, lengths.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca761f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded, question_lengths = get_decoder_batches(encode_outputs, input_question_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "db_padded, db_lengths = get_decoder_batches(encode_outputs, input_db_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "col_padded, col_lengths = get_decoder_batches(encode_outputs, input_col_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "471c27c1-b7c5-4dc5-b504-2d1ed12ed92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 768])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_padded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "780b33d2-9d82-4e21-9456-f030494a1bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 17]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6924915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 11], [112, 122], [14, 17])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_lengths, db_lengths, col_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7348ae",
   "metadata": {},
   "source": [
    "## Create the Answers for decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34e74187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_answers(batch_sqls, tokenizer, end_tkn_idx=1):\n",
    "    \"\"\"\n",
    "    for backward compatibility, separated with get_g\n",
    "    \n",
    "    sc: select column\n",
    "    sa: select agg\n",
    "    wn: where number\n",
    "    wc: where column\n",
    "    wo: where operator\n",
    "    wv: where value\n",
    "    \"\"\"\n",
    "\n",
    "    get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, sql_dict in enumerate(batch_sqls):\n",
    "        g_sc.append( sql_dict[\"sel\"] )\n",
    "        g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "        conds = sql_dict[\"conds\"]\n",
    "        if not sql_dict[\"agg\"] < 0:\n",
    "            g_wn.append( len(conds) )\n",
    "            g_wc.append( get_ith_element(conds, 0) )\n",
    "            g_wo.append( get_ith_element(conds, 1) )\n",
    "            g_wv.append( get_ith_element(conds, 2) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    \n",
    "    \n",
    "    # get where value tokenized \n",
    "    end_tkn = tokenizer.additional_special_tokens[end_tkn_idx]\n",
    "    pad_tkn_id = tokenizer.pad_token_id\n",
    "    g_wv_tkns = [[f\"{s}{end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "    g_wv_tkns = [tokenizer(batch_wv, add_special_tokens=False)[\"input_ids\"] if len(batch_wv) > 0 else batch_wv for batch_wv in g_wv_tkns]\n",
    "    # add empty list if batch has different where column number\n",
    "    max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "    g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "    temp = []\n",
    "    for batch_wv in list(zip(*g_wv_tkns)):\n",
    "        batch_max_len = max(map(len, batch_wv))\n",
    "        batch_temp = []\n",
    "        for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "            if len(wv_tkns) < batch_max_len:\n",
    "                batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "            else:\n",
    "                batch_temp.append(wv_tkns)\n",
    "        temp.append(batch_temp)\n",
    "    g_wv_tkns = list(zip(*temp))\n",
    "    g_wv_tkns = list(map(list, g_wv_tkns))\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bb9ec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12, 5],\n",
       " [0, 0],\n",
       " [1, 1],\n",
       " [[11], [4]],\n",
       " [[0], [0]],\n",
       " [['제 5 기'], ['000210']],\n",
       " [[[4128, 611, 1258, 8003, 1]], [[517, 79, 83, 105, 8003]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert, 1)\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959277eb",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Similar structure in SQLova but a little difference in here.\n",
    "\n",
    "- SQLova is a neural semantic parser translating natural language utterance to SQL query.\n",
    "- Official Github: [https://github.com/naver/sqlova](https://github.com/naver/sqlova)\n",
    "- Paper: [A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization](https://arxiv.org/abs/1902.01069)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PW9oAXfW-ZI-jxGn5q9O_gzUIZnNYaet\" alt=\"Sqlova Decoder Architecture \" width=\"50%\" height=\"auto\">\n",
    "\n",
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04cc8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AttentionBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def wipe_out_pad_tkn_score(self, score, lengths, dim=2):\n",
    "        batch_size = score.size(0)\n",
    "        mask = torch.zeros_like(score, dtype=torch.bool)\n",
    "        max_len = max(lengths)\n",
    "        for batch_idx in range(batch_size):\n",
    "            l = lengths[batch_idx]\n",
    "            if l < max_len:\n",
    "                if dim == 2:\n",
    "                    mask[batch_idx, :, l:] = True\n",
    "                elif dim == 1:\n",
    "                    mask[batch_idx, l:, :] = True\n",
    "                else:\n",
    "                    raise ValueError(f\"`dim` in wipe_out_pad_tkn_score should be 1 or 2\")\n",
    "            if l == 0 and dim == 1:\n",
    "                # for 0 where numbers\n",
    "                mask[batch_idx, l:, :] = True\n",
    "        if dim == 2:\n",
    "            score = score.masked_fill(mask, -np.inf)\n",
    "        elif dim == 1:\n",
    "            score = score.masked_fill(mask, 0.0)\n",
    "        else:\n",
    "            raise ValueError(f\"`dim` in wipe_out_pad_tkn_score should be 1 or 2\")\n",
    "        \n",
    "        return score\n",
    "\n",
    "class C2QAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Column to Question Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, o_c, o_q, q_lengths, c_lengths=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each column tokens, How much related to question tokens?\n",
    "        \n",
    "        o_c: LSTM output of column\n",
    "        o_q: LSTM output of question \n",
    "        \n",
    "        c_lengths: wipe out row length\n",
    "        return context atttended to question tokens\n",
    "        \"\"\"\n",
    "        sqrt_H = np.sqrt(o_c.size(-1)) # Apply Attention is All you Need Technique\n",
    "        o_q_transform = self.linear(o_q)  # (B, T_q, H)\n",
    "        score_c2q = torch.bmm(o_c, o_q_transform.transpose(1, 2)) / sqrt_H  # (B, T_c, H) x (B, H, T_q) = (B, T_c, T_q)\n",
    "        score_c2q = self.wipe_out_pad_tkn_score(score_c2q, q_lengths, dim=2)\n",
    "        \n",
    "        prob_c2q = self.softmax(score_c2q)\n",
    "        if c_lengths is not None:\n",
    "            prob_c2q = self.wipe_out_pad_tkn_score(prob_c2q, c_lengths, dim=1)\n",
    "        # prob_c2q: (B, T_c, T_q) -> (B, T_c, T_q, 1)\n",
    "        # o_q: (B, 1, T_q, H)\n",
    "        # p_col2question \\odot o_q = (B, T_c, T_q, 1) \\odot (B, 1, T_q, H) = (B, T_c, T_q, H)\n",
    "        # -> reduce sum to T_q to get context for each column (B, T_c, H)\n",
    "        context = torch.mul(prob_c2q.unsqueeze(3), o_q.unsqueeze(1)).sum(dim=2)\n",
    "        if rt_attn:\n",
    "            attn = prob_c2q\n",
    "        else:\n",
    "            attn = None\n",
    "        return context, attn\n",
    "\n",
    "class SelfAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Self Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, o, lengths, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each o tokens, How much related to o tokens?\n",
    "        \n",
    "        return attended summary of o\n",
    "        \"\"\"\n",
    "        o_transform = self.linear(o)  # (B, T_o, H) -> (B, T_o, 1)\n",
    "        o_transform = self.wipe_out_pad_tkn_score(o_transform, lengths) \n",
    "        o_prob = self.softmax(o_transform)  # (B, T_o, 1)\n",
    "        \n",
    "        o_summary = torch.mul(o, o_prob).sum(1)  # (B, T_o, H) \\odot (B, T_o, 1) -> (B, H)\n",
    "\n",
    "        if rt_attn:\n",
    "            attn = o_prob\n",
    "        else:\n",
    "            attn = None\n",
    "        return o_summary, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d64bff",
   "metadata": {},
   "source": [
    "## Decoder Sub Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f62776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c11f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectDecoder(nn.Module):\n",
    "    r\"\"\"SELECT Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, _ = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "\n",
    "class AggDecoder(nn.Module):\n",
    "    r\"\"\"AGG Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], col_lengths: List[int], select_idxes: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, _ = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        \n",
    "        col_selected = col_context[list(range(batch_size)), select_idxes].unsqueeze(1)  # col_selected: (B, 1, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_selected, o_q, question_lengths, col_lengths, rt_attn)  # (B, 1, H), (B, 1, T_q)\n",
    "        output = self.output_layer(col_q_context.squeeze(1))\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    \n",
    "class WhereNumDecoder(nn.Module):\n",
    "    r\"\"\"WHERE number Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        if self.output_size > self.max_where_conds+1:\n",
    "            # HERE output will be dilivered to cross-entropy loss, not guessing the real number of where clause\n",
    "            raise ValueError(f\"`WhereNumDecoder` only support maximum {max_where_conds} where clause\")\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_self_attn = SelfAttention(2*hidden_size, 1)\n",
    "        self.lstm_q_hidden_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.lstm_q_cell_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        self.context_self_attn = SelfAttention(hidden_size, 1)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "\n",
    "        col_self_attn, col_attn = self.col_self_attn(col_context, col_lengths, rt_attn)  # (B, 2H), (B, T_c)\n",
    "\n",
    "        h_0 = self.lstm_q_hidden_init_linear(col_self_attn)  # (B, 2H)\n",
    "        h_0 = h_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        c_0 = self.lstm_q_cell_init_linear(col_self_attn)  # (B, 2H)\n",
    "        c_0 = c_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        o_q, _ = self.lstm_q(question_padded, (h_0, c_0))  # o_q: (B, T_q, H)\n",
    "        o_summary, o_attn = self.context_self_attn(o_q, question_lengths, rt_attn)  # (B, H), (B, T_q)\n",
    "        output = self.output_layer(o_summary)\n",
    "        \n",
    "        return output, (col_attn, o_attn)\n",
    "\n",
    "    \n",
    "class WhereColumnDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Column Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int=1, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "\n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, _ = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec).squeeze(-1)  # (B, T_c)\n",
    "        for batch_idx, c_len in enumerate(col_lengths):\n",
    "            output[batch_idx, c_len:] = -1e10\n",
    "        return output, attn\n",
    "    \n",
    "    \n",
    "class WhereOpDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Opperator Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is settled at WhereColumnDecoder, but it can be lower than or equal to `max_where_conds`\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, _ = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        # (B, max_where_col_nums, H), (B,)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_col_idxes)\n",
    "        \n",
    "        # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  \n",
    "        vec = torch.cat([col_q_context, col_context_padded], dim=2)  # (B, max_where_col_nums, 2H)\n",
    "        output = self.output_layer(vec)  # (B, max_where_col_nums, n_cond_ops)\n",
    "\n",
    "        return output, attn\n",
    "        \n",
    "    def get_context_padded(self, col_context, where_col_idxes):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = self.max_where_conds # max(where_nums) \n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                # self.register_buffer(\"pad_zeros_context\", torch.zeros((max_where_col_nums-where_col_nums), col_context.size(2)))\n",
    "                # b_padded = torch.cat([b, self.pad_zeros_context.type_as(col_context)], dim=0)\n",
    "                # Use Register Buffer to code following code for PyTroch Lightning\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), col_context.size(2)).type_as(col_context).contiguous()], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "\n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    \n",
    "class WhereValueDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Value Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4, n_cond_ops: int=4,\n",
    "                 start_tkn_id=8002, end_tkn_id=8003, embedding_layer=None) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        \n",
    "        self.start_tkn_id = start_tkn_id\n",
    "        self.end_tkn_id = end_tkn_id\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.where_op_linear = nn.Linear(n_cond_ops, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        if embedding_layer is None:\n",
    "            raise KeyError(\"Must initialize the embedding_layer to BertModel's word embedding layer\")\n",
    "        else:\n",
    "            if not isinstance(embedding_layer, torch.nn.modules.sparse.Embedding):\n",
    "                embedding_layer = embedding_layer.word_embeddings\n",
    "            self.embedding_layer = embedding_layer\n",
    "            vocab_size, bert_hidden_size = embedding_layer.weight.data.size()\n",
    "            self.output_lstm_hidden_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm_cell_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm = nn.LSTM(bert_hidden_size, bert_hidden_size, 1, batch_first=True)\n",
    "            self.output_linear = nn.Linear(bert_hidden_size, vocab_size)\n",
    "            self.output_linear.weight.data = embedding_layer.weight.data\n",
    "        \n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], where_op_idxes: List[List[int]], value_tkn_max_len=None, g_wv_tkns=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is setted at WhereColumnDecoder\n",
    "        value_tkn_max_len = Train if None else Test\n",
    "        g_wv_tkns = When Train should not be None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, _ = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, _ = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        _, (h_h, _) = self.lstm_h(db_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).expand(batch_size, n_col, -1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        where_op_one_hot_padded = self.get_where_op_one_hot_padded(where_op_idxes, where_nums, device=col_context.device)#.type_as(col_padded)  # (B, max_where_col_nums, n_cond_ops)\n",
    "\n",
    "        where_op = self.where_op_linear(where_op_one_hot_padded)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        vec = torch.cat([col_q_context, col_context_padded, where_op], dim=2)  # (B, max_where_col_nums, 3H)\n",
    "        \n",
    "        # predict each where_col\n",
    "        total_scores = []\n",
    "        # max_where_col_nums = vec.size(1)\n",
    "        # for i in range(max_where_col_nums):\n",
    "        if g_wv_tkns is not None:\n",
    "            g_wv_tkns = deepcopy(g_wv_tkns)\n",
    "            g_max_len_where_num = len(list(zip(*g_wv_tkns)))\n",
    "            if g_max_len_where_num < self.max_where_conds:\n",
    "                for b in range(batch_size):\n",
    "                    g_wv_tkns[b].extend([[self.end_tkn_id]]*(self.max_where_conds-g_max_len_where_num))\n",
    "\n",
    "        for i in range(self.max_where_conds):\n",
    "            g_wv_tkns_i = torch.LongTensor([g_wv_tkns[b_idx][i] for b_idx in range(batch_size)]).to(col_padded.device) if g_wv_tkns is not None else None  # (B, T_d_i)\n",
    "            vec_i = vec[:, i, :]  # (B, 3H)\n",
    "            \n",
    "            h_0 = self.output_lstm_hidden_init_linear(vec_i).unsqueeze(1).transpose(0, 1)  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            c_0 = self.output_lstm_cell_init_linear(vec_i).unsqueeze(1).transpose(0, 1)  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            \n",
    "            scores = self.decode_single_where_col(batch_size, h_0, c_0, value_tkn_max_len=value_tkn_max_len, g_wv_tkns_i=g_wv_tkns_i, device=col_padded.device)  # (B, T_d_i, vocab_size)\n",
    "            total_scores.append(scores)\n",
    "        \n",
    "        # total_scores: [(B, T_d_i, vocab_size)] x max_where_col_nums\n",
    "        return total_scores, attn\n",
    "    \n",
    "    def start_token(self, batch_size, device):\n",
    "        sos = torch.LongTensor([self.start_tkn_id]*batch_size).unsqueeze(1).to(device).contiguous()  # (B, 1)\n",
    "        return sos\n",
    "    \n",
    "    def decode_single_where_col(self, batch_size, h_0, c_0, value_tkn_max_len=None, g_wv_tkns_i=None, device: str=\"cpu\"):\n",
    "        if value_tkn_max_len is None:\n",
    "            # [Training] set the max length to gold token max length (already padded)\n",
    "            max_len = len(g_wv_tkns_i[0])\n",
    "        else:\n",
    "            # [Testing]  don't know the max length\n",
    "            max_len = value_tkn_max_len\n",
    "            \n",
    "        # Version2: left_batch_size = batch_size\n",
    "        \n",
    "        sos = self.start_token(batch_size, device)  # (B, 1)\n",
    "        emb = self.embedding_layer(sos)  # (B, 1, bert_H)\n",
    "        scores = [] \n",
    "        for i in range(max_len):\n",
    "            _, (h, _) = self.output_lstm(emb, (h_0, c_0))  # h: (1, B, bert_H)  \n",
    "            s = self.output_linear(h[-1, :]) # select last layer if use multiple rnn layers, h: (1, B, bert_H) -> (B, bert_H) -> s: (B, vocab_size)\n",
    "            scores.append(s)\n",
    "            if g_wv_tkns_i is not None:\n",
    "                # [Training] Teacher Force model\n",
    "                pred = g_wv_tkns_i[:, i]  # (B, )\n",
    "            else:\n",
    "                # [Testing]\n",
    "                pred = s.argmax(1)  # (B, )\n",
    "                if (pred == self.end_tkn_id).sum() == batch_size:  # all stop\n",
    "                    break\n",
    "                # Version2: Seperate all tokens\n",
    "                # if (pred == dd.end_tkn_id).sum() == left_batch_size:  # all stop\n",
    "                #     scores.append(s)\n",
    "                #     break\n",
    "                # else:\n",
    "                #     stop_mask = pred == dd.end_tkn_id\n",
    "                #     pred = pred[~stop_mask]\n",
    "                #     scores.append(pred)\n",
    "                #     left_batch_size -= stop_mask.sum().item()\n",
    "                    \n",
    "            emb = self.embedding_layer(pred.unsqueeze(1))  # (B, 1, bert_H)\n",
    "            \n",
    "        return torch.stack(scores).transpose(0, 1).contiguous() # (T_d_i, B, vocab_size) -> (B, T_d_i, vocab_size)\n",
    "        \n",
    "    def get_context_padded(self, col_context: torch.Tensor, where_col_idxes: List[List[int]]):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = self.max_where_conds # max(where_nums)\n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                # self.register_buffer(\"pad_zeros_context\", torch.zeros((max_where_col_nums-where_col_nums), col_context.size(2)))\n",
    "                # b_padded = torch.cat([b, self.pad_zeros_context.type_as(col_context)], dim=0)\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), col_context.size(2)).type_as(col_context).contiguous()], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    def get_where_op_one_hot_padded(self, where_op_idxes: List[List[int]], where_nums: List[int], device=\"cpu\"):\n",
    "        r\"\"\"\n",
    "        Turn where operation indexs into one hot encoded vectors\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = self.max_where_conds # max(where_nums)\n",
    "        # batches = []\n",
    "        # for where_num, batch_col in zip(where_nums, where_op_idxes):\n",
    "        #     scatter_idx = torch.LongTensor(batch_col).unsqueeze(1)\n",
    "        #     batch = torch.zeros(where_num, self.n_cond_ops).scatter(1, scatter_idx, 1)\n",
    "        #     batches.append(batch)\n",
    "        batches = [\n",
    "            torch.zeros(where_num, self.n_cond_ops).scatter(1, torch.LongTensor(batch_col).unsqueeze(1), 1).to(device).contiguous() \n",
    "            for where_num, batch_col in zip(where_nums, where_op_idxes)\n",
    "        ]  \n",
    "        # batches = [(where_col_nums, n_cond_ops), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                # self.register_buffer(\"pad_zeros_where_op\", torch.zeros((max_where_col_nums-where_col_nums), self.n_cond_ops))\n",
    "                # b_padded = torch.cat([b, self.pad_zeros_where_op], dim=0)\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), self.n_cond_ops, device=device).contiguous()], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418a737",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08629256",
   "metadata": {},
   "source": [
    "## Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0600431",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7c41282",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_bert.hidden_size\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "dropout_ratio = 0.3\n",
    "max_where_conds = 4\n",
    "n_agg_ops = len(dbengine.agg_ops)\n",
    "n_cond_ops = len(dbengine.cond_ops)\n",
    "start_tkn_id = tokenizer_bert.additional_special_tokens_ids[0]\n",
    "end_tkn_id = tokenizer_bert.additional_special_tokens_ids[1]\n",
    "embedding_layer = model_bert.embeddings.word_embeddings\n",
    "train = True\n",
    "if train:\n",
    "    value_tkn_max_len = None\n",
    "else:\n",
    "    value_tkn_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f78b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.select_decoder = SelectDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.agg_decoder = AggDecoder(\n",
    "            input_size, hidden_size, output_size=n_agg_ops, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_num_decoder = WhereNumDecoder(\n",
    "            input_size, hidden_size, output_size=(max_where_conds+1), num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_col_decoder = WhereColumnDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_op_decoder = WhereOpDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_value_decoder = WhereValueDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds, \n",
    "            n_cond_ops=n_cond_ops, start_tkn_id=start_tkn_id, end_tkn_id=end_tkn_id, embedding_layer=embedding_layer\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, question_padded, db_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=None, gold=None, rt_attn=False):\n",
    "        \"\"\"\n",
    "        # Outputs Size\n",
    "        # sc = (B, T_c)\n",
    "        # sa = (B, n_agg_ops)\n",
    "        # wn = (B, max_where_conds+1)\n",
    "        # wc = (B, T_c): binary\n",
    "        # wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        # wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        if gold is None:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = [None] * 6\n",
    "        else:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "\n",
    "        select_outputs, attn_c2q_sc = self.select_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, rt_attn)\n",
    "        select_idxes = g_sc if g_sc else self.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "        agg_outputs, attn_c2q_sa = self.agg_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, select_idxes, rt_attn)\n",
    "\n",
    "        where_num_outputs, (attn_self_col_wn, attn_self_cntxt_wn)  = self.where_num_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, rt_attn)\n",
    "        where_nums = g_wn if g_wn else self.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "        where_col_outputs, attn_c2q_wc = self.where_col_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, rt_attn)\n",
    "        where_col_idxes = g_wc if g_wc else self.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_op_outputs, attn_c2q_wo = self.where_op_decoder(question_padded, db_padded, col_padded, question_lengths, where_nums, where_col_idxes, rt_attn)\n",
    "        where_op_idxes = g_wo if g_wo else self.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_value_outputs, attn_c2q_wv = self.where_value_decoder(\n",
    "            question_padded, db_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns, rt_attn\n",
    "        )\n",
    "        \n",
    "        decoder_outputs = {\n",
    "            \"sc\": select_outputs,  # cross entropy\n",
    "            \"sa\": agg_outputs,  # cross entropy\n",
    "            \"wn\": where_num_outputs,  # cross entropy\n",
    "            \"wc\": where_col_outputs,  # binary cross entropy\n",
    "            \"wo\": where_op_outputs,  # cross entropy\n",
    "            \"wv\": where_value_outputs  # cross entropy\n",
    "        }\n",
    "        \n",
    "        decoder_attns = {\n",
    "            \"sc\": attn_c2q_sc,  # cross entropy\n",
    "            \"sa\": attn_c2q_sa,  # cross entropy\n",
    "            \"wn\": (attn_self_col_wn, attn_self_cntxt_wn),  # cross entropy\n",
    "            \"wc\": attn_c2q_wc,  # binary cross entropy\n",
    "            \"wo\": attn_c2q_wo,  # cross entropy\n",
    "            \"wv\": attn_c2q_wv  # cross entropy\n",
    "        }\n",
    "        \n",
    "        return decoder_outputs, decoder_attns\n",
    "        \n",
    "    def predict_decoder(self, typ, **kwargs):\n",
    "        r\"\"\"\n",
    "        if not using teacher force model will use this function to predict answer\n",
    "        # Outputs Size\n",
    "        # sc = (B, T_c)\n",
    "        # sa = (B, n_agg_ops)\n",
    "        # wn = (B, max_where_conds+1)\n",
    "        # wc = (B, T_c): binary\n",
    "        # wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        # wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        if typ == \"sc\":  # SELECT column\n",
    "            select_outputs = kwargs[\"select_outputs\"]\n",
    "            return select_outputs.argmax(1).tolist()\n",
    "        elif typ == \"sa\":  # SELECT aggregation operator\n",
    "            agg_outputs = kwargs[\"agg_outputs\"]\n",
    "            return agg_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wn\":  # WHERE number\n",
    "            where_num_outputs = kwargs[\"where_num_outputs\"]\n",
    "            return where_num_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wc\":  # WHERE clause column\n",
    "            where_col_outputs = kwargs[\"where_col_outputs\"]\n",
    "            where_col_argsort = torch.sigmoid(where_col_outputs).argsort(1)\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_col_idxes = [where_col_argsort[b_idx, :w_num].tolist() for b_idx, w_num in enumerate(where_nums)]\n",
    "            return where_col_idxes\n",
    "        elif typ == \"wo\":  # WHERE clause operator\n",
    "            where_op_outputs = kwargs[\"where_op_outputs\"]\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_op_idxes = []\n",
    "            for b_idx, w_num in enumerate(where_nums):\n",
    "                if w_num == 0:  # means no where number\n",
    "                    where_op_idxes.append([])\n",
    "                else:\n",
    "                    where_op_idxes.append(where_op_outputs.argmax(2)[b_idx, :w_num].tolist())\n",
    "            return where_op_idxes\n",
    "        elif typ == \"wv\":  # WHERE clause value\n",
    "            where_value_outputs = kwargs[\"where_value_outputs\"]\n",
    "            return [o.argmax(2).tolist() for o in where_value_outputs]  # iter with each where clause\n",
    "        else:\n",
    "            raise KeyError(\"`typ` must be in ['sc', 'sa', 'wn', 'wc', 'wo', 'wv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a87f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts = {}\n",
    "# predicts[\"sc\"] = model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "# predicts[\"sa\"] = model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "# predicts[\"wn\"] = model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "# predicts[\"wc\"] = model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "# predicts[\"wo\"] = model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "# predicts[\"wv\"] = model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0cf149ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decoder = Decoder(input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01870f",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "731e66ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4128, 611, 1258, 8003, 1]], [[517, 79, 83, 105, 8003]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[-0.0123, -0.0051, -0.0051, -0.0003, -0.0024, -0.0036,  0.0030, -0.0023,\n",
       "          -0.0014, -0.0021, -0.0008, -0.0059, -0.0061, -0.0042,  0.0253,  0.0232,\n",
       "           0.0237],\n",
       "         [-0.0145, -0.0185, -0.0126, -0.0099, -0.0048, -0.0154, -0.0154, -0.0084,\n",
       "          -0.0106, -0.0078, -0.0067, -0.0103, -0.0139, -0.0229, -0.0246, -0.0226,\n",
       "          -0.0215]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[ 0.0297,  0.0832, -0.0316, -0.1003,  0.0522, -0.0596],\n",
       "         [ 0.0017,  0.0792, -0.0260, -0.0826,  0.0799, -0.0615]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[ 0.0416, -0.0566,  0.0300, -0.1083, -0.0159],\n",
       "         [ 0.0359, -0.0532,  0.0313, -0.1002, -0.0226]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[-6.8857e-02, -6.9766e-02, -7.2719e-02, -7.0426e-02, -6.7729e-02,\n",
       "          -7.0161e-02, -7.2101e-02, -7.2078e-02, -7.5271e-02, -7.1060e-02,\n",
       "          -7.2412e-02, -7.7091e-02, -7.5863e-02, -7.6326e-02, -1.0000e+10,\n",
       "          -1.0000e+10, -1.0000e+10],\n",
       "         [-5.4399e-02, -5.0122e-02, -5.0165e-02, -5.7158e-02, -5.4450e-02,\n",
       "          -5.3785e-02, -5.6904e-02, -6.1537e-02, -6.3887e-02, -5.9811e-02,\n",
       "          -6.1237e-02, -6.0078e-02, -6.2473e-02, -6.5535e-02, -6.2669e-02,\n",
       "          -6.2604e-02, -6.3670e-02]], grad_fn=<AsStridedBackward>),\n",
       " 'wo': tensor([[[ 0.1154,  0.1145, -0.0115,  0.0333],\n",
       "          [ 0.0927,  0.1049, -0.0159,  0.0385],\n",
       "          [ 0.0927,  0.1049, -0.0159,  0.0385],\n",
       "          [ 0.0927,  0.1049, -0.0159,  0.0385]],\n",
       " \n",
       "         [[ 0.1215,  0.1420, -0.0102,  0.0393],\n",
       "          [ 0.0920,  0.1230, -0.0122,  0.0382],\n",
       "          [ 0.0920,  0.1230, -0.0122,  0.0382],\n",
       "          [ 0.0920,  0.1230, -0.0122,  0.0382]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[-1.8843e-02,  1.0053e-01,  5.4126e-03,  ..., -3.0438e-03,\n",
       "            -3.7335e-02, -5.3672e-03],\n",
       "           [-1.9310e-02,  6.6888e-02,  9.5742e-03,  ..., -7.6137e-03,\n",
       "            -3.9941e-02, -1.3976e-02],\n",
       "           [-2.7991e-02,  1.0868e-01,  5.5710e-03,  ..., -4.0589e-03,\n",
       "            -4.0325e-02, -8.6588e-03],\n",
       "           [-3.2752e-02,  7.8239e-02, -7.8302e-05,  ..., -4.7745e-03,\n",
       "            -3.8147e-02, -4.7895e-03],\n",
       "           [-1.8489e-02,  8.7233e-02,  4.8544e-03,  ..., -6.6772e-03,\n",
       "            -4.1388e-02, -6.1779e-03]],\n",
       "  \n",
       "          [[-3.1375e-02,  1.1501e-01,  3.6035e-03,  ..., -4.5291e-03,\n",
       "            -3.7827e-02, -5.8901e-03],\n",
       "           [-4.0783e-02,  1.0712e-01, -4.6460e-03,  ..., -4.3598e-03,\n",
       "            -3.6439e-02, -8.2740e-03],\n",
       "           [-3.8140e-02,  9.2754e-02, -5.1047e-05,  ..., -7.7920e-03,\n",
       "            -4.2912e-02, -3.2523e-03],\n",
       "           [-4.6925e-02,  1.3102e-01,  6.8833e-03,  ..., -3.0897e-03,\n",
       "            -4.3840e-02, -7.7065e-03],\n",
       "           [-4.7948e-02,  1.2973e-01,  6.1135e-03,  ..., -2.3794e-03,\n",
       "            -4.4098e-02, -2.4982e-03]]], grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-0.0155,  0.0054,  0.0024,  ..., -0.0088, -0.0214, -0.0220]],\n",
       "  \n",
       "          [[-0.0241,  0.0174,  0.0041,  ..., -0.0107, -0.0222, -0.0222]]],\n",
       "         grad_fn=<TransposeBackward0>),\n",
       "  tensor([[[-0.0155,  0.0054,  0.0024,  ..., -0.0088, -0.0214, -0.0220]],\n",
       "  \n",
       "          [[-0.0241,  0.0174,  0.0041,  ..., -0.0107, -0.0222, -0.0222]]],\n",
       "         grad_fn=<TransposeBackward0>),\n",
       "  tensor([[[-0.0155,  0.0054,  0.0024,  ..., -0.0088, -0.0214, -0.0220]],\n",
       "  \n",
       "          [[-0.0241,  0.0174,  0.0041,  ..., -0.0107, -0.0222, -0.0222]]],\n",
       "         grad_fn=<TransposeBackward0>)]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert)\n",
    "gold = g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns\n",
    "print(g_wv_tkns)\n",
    "decoder_outputs, _ = model_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=None, gold=gold)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df901494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc torch.Size([2, 17])\n",
      "sa torch.Size([2, 6])\n",
      "wn torch.Size([2, 5])\n",
      "wc torch.Size([2, 17])\n",
      "wo torch.Size([2, 4, 4])\n",
      "wv\n",
      "torch.Size([2, 5, 8005])\n",
      "torch.Size([2, 1, 8005])\n",
      "torch.Size([2, 1, 8005])\n",
      "torch.Size([2, 1, 8005])\n"
     ]
    }
   ],
   "source": [
    "for k, v in decoder_outputs.items():\n",
    "    if k != \"wv\":\n",
    "        print(k, v.size())\n",
    "    else:\n",
    "        print(k)\n",
    "        for x in v:\n",
    "            print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0d3f4",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "053732c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[0.0390, 0.0438, 0.0391, 0.0380, 0.0396, 0.0396, 0.0380, 0.0378, 0.0449,\n",
       "          0.0395, 0.0431, 0.0400, 0.0420, 0.0456],\n",
       "         [0.0324, 0.0298, 0.0219, 0.0226, 0.0258, 0.0242, 0.0236, 0.0264, 0.0310,\n",
       "          0.0268, 0.0317, 0.0341, 0.0349, 0.0346]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[ 0.0699, -0.0111,  0.1538,  0.0446, -0.0945,  0.1153],\n",
       "         [ 0.0781, -0.0354,  0.1336,  0.0508, -0.1009,  0.1255]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[ 0.0413, -0.0732, -0.0241,  0.0776, -0.0039],\n",
       "         [ 0.0092, -0.0492, -0.0416,  0.0718, -0.0115]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[-0.0593, -0.0559, -0.0552, -0.0561, -0.0559, -0.0527, -0.0498, -0.0534,\n",
       "          -0.0565, -0.0523, -0.0527, -0.0514, -0.0522, -0.0537],\n",
       "         [-0.0675, -0.0684, -0.0705, -0.0679, -0.0674, -0.0680, -0.0655, -0.0643,\n",
       "          -0.0600, -0.0605, -0.0617, -0.0615, -0.0620, -0.0631]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[-0.0037,  0.0387, -0.0475,  0.0311],\n",
       "          [ 0.0015,  0.0335, -0.0495,  0.0453],\n",
       "          [ 0.0030,  0.0403, -0.0476,  0.0393],\n",
       "          [ 0.0254,  0.0224, -0.0293,  0.0112]],\n",
       " \n",
       "         [[ 0.0084,  0.0309, -0.0622,  0.0518],\n",
       "          [ 0.0047,  0.0339, -0.0617,  0.0435],\n",
       "          [ 0.0131,  0.0290, -0.0622,  0.0468],\n",
       "          [ 0.0351,  0.0171, -0.0383,  0.0151]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[-0.0501, -0.1533, -0.0104,  ..., -0.0376,  0.0365,  0.0110],\n",
       "           [-0.0351, -0.1678, -0.0159,  ..., -0.0329,  0.0389,  0.0067],\n",
       "           [-0.0351, -0.1678, -0.0159,  ..., -0.0329,  0.0389,  0.0067],\n",
       "           ...,\n",
       "           [-0.0351, -0.1678, -0.0159,  ..., -0.0329,  0.0389,  0.0067],\n",
       "           [-0.0351, -0.1678, -0.0159,  ..., -0.0329,  0.0389,  0.0067],\n",
       "           [-0.0351, -0.1678, -0.0159,  ..., -0.0329,  0.0389,  0.0067]],\n",
       "  \n",
       "          [[-0.1246, -0.0901, -0.0102,  ...,  0.0018,  0.0209, -0.0132],\n",
       "           [-0.1100, -0.0738, -0.0229,  ...,  0.0065,  0.0173, -0.0039],\n",
       "           [-0.1229, -0.0723,  0.0008,  ...,  0.0111,  0.0209, -0.0082],\n",
       "           ...,\n",
       "           [-0.1229, -0.0723,  0.0008,  ...,  0.0111,  0.0209, -0.0082],\n",
       "           [-0.1229, -0.0723,  0.0008,  ...,  0.0111,  0.0209, -0.0082],\n",
       "           [-0.1229, -0.0723,  0.0008,  ...,  0.0111,  0.0209, -0.0082]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-0.1280, -0.0836, -0.0121,  ..., -0.0050,  0.0230, -0.0102],\n",
       "           [-0.1260, -0.0660, -0.0011,  ...,  0.0044,  0.0230, -0.0053],\n",
       "           [-0.1260, -0.0660, -0.0011,  ...,  0.0044,  0.0230, -0.0053],\n",
       "           ...,\n",
       "           [-0.1260, -0.0660, -0.0011,  ...,  0.0044,  0.0230, -0.0053],\n",
       "           [-0.1260, -0.0660, -0.0011,  ...,  0.0044,  0.0230, -0.0053],\n",
       "           [-0.1260, -0.0660, -0.0011,  ...,  0.0044,  0.0230, -0.0053]],\n",
       "  \n",
       "          [[-0.1250, -0.0905, -0.0106,  ...,  0.0016,  0.0201, -0.0117],\n",
       "           [-0.1104, -0.0743, -0.0233,  ...,  0.0063,  0.0165, -0.0024],\n",
       "           [-0.1235, -0.0728,  0.0005,  ...,  0.0109,  0.0201, -0.0067],\n",
       "           ...,\n",
       "           [-0.1235, -0.0728,  0.0005,  ...,  0.0109,  0.0201, -0.0067],\n",
       "           [-0.1235, -0.0728,  0.0005,  ...,  0.0109,  0.0201, -0.0067],\n",
       "           [-0.1235, -0.0728,  0.0005,  ...,  0.0109,  0.0201, -0.0067]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-4.9944e-02, -1.5722e-01, -1.0206e-02,  ..., -3.9656e-02,\n",
       "             3.5973e-02,  1.0918e-02],\n",
       "           [-5.3018e-02, -1.8282e-01, -1.3616e-02,  ..., -3.7705e-02,\n",
       "             3.5456e-02,  1.4520e-02],\n",
       "           [-3.5017e-02, -1.7169e-01, -1.5742e-02,  ..., -3.4992e-02,\n",
       "             3.8364e-02,  6.4817e-03],\n",
       "           ...,\n",
       "           [-3.5017e-02, -1.7169e-01, -1.5742e-02,  ..., -3.4992e-02,\n",
       "             3.8364e-02,  6.4817e-03],\n",
       "           [-3.5017e-02, -1.7169e-01, -1.5742e-02,  ..., -3.4992e-02,\n",
       "             3.8364e-02,  6.4817e-03],\n",
       "           [-3.5017e-02, -1.7169e-01, -1.5742e-02,  ..., -3.4992e-02,\n",
       "             3.8364e-02,  6.4817e-03]],\n",
       "  \n",
       "          [[-1.2368e-01, -9.0512e-02, -1.0034e-02,  ...,  1.0321e-04,\n",
       "             1.8834e-02, -1.1363e-02],\n",
       "           [-1.0899e-01, -7.4286e-02, -2.2784e-02,  ...,  4.8300e-03,\n",
       "             1.5210e-02, -2.1091e-03],\n",
       "           [-1.2203e-01, -7.2689e-02,  9.9661e-04,  ...,  9.4575e-03,\n",
       "             1.8863e-02, -6.3452e-03],\n",
       "           ...,\n",
       "           [-1.2203e-01, -7.2689e-02,  9.9661e-04,  ...,  9.4575e-03,\n",
       "             1.8863e-02, -6.3452e-03],\n",
       "           [-1.2285e-01, -1.2352e-01, -1.2535e-02,  ..., -3.8827e-03,\n",
       "             2.8125e-02, -1.4484e-02],\n",
       "           [-1.0899e-01, -7.4286e-02, -2.2784e-02,  ...,  4.8300e-03,\n",
       "             1.5210e-02, -2.1091e-03]]], grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-4.3564e-02, -3.8079e-02, -7.9343e-03,  ..., -3.7057e-03,\n",
       "             2.4338e-02,  3.5490e-04],\n",
       "           [-4.7132e-02, -4.2671e-02, -1.4685e-02,  ...,  1.1269e-05,\n",
       "             2.4036e-02,  3.3160e-03],\n",
       "           [-4.7132e-02, -4.2671e-02, -1.4685e-02,  ...,  1.1269e-05,\n",
       "             2.4036e-02,  3.3160e-03],\n",
       "           ...,\n",
       "           [-4.7132e-02, -4.2671e-02, -1.4685e-02,  ...,  1.1269e-05,\n",
       "             2.4036e-02,  3.3160e-03],\n",
       "           [-4.7132e-02, -4.2671e-02, -1.4685e-02,  ...,  1.1269e-05,\n",
       "             2.4036e-02,  3.3160e-03],\n",
       "           [-4.7132e-02, -4.2671e-02, -1.4685e-02,  ...,  1.1269e-05,\n",
       "             2.4036e-02,  3.3160e-03]],\n",
       "  \n",
       "          [[-4.3580e-02, -4.6535e-02, -8.0982e-03,  ..., -1.0509e-03,\n",
       "             2.4723e-02, -2.6619e-03],\n",
       "           [-4.7207e-02, -5.0951e-02, -1.4882e-02,  ...,  2.6372e-03,\n",
       "             2.4347e-02,  2.8770e-04],\n",
       "           [-4.7207e-02, -5.0951e-02, -1.4882e-02,  ...,  2.6372e-03,\n",
       "             2.4347e-02,  2.8770e-04],\n",
       "           ...,\n",
       "           [-4.7207e-02, -5.0951e-02, -1.4882e-02,  ...,  2.6372e-03,\n",
       "             2.4347e-02,  2.8770e-04],\n",
       "           [-4.7207e-02, -5.0951e-02, -1.4882e-02,  ...,  2.6372e-03,\n",
       "             2.4347e-02,  2.8770e-04],\n",
       "           [-4.7207e-02, -5.0951e-02, -1.4882e-02,  ...,  2.6372e-03,\n",
       "             2.4347e-02,  2.8770e-04]]], grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs, _ = model_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=20, gold=None)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "671a1916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc torch.Size([2, 14])\n",
      "sa torch.Size([2, 6])\n",
      "wn torch.Size([2, 5])\n",
      "wc torch.Size([2, 14])\n",
      "wo torch.Size([2, 4, 4])\n",
      "wv\n",
      "torch.Size([2, 20, 8005])\n",
      "torch.Size([2, 20, 8005])\n",
      "torch.Size([2, 20, 8005])\n",
      "torch.Size([2, 20, 8005])\n"
     ]
    }
   ],
   "source": [
    "for k, v in decoder_outputs.items():\n",
    "    if k != \"wv\":\n",
    "        print(k, v.size())\n",
    "    else:\n",
    "        print(k)\n",
    "        for x in v:\n",
    "            print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea841381",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e60b73",
   "metadata": {},
   "source": [
    "# TODO : Loss add ignore index 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6a857",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7272d7",
   "metadata": {},
   "source": [
    "TRAIN for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cced6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [{'sel': 13, 'agg': 0, 'conds': [[10, 0, '이익잉여금'], [3, 0, 2015]]},\n",
    "#  {'sel': 16, 'agg': 0, 'conds': []}]\n",
    "# g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(t, tokenizer_bert, 1)\n",
    "# g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "857f36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = \\\n",
    "#     ([13, 16],\n",
    "#      [0, 0],\n",
    "#      [3, 1],\n",
    "#      [[10, 3, 3], [10]],\n",
    "#      [[0, 0, 0], [0]],\n",
    "#      [['유동부채', 2018, 2018], ['유동자산']],\n",
    "#      [([3574, 5872, 6398, 7405, 8003], [554, 115, 8003], [554, 115, 8003]),\n",
    "#       ([3574, 5872, 7162, 8003, 1], [1, 1, 1], [1, 1, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfdcec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "533b093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "totensor = lambda x: torch.LongTensor(x).to(device)\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "087239a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "class Perplexity(torchmetrics.Metric):\n",
    "    def __init__(self, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "        self.add_state(\"pp\", default=torch.FloatTensor([0]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"cnt\", default=torch.FloatTensor([0]), dist_reduce_fx=\"sum\")\n",
    "        \n",
    "    def update(self, pp: torch.Tensor):\n",
    "        self.pp += pp\n",
    "        self.cnt += 1\n",
    "\n",
    "    def compute(self):\n",
    "        return self.pp / self.cnt\n",
    "    \n",
    "    \n",
    "def pad_metrics(predict, gold, pad_idx=999):\n",
    "    \"\"\"\n",
    "    predict, gold = (B, where_num)\n",
    "    Case1 len(p) > len(g) \n",
    "        -> just get len(g)\n",
    "        -> if g is empty, pad g to len(p)\n",
    "    Case2 len(p) < len(g) \n",
    "        -> pad p\n",
    "    Case3 len(p) = len(g) \n",
    "        -> if p & g is empty, pad only 1 with p & g\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for p, g in zip(predict, gold):\n",
    "        if len(p) > len(g):\n",
    "            if len(g) == 0:\n",
    "                g = [pad_idx] * len(p)\n",
    "            else:\n",
    "                p = p[:len(g)]\n",
    "        elif len(p) < len(g):\n",
    "            p.extend([pad_idx]*(len(g)-len(p)))\n",
    "        else:\n",
    "            if len(p) == 0 and len(g) == 0:\n",
    "                g = [pad_idx]\n",
    "                p = [pad_idx]\n",
    "        \n",
    "        res.append([p, g])\n",
    "    \n",
    "    return tuple(map(list, zip(*res)))\n",
    "\n",
    "acc_sc = torchmetrics.Accuracy()\n",
    "pre_sc = torchmetrics.Precision()\n",
    "rec_sc = torchmetrics.Recall()\n",
    "\n",
    "acc_sa = torchmetrics.Accuracy(num_classes=n_agg_ops)\n",
    "pre_sa = torchmetrics.Precision(num_classes=n_agg_ops)\n",
    "rec_sa = torchmetrics.Recall(num_classes=n_agg_ops)\n",
    "\n",
    "acc_wn = torchmetrics.Accuracy(num_classes=max_where_conds+1)\n",
    "pre_wn = torchmetrics.Precision(num_classes=max_where_conds+1)\n",
    "rec_wn = torchmetrics.Recall(num_classes=max_where_conds+1)\n",
    "\n",
    "acc_wc = torchmetrics.Accuracy()\n",
    "pre_wc = torchmetrics.Precision()\n",
    "rec_wc = torchmetrics.Recall()\n",
    "\n",
    "acc_wo = torchmetrics.Accuracy(num_classes=n_cond_ops+1) # add one to calculate if where number is missing\n",
    "pre_wo = torchmetrics.Precision(num_classes=n_cond_ops+1)\n",
    "rec_wo = torchmetrics.Recall(num_classes=n_cond_ops+1)\n",
    "\n",
    "acc_wv = torchmetrics.Accuracy(num_classes=len(tokenizer_bert), ignore_index=1)\n",
    "pre_wv = torchmetrics.Precision(num_classes=len(tokenizer_bert), ignore_index=1)\n",
    "rec_wv = torchmetrics.Recall(num_classes=len(tokenizer_bert), ignore_index=1)\n",
    "\n",
    "pp_wv = Perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c08c1b-32ca-45bd-bfd3-d14849c638ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metricollection(prefix, num_classes=None, ignore_index=None):\n",
    "    metrics = torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=num_classes, ignore_index=ignore_index), \n",
    "        torchmetrics.Precision(num_classes=num_classes, ignore_index=ignore_index), \n",
    "        torchmetrics.Recall(num_classes=num_classes, ignore_index=ignore_index)\n",
    "    ])\n",
    "    return metrics.clone(prefix=prefix)\n",
    "\n",
    "metric_train_sc = create_metricollection(prefix=\"train_sc_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea038b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicts = model.predict_to_dict(decoder_outputs)\n",
    "p_sc = predicts[\"sc\"]\n",
    "p_wc = predicts[\"wc\"]\n",
    "p_wo = predicts[\"wo\"]\n",
    "p_wv_tkns = predicts[\"wv_tkns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bdc3c86-f2fe-4a97-8721-0bf0d551b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0], [13, 13])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sc, g_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a2f8acb-693b-4c11-bf52-027aa96e3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sc, g_sc = [2, 13], [13, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd7f9476-4d16-491d-876b-3e9aaa292c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_sc_Accuracy': tensor(0.5000),\n",
       " 'train_sc_Precision': tensor(0.5000),\n",
       " 'train_sc_Recall': tensor(0.5000)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_train_sc(totensor(p_sc), totensor(g_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21bcfe3e-8cf4-4a26-a2b7-52b8500ef49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_sc_Accuracy': tensor(0.2500),\n",
       " 'train_sc_Precision': tensor(0.2500),\n",
       " 'train_sc_Recall': tensor(0.2500)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_train_sc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "147a126d-9602-4035-8cb8-33c80115cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_train_sc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "249de7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases\n",
    "g_wc = [[10, 3], [3]]\n",
    "p_wc = [[0, 8, 3], [2, 1, 5]]\n",
    "\n",
    "g_wc = [[], []]\n",
    "p_wc = [[10, 3], [3]]\n",
    "\n",
    "g_wc = [[], []]\n",
    "p_wc = [[], []]\n",
    "\n",
    "g_wc = [[10, 3], [3]]\n",
    "p_wc = [[10], []]\n",
    "\n",
    "g_wc = [[10, 3], [3]]\n",
    "p_wc = [[0], [2, 1, 5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7764e325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 3], [10, 11]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff7ba10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9], [7]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33e2a470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[9, 999], [7, 999]], [[10, 3], [10, 11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wc, g_wc = pad_metrics(p_wc, g_wc)\n",
    "p_wc, g_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb39fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases\n",
    "g_wo = [[0, 0], [0]]\n",
    "p_wo = [[1, 3, 1], [3, 3, 3]]\n",
    "\n",
    "g_wo = [[], []]\n",
    "p_wo = [[0, 0], [0]]\n",
    "\n",
    "g_wo = [[], []]\n",
    "p_wo = [[], []]\n",
    "\n",
    "g_wo = [[0, 0], [0]]\n",
    "p_wo = [[0], []]\n",
    "\n",
    "g_wo = [[2, 3], [3]]\n",
    "p_wo = [[0], [2, 1, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea137a80-91b2-4f3a-b965-1b415fd6fa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7335a9d7-5359-41b8-9bfe-d3995eefd212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [1]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17115293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2, 4], [1, 4]], [[0, 0], [0, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wo, g_wo = pad_metrics(p_wo, g_wo, pad_idx=n_cond_ops)\n",
    "p_wo, g_wo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71c510f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict_to_dict(decoder_outputs)\n",
    "p_wv_tkns = predicts[\"wv_tkns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dbd87235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predicts = model.predict_to_dict(decoder_outputs)\n",
    "# # p_wv_tkns = predicts[\"wv_tkns\"]\n",
    "# # (where_nums, B, T_d)\n",
    "# p_wv_tkns = [\n",
    "#     [\n",
    "#         [11,6083,6083,6083,6083],[12,7896,7896,7896,7896]],\n",
    "#     [\n",
    "#         [21,7896,7896,7896,7896],[22,7896,7896,7896,7896]],\n",
    "#     [\n",
    "#         [31,6083,6083,6083,6083],[32,6083,6083,6083,6083]],\n",
    "#     [\n",
    "#         [41,6083,6083,6083,6083],[42,6083,6083,6083,6083]]\n",
    "# ]\n",
    "# # (B, where_nums, T_d)\n",
    "# g_wv_tkns = [[[2002, 8003, 1], [554, 116, 8003]], [[554, 115, 8003], [1, 1, 1]]]\n",
    "\n",
    "\n",
    "# g_wv_tkns = [[[], []], [[], []]]\n",
    "\n",
    "\n",
    "# acc_wv = torchmetrics.Accuracy(num_classes=len(tokenizer_bert), ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81bcf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wv_tkns = list(map(list, zip(*p_wv_tkns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d435d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_wv_tkns), len(p_wv_tkns[0]), len(p_wv_tkns[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9ed768e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_wv_tkns), len(g_wv_tkns[0]), len(g_wv_tkns[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e6d8049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[6272, 6272, 6272, 6272, 6272], [6272, 6272, 6272, 6272, 5712]],\n",
       "  [[6272, 6272, 6272, 6272, 6272], [6272, 6272, 6272, 6272, 6272]]],\n",
       " [[[3905, 7452, 5436, 8003, 1], [554, 115, 8003, 1, 1]],\n",
       "  [[2514, 7063, 5872, 7162, 8003], [4128, 517, 169, 1258, 8003]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the where number first: (B, where_nums, T_d)\n",
    "p_wv_tkns, g_wv_tkns = pad_metrics(p_wv_tkns, g_wv_tkns, pad_idx=1)\n",
    "p_wv_tkns, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc1ab9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, where_num in enumerate(g_wn):\n",
    "    batch_g_wc = g_wc[batch_idx]\n",
    "    batch_wc = p_wc[batch_idx]\n",
    "    acc_wc(*map(totensor, [batch_wc, batch_g_wc]))\n",
    "    \n",
    "    batch_g_wo = g_wo[batch_idx]\n",
    "    batch_wo = p_wo[batch_idx]\n",
    "    acc_wo(*map(totensor, [batch_wo, batch_g_wo]))\n",
    "    \n",
    "    batch_g_wv_tkns = g_wv_tkns[batch_idx]\n",
    "    batch_wv_tkns = p_wv_tkns[batch_idx]\n",
    "    batch_wv_tkns, batch_g_wv_tkns = pad_metrics(batch_wv_tkns, batch_g_wv_tkns, pad_idx=1)\n",
    "    for b_wv_tkns, b_g_wv_tkns in zip(batch_wv_tkns, batch_g_wv_tkns):\n",
    "        # iter for each where number tokens\n",
    "        acc_wv(*map(totensor, [b_wv_tkns, b_g_wv_tkns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50fa8b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_wc.compute(), acc_wo.compute(), acc_wv.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47594c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ae42edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_g_wc = g_wc[batch_idx]\n",
    "batch_wc = p_wc[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173918a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e0f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bc43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f4b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, where_num in enumerate(g_wn):\n",
    "    g_wc\n",
    "    \n",
    "    \n",
    "    batch_g_wo = g_wo[batch_idx]  # (where_num_gold,)\n",
    "    batch_wo = p_wo[batch_idx]  # (where_num_predict,)\n",
    "    break\n",
    "    self.acc_wo(*map(self.totensor_cpu, [batch_wo, batch_g_wo]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wc, g_wc = model.pad_empty_predict_gold(p_wc, g_wc, 999)\n",
    "p_wc = [p_wc[batch_idx][:wn] for batch_idx, wn in enumerate(g_wn)]\n",
    "g_wc = [x for inner in g_wc for x in inner]\n",
    "p_wc = [x for inner in p_wc for x in inner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ddc6004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wo = predicts[\"wo\"]\n",
    "p_wv_tkns = predicts[\"wv_tkns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "966344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_wo, g_wv_tkns = ([[0, 0], [0]],\n",
    " [[[2002, 8003, 1], [554, 116, 8003]], [[554, 115, 8003], [1, 1, 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0eda5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wo, g_wo = model.pad_empty_predict_gold(p_wo, g_wo, pad_idx=n_cond_ops)  # (B, where_col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c7a118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wo = [[0, 3, 1], [3, 3, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d5ea838a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 4], [0, 4, 4])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4f83aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([1, 3, 1], [3, 3, 3]), ([0, 0, 4], [0, 4, 4]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wo, g_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "251157f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, where_num in enumerate(g_wn):\n",
    "    batch_g_wo = g_wo[batch_idx]  # (where_num_gold,)\n",
    "    batch_wo = p_wo[batch_idx]  # (where_num_predict,)\n",
    "    break\n",
    "    self.acc_wo(*map(self.totensor_cpu, [batch_wo, batch_g_wo]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "dd98f2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 4], [0, 3, 1])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_g_wo, batch_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2f562b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_wo(*map(totensor, [batch_g_wo, batch_wo]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "97d30ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs[\"wo\"][batch_idx, :where_num, :].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba349125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_g_wo = g_wo[batch_idx]  # (where_num,)\n",
    "batch_wo = decoder_outputs[\"wo\"][batch_idx, :where_num, :]  # (where_num, n_cond_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "11aea0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c95c2ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicts = {'sc': [13, 11],\n",
    " 'sa': [2, 2],\n",
    " 'wn': [0, 0],\n",
    " 'wc': [[], []],\n",
    " 'wo': [[], []],\n",
    " 'wv_tkns': [[[3280], [3280]],\n",
    "  [[3280], [3280]],\n",
    "  [[3280], [3280]],\n",
    "  [[3280], [3280]]],\n",
    " 'wv': [('라이프탱크퀄', '라이프라이프라이프', '없었다', '없었다'),\n",
    "  ('라이프라이프 명단', '없었다 순 순', '없었다', '없었다')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05c85cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60.2921, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_wv = Perplexity()\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "cross_entropy_wv = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=1)\n",
    "\n",
    "binary_cross_entropy = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "vocab_size = len(tokenizer_bert)\n",
    "hparam_wn_penalty = 2.0  # scale up for guessing where number\n",
    "# hparam_wo_penalty = 4.0\n",
    "# hparam_wv_penalty = 5.0\n",
    "# Outputs Size\n",
    "# sc = (B, T_c)\n",
    "# sa = (B, n_agg_ops)\n",
    "# wn = (B, 5)\n",
    "# wc = (B, T_c): binary\n",
    "# wo = (B, max_where_col_nums, n_cond_ops)\n",
    "# wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "loss_sc = cross_entropy(decoder_outputs[\"sc\"], totensor(g_sc))\n",
    "loss_sa = cross_entropy(decoder_outputs[\"sa\"], totensor(g_sa))\n",
    "loss_wn = cross_entropy(decoder_outputs[\"wn\"], totensor(g_wn)) * hparam_wn_penalty\n",
    "loss_wc = 0\n",
    "loss_wo = 0\n",
    "loss_wv = 0\n",
    "for batch_idx, where_num in enumerate(g_wn):  # iter by batch_size: B\n",
    "\n",
    "    one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"][batch_idx], device=device).scatter(0, totensor(g_wc[batch_idx]), 1.0)\n",
    "    loss_wc += binary_cross_entropy(decoder_outputs[\"wc\"][batch_idx], one_hot_dist)\n",
    "\n",
    "    batch_g_wo = g_wo[batch_idx]  # (where_num,)\n",
    "    batch_wo = decoder_outputs[\"wo\"][batch_idx, :where_num, :]  # (where_num, n_cond_ops)\n",
    "    loss_wo += cross_entropy(batch_wo, totensor(batch_g_wo))\n",
    "    \n",
    "#     if (len(batch_wo) == 0 and where_num != 0):\n",
    "#         # if predict nothing where clause and answer is not, what loss should be added?\n",
    "#         # simply giving big loss will be enough?\n",
    "#         loss_wo += loss_wn * 100\n",
    "#     else:\n",
    "#         give_wo_penalty = False\n",
    "#         if len(batch_wo) > len(batch_g_wo): \n",
    "#             wo_penalty = hparam_wo_penalty / 2\n",
    "#             give_wo_penalty = True\n",
    "#             batch_wo = batch_wo[:len(batch_g_wo), :]  # (where_num_predict, n_cond_ops)\n",
    "#         elif len(batch_wo) < len(batch_g_wo):\n",
    "#             # giving penalty if not guessed right where numbers\n",
    "#             # It becomes problem when reduce the gold tokens but predicted corrected \n",
    "#             # Then `loss_wo_base` will be 0, if simply multiply by `loss_wv_base` to loss_base will be zero\n",
    "#             wo_penalty = hparam_wo_penalty\n",
    "#             give_wo_penalty = True\n",
    "#             batch_g_wo = batch_g_wo[:len(batch_wo)]  # (where_num_gold,)\n",
    "#         else:\n",
    "#             wo_penalty = 1.0\n",
    "#             give_wo_penalty = False\n",
    "#         loss_wo_base = cross_entropy(batch_wo, totensor(batch_g_wo))\n",
    "#         if give_wo_penalty:\n",
    "#             loss_wo += loss_wo_base + loss_wn * wo_penalty\n",
    "#         else:\n",
    "#             loss_wo += loss_wo_base\n",
    "\n",
    "    batch_g_wv = g_wv_tkns[batch_idx][:where_num]  # (where_num, T_d_i)\n",
    "    batch_wv = [wv[batch_idx] for wv in decoder_outputs[\"wv\"]][:where_num]  # (where_num, T_d_i, vocab_size)\n",
    "    for wv, g_wv_i in zip(batch_wv, batch_g_wv):\n",
    "        if len(wv) > len(g_wv_i):\n",
    "            g_wv_i = g_wv_i + [tokenizer_bert.pad_token_id]*(len(wv) - len(g_wv_i))\n",
    "#             give_wv_penalty = True\n",
    "        elif len(wv) < len(g_wv_i):\n",
    "            # g_wv_i = g_wv_i[:len(wv)] \n",
    "            pad_tkns = model.model_decoder.where_value_decoder.embedding_layer(torch.LongTensor([model.tokenizer_bert.pad_token_id]*(len(g_wv_i) - len(wv))))\n",
    "            pad_tkns_res = model.model_decoder.where_value_decoder.output_linear(pad_tkns)\n",
    "            wv = torch.cat([wv, pad_tkns_res])\n",
    "#             give_wv_penalty = True\n",
    "        else:\n",
    "            wv_penalty = 1.0\n",
    "#             give_wv_penalty = False\n",
    "#         if give_wv_penalty:\n",
    "#             loss_wv += loss_wv_base + loss_wn * wv_penalty\n",
    "#         else:\n",
    "#             loss_wv += loss_wv_base\n",
    "            \n",
    "        loss_wv += cross_entropy_wv(wv, totensor(g_wv_i))\n",
    "#     if len(batch_wo) == 0 and where_num != 0:\n",
    "#         # if predict nothing where clause and answer is not, what loss should be added?\n",
    "#         loss_wv += loss_wn * 100\n",
    "#     else:\n",
    "#         for wv, g_wv_i in zip(batch_wv, batch_g_wv):  # will iter by where_num\n",
    "#             give_wv_penalty = False\n",
    "#             if len(wv) > len(g_wv_i):\n",
    "#                 wv_penalty = hparam_wo_penalty / 2\n",
    "#                 give_wv_penalty = True\n",
    "#                 wv = wv[:len(g_wv_i), :]  # (T_d_gold, vocab_size)\n",
    "#             elif len(wv) < len(g_wv_i):\n",
    "#                 # giving penalty if not generate enough tokens\n",
    "#                 # It becomes problem when reduce the gold tokens but predicted corrected \n",
    "#                 # Then `loss_wv_base` will be 0, if simply multiply by `loss_wv_base` to loss_base will be zero\n",
    "#                 wv_penalty = hparam_wo_penalty\n",
    "#                 give_wv_penalty = True\n",
    "#                 g_wv_i = g_wv_i[:len(wv)]  # (T_d_predict,)\n",
    "#             else:\n",
    "#                 wv_penalty = 1.0\n",
    "#                 give_wv_penalty = False\n",
    "#             # now have the same T_d size, ignore all over lengthed\n",
    "#             loss_wv_base = cross_entropy(wv, totensor(g_wv_i))\n",
    "#             pp_wv.update(torch.exp(loss_wv_base))\n",
    "#             if give_wv_penalty:\n",
    "#                 loss_wv += loss_wv_base + loss_wn * wv_penalty\n",
    "#             else:\n",
    "#                 loss_wv += loss_wv_base\n",
    "\n",
    "pp_wv.update(torch.exp(loss_wv/ batch_size) ) \n",
    "loss = (loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv) / batch_size\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8eda4b56-9881-4ee0-8f56-31515258bb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11], [4]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dc7deff1-cf71-4500-b0f9-eb4c3ce7c151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4828, 0.4826, 0.4818, 0.4824, 0.4831, 0.4825, 0.4820, 0.4820, 0.4812,\n",
       "        0.4822, 0.4819, 0.4807, 0.4810, 0.4809, 0.0000, 0.0000, 0.0000],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(decoder_outputs[\"wc\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a67ec0d-5416-48f9-be1c-cd6c38c4dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"][0], device=device).scatter(0, totensor(g_wc[0]), 1.0)\n",
    "# loss_wc += binary_cross_entropy(decoder_outputs[\"wc\"][batch_idx], one_hot_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "faf79f2e-c7c8-411d-b141-79a48138ccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ebaff366-7b12-4fbd-bb2b-c7dc680c5ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 17)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table[\"007810\"][\"header\"]), len(table[\"company\"][\"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e931cd8-3c6d-4460-aa63-094ae2c766b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs[\"wc\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a70ba33-05a5-472f-bc69-415875eab373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2750, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cross_entropy(decoder_outputs[\"wc\"][0], one_hot_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c02c5a2a-967f-4b3f-abda-acb0d8d8fb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258d9a7-c988-4f96-a374-66c4422d1515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc3917f",
   "metadata": {},
   "source": [
    "# Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b453dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os \n",
    "import pytorch_lightning as pl\n",
    "from copy import deepcopy\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")\n",
    "\n",
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ddc898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQL(pl.LightningModule):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.dbengine = DBEngine(Path(self.hparams.db_path))\n",
    "        self.n_agg_ops = len(self.dbengine.agg_ops)\n",
    "        self.n_cond_ops = len(self.dbengine.cond_ops)\n",
    "        # Encoder\n",
    "        self.model_bert, self.tokenizer_bert, self.config_bert = self.get_bert(model_path=self.hparams.model_bert_path)\n",
    "        # Decoder\n",
    "        self.model_decoder = Decoder(\n",
    "            input_size=self.config_bert.hidden_size, \n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            dropout_ratio=self.hparams.dropout_ratio,\n",
    "            max_where_conds=self.hparams.max_where_conds,\n",
    "            n_agg_ops=self.n_agg_ops,\n",
    "            n_cond_ops=self.n_cond_ops,\n",
    "            start_tkn_id = self.tokenizer_bert.additional_special_tokens_ids[0],\n",
    "            end_tkn_id = self.tokenizer_bert.additional_special_tokens_ids[1],\n",
    "            embedding_layer = self.model_bert.embeddings.word_embeddings\n",
    "        )\n",
    "        \n",
    "        # Loss function & Metrics\n",
    "        self.vocab_size = len(self.tokenizer_bert)\n",
    "        self.totensor = lambda x: torch.LongTensor(x).to(self.device)\n",
    "        self.totensor_cpu = lambda x: torch.LongTensor(x).to(\"cpu\")\n",
    "        self.table_dict = {\"train\": None, \"eval\": None}\n",
    "        self.create_metrics()\n",
    "\n",
    "    def get_bert(self, model_path: str, output_hidden_states: bool=False):\n",
    "        self.special_tokens = [self.hparams.special_start_tkn, self.hparams.special_end_tkn, self.hparams.special_col_tkn] # sequence start, sequence end, column tokens\n",
    "        tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=self.special_tokens)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        model = BertModel.from_pretrained(model_path)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        return model, tokenizer, config\n",
    "    \n",
    "    def create_metrics(self):\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.binary_cross_entropy = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        self.cross_entropy_wv = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=1)\n",
    "\n",
    "        self.acc_sc = torchmetrics.Accuracy()\n",
    "        self.acc_sa = torchmetrics.Accuracy(num_classes=self.n_agg_ops)\n",
    "        self.acc_wn = torchmetrics.Accuracy(num_classes=self.hparams.max_where_conds+1)\n",
    "        self.acc_wo = torchmetrics.Accuracy(num_classes=self.n_cond_ops+1) # add one to calculate if where number is missing\n",
    "        self.pp_wv = Perplexity()\n",
    "\n",
    "    def reset_metrics_epoch_end(self):\n",
    "        self.acc_sc.reset()\n",
    "        self.acc_sa.reset()\n",
    "        self.acc_wn.reset()\n",
    "        self.acc_wo.reset()\n",
    "        self.pp_wv.reset()\n",
    "        \n",
    "    def forward(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls, value_tkn_max_len, train)\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "        gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "        loss = self.calculate_loss(outputs, gold)  # when calculate loss must need gold answer\n",
    "        return loss, outputs\n",
    "            \n",
    "    def forward_outputs(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        # --- Get Answer & Variables ---\n",
    "        if train:\n",
    "            assert value_tkn_max_len is None, \"In train phase, `value_tkn_max_len` must be None\"\n",
    "            assert batch_sqls is not None, \"In train phase, `batch_sqls` must not be None\"\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "            gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "        else:\n",
    "            assert value_tkn_max_len is not None, \"In validation Phase, `value_tkn_max_len` must not be None\"\n",
    "            gold = None\n",
    "            value_tkn_max_len = value_tkn_max_len\n",
    "            \n",
    "        # --- Get Inputs for Encoder --- \n",
    "        encode_inputs = self.tokenizer_bert(\n",
    "            batch_qs, batch_ts, \n",
    "            max_length=self.hparams.max_length, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "            return_attention_mask=True, \n",
    "            return_special_tokens_mask=False, \n",
    "        ).to(self.device)  # encode_input doesn't return the cuda device\n",
    "        \n",
    "        # --- Forward Encoder ---\n",
    "        encode_outputs = self.model_bert(**encode_inputs)\n",
    "        \n",
    "        # --- Get Inputs for Decoder ---\n",
    "        input_question_mask, input_db_mask, input_col_mask = self.get_input_mask(encode_inputs)\n",
    "        question_padded, question_lengths = self.get_decoder_batches(encode_outputs, input_question_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        db_padded, db_lengths = self.get_decoder_batches(encode_outputs, input_db_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        col_padded, col_lengths = self.get_decoder_batches(encode_outputs, input_col_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        \n",
    "        # --- Forward Decoder ---\n",
    "        decoder_outputs = self.model_decoder(\n",
    "            question_padded, \n",
    "            db_padded, \n",
    "            col_padded, \n",
    "            question_lengths, \n",
    "            col_lengths, \n",
    "            value_tkn_max_len, \n",
    "            gold\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "    def get_input_mask(self, encode_inputs):\n",
    "        sep_tkn_mask = encode_inputs[\"input_ids\"] == self.tokenizer_bert.sep_token_id\n",
    "        col_tkn_id = self.tokenizer_bert.additional_special_tokens_ids[2]\n",
    "\n",
    "        input_question_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 0, encode_inputs[\"attention_mask\"].bool())\n",
    "        input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "        input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "        db_mask = torch.bitwise_and(encode_inputs[\"token_type_ids\"] == 1, encode_inputs[\"attention_mask\"].bool())\n",
    "        col_tkn_mask = encode_inputs[\"input_ids\"] == col_tkn_id\n",
    "        db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "        db_mask = torch.bitwise_xor(db_mask, torch.bitwise_and(sep_tkn_mask, encode_inputs[\"token_type_ids\"] == 1))\n",
    "        \n",
    "        return input_question_mask, db_mask, col_tkn_mask\n",
    "\n",
    "\n",
    "    # def get_input_mask_and_answer(self, encode_input: transformers.tokenization_utils_base.BatchEncoding, tokenizer: KoBertTokenizer) -> Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]:\n",
    "    #     \"\"\"[summary]\n",
    "\n",
    "    #     In this code 'table' means database table name(id), 'header' means database header, 'col' means index of header \n",
    "\n",
    "    #     Args:\n",
    "    #         encode_input (transformers.tokenization_utils_base.BatchEncoding): [description]\n",
    "    #         tokenizer (KoBertTokenizer): [description]\n",
    "\n",
    "    #     Returns:\n",
    "    #         Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]: [description]\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "    #     sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "    #     start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "\n",
    "    #     input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool()).contiguous()\n",
    "    #     input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask).contiguous() # [SEP] mask out\n",
    "    #     input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "    #     db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool()).contiguous()\n",
    "    #     db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask).contiguous()\n",
    "    #     col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "    #     db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask).contiguous()\n",
    "    #     # split table_mask and header_mask\n",
    "    #     input_idx = torch.arange(max_length).repeat(batch_size, 1).to(self.device).contiguous()\n",
    "    #     db_idx = input_idx[db_mask]\n",
    "    #     table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "    #     table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "    #     start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "    #     table_end_sep_idx = start_idx[:, 0] - 1\n",
    "    #     split_size = torch.stack([\n",
    "    #         table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "    #     ]).transpose(0, 1)\n",
    "\n",
    "    #     # Token idx\n",
    "    #     table_tkn_idx, header_tkn_idx = map(\n",
    "    #         lambda x: torch.stack(x).to(self.device), \n",
    "    #         zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "    #     )\n",
    "\n",
    "    #     table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "\n",
    "    #     # TODO: [EXP] Experiment for generate column directly\n",
    "    #     # If [EXP], `table_tkn_mask` and `header_tkn_mask` should include [S] & [E] tokens\n",
    "    #     table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool, device=self.device).scatter(1, table_tkn_idx, True).contiguous()\n",
    "    #     header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool, device=self.device).scatter(1, header_tkn_idx, True).contiguous()\n",
    "\n",
    "    #     # TODO: [EXP] Experiment for generate column directly\n",
    "    #     # For Decoder Input, Maskout [S], [E] for table & header -> will be done automatically\n",
    "    #     input_table_mask = self.get_decoder_input_mask(\n",
    "    #         encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    #     )\n",
    "    #     input_header_mask = self.get_decoder_input_mask(\n",
    "    #         encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    #     )\n",
    "\n",
    "    #     # [COL] token mask: this is for attention\n",
    "    #     col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "    #     input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], device=self.device, dtype=torch.bool).scatter(1, col_tkn_idx, True).contiguous()\n",
    "\n",
    "    #     # TODO: [EXP] Experiment for generate column directly\n",
    "    #     # For Answer, Maskout [S] for table & header \n",
    "    #     # answer_table_tkns = get_answer(\n",
    "    #     #     encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    #     # )\n",
    "    #     # answer_header_tkns = get_answer(\n",
    "    #     #     encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    #     # )\n",
    "\n",
    "    #     return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns    \n",
    "\n",
    "    ## Masks\n",
    "    # TODO: [EXP] Experiment for generate column directly\n",
    "    # def get_answer(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    #     r\"\"\"\n",
    "    #     answer should include end token: [E]\n",
    "    #     \"\"\"\n",
    "    #     masked_input_ids = input_ids[mask]\n",
    "    #     start_tkn_mask = masked_input_ids == start_tkn_id\n",
    "    #     end_tkn_mask = masked_input_ids == end_tkn_id\n",
    "    #     table_col_length = masked_input_ids.view(batch_size, -1).size(1)\n",
    "    #     start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    #     index = torch.arange(table_col_length).repeat(batch_size)[start_end_mask].view(batch_size, -1, 2)\n",
    "    #     tkn_lengths = index[:, :, 1] - index[:, :, 0]\n",
    "    #     answer_col_tkns = [x.split(tkn_length.tolist()) for x, tkn_length in zip(\n",
    "    #         masked_input_ids[~start_tkn_mask].view(batch_size, -1), tkn_lengths)]\n",
    "    #     return answer_col_tkns\n",
    "\n",
    "    # def get_decoder_input_mask(self, input_ids: torch.Tensor, mask: torch.BoolTensor, batch_size: int, start_tkn_id: int, end_tkn_id: int) -> torch.BoolTensor:\n",
    "    #     \"\"\"[summary]\n",
    "\n",
    "    #     Args:\n",
    "    #         input_ids (torch.Tensor): [description]\n",
    "    #         mask (torch.BoolTensor): [description]\n",
    "    #         batch_size (int): [description]\n",
    "    #         start_tkn_id (int): [description]\n",
    "    #         end_tkn_id (int): [description]\n",
    "\n",
    "    #     Returns:\n",
    "    #         torch.BoolTensor: [description]\n",
    "    #     \"\"\"    \n",
    "    #     start_tkn_mask = input_ids == start_tkn_id\n",
    "    #     end_tkn_mask = input_ids == end_tkn_id\n",
    "    #     start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    #     index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1).contiguous()\n",
    "    #     return mask.scatter(1, index, False)\n",
    "    \n",
    "    def get_decoder_batches(self, encode_output: transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions, mask: torch.BoolTensor, pad_idx: int) -> Tuple[torch.Tensor, List[int]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            encode_output (transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions): [description]\n",
    "            mask (torch.BoolTensor): [description]\n",
    "            model (BertModel): [description]\n",
    "            pad_idx (int): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, List[int]]: [description]\n",
    "        \"\"\"    \n",
    "        lengths = mask.sum(1)\n",
    "        tensors = encode_output.last_hidden_state[mask, :]\n",
    "        batches = torch.split(tensors, lengths.tolist())\n",
    "        if lengths.ne(lengths.max()).sum() != 0:\n",
    "            # pad not same length tokens\n",
    "            tensors_padded = self.pad(batches, lengths.tolist(), pad_idx=pad_idx)\n",
    "        else:\n",
    "            # just stack the splitted tensors\n",
    "            tensors_padded = torch.stack(batches)\n",
    "        return tensors_padded, lengths.tolist()\n",
    "\n",
    "    def pad(self, batches: Tuple[torch.Tensor], lengths: List[int], pad_idx: int=1) -> torch.Tensor:\n",
    "        \"\"\"Pad for decoder inputs\n",
    "\n",
    "        Args:\n",
    "            batches (Tuple[torch.Tensor]): [description]\n",
    "            lengths (List[int]): [description]\n",
    "            model (transformers.models.bert.modeling_bert.BertModel): [description]\n",
    "            pad_idx (int, optional): [description]. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [description]\n",
    "        \"\"\"       \n",
    "        padded = []\n",
    "        max_length = max(lengths)\n",
    "        for x in batches:\n",
    "            if len(x) < max_length:\n",
    "                idxes = self.totensor([pad_idx]*(max_length - len(x)))\n",
    "                pad_tensor = self.model_bert.embeddings.word_embeddings(idxes)\n",
    "                padded.append(torch.cat([x, pad_tensor]))\n",
    "            else:\n",
    "                padded.append(x)\n",
    "        return torch.stack(padded)\n",
    "\n",
    "    def get_sql_answers(self, batch_sqls: List[Dict[str, Any]]):\n",
    "        \"\"\"[summary]\n",
    "        sc: select column\n",
    "        sa: select agg\n",
    "        wn: where number\n",
    "        wc: where column\n",
    "        wo: where operator\n",
    "        wv: where value\n",
    "\n",
    "        Args:\n",
    "            batch_sqls (List[Dict[str, Any]]): [description]\n",
    "            tokenizer (KoBertTokenizer): [description]\n",
    "\n",
    "        Raises:\n",
    "            EnvironmentError: [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "        g_sc = []\n",
    "        g_sa = []\n",
    "        g_wn = []\n",
    "        g_wc = []\n",
    "        g_wo = []\n",
    "        g_wv = []\n",
    "        for b, sql_dict in enumerate(batch_sqls):\n",
    "            g_sc.append( sql_dict[\"sel\"] )\n",
    "            g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "            conds = sql_dict[\"conds\"]\n",
    "            if not sql_dict[\"agg\"] < 0:\n",
    "                g_wn.append( len(conds) )\n",
    "                g_wc.append( get_ith_element(conds, 0) )\n",
    "                g_wo.append( get_ith_element(conds, 1) )\n",
    "                g_wv.append( get_ith_element(conds, 2) )\n",
    "            else:\n",
    "                raise EnvironmentError\n",
    "\n",
    "        # get where value tokenized \n",
    "        pad_tkn_id = self.tokenizer_bert.pad_token_id\n",
    "        g_wv_tkns = [[f\"{s}{self.hparams.special_end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "        g_wv_tkns = [self.tokenizer_bert(batch_wv, add_special_tokens=False)[\"input_ids\"] if len(batch_wv) > 0 else batch_wv for batch_wv in g_wv_tkns]\n",
    "        # add empty list if batch has different where column number\n",
    "        max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "        g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "        temp = []\n",
    "        for batch_wv in list(zip(*g_wv_tkns)):\n",
    "            batch_max_len = max(map(len, batch_wv))\n",
    "            batch_temp = []\n",
    "            for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "                if len(wv_tkns) < batch_max_len:\n",
    "                    batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "                else:\n",
    "                    batch_temp.append(wv_tkns)\n",
    "            temp.append(batch_temp)\n",
    "        g_wv_tkns = list(zip(*temp))\n",
    "        g_wv_tkns = list(map(list, g_wv_tkns))\n",
    "        return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns\n",
    "        \n",
    "        \n",
    "    def calculate_loss(self, decoder_outputs, gold):\n",
    "        \"\"\"\n",
    "        # Outputs Size\n",
    "        sc = (B, T_c)\n",
    "        sa = (B, n_agg_ops)\n",
    "        wn = (B, 5)\n",
    "        wc = (B, T_c): binary\n",
    "        wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        # Loss Calculation\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "        # g_wv_tkns = list(zip(*g_wv_tkns))  # (B, where_col_num, T_d_i) -> (where_col_num, B, T_d_i)\n",
    "\n",
    "        batch_size = decoder_outputs[\"sc\"].size(0)\n",
    "        loss_sc = self.cross_entropy(decoder_outputs[\"sc\"], self.totensor(g_sc))\n",
    "        loss_sa = self.cross_entropy(decoder_outputs[\"sa\"], self.totensor(g_sa))\n",
    "        loss_wn = self.cross_entropy(decoder_outputs[\"wn\"], self.totensor(g_wn)) * self.hparams.wn_penalty\n",
    "\n",
    "        # need consider: might have different length of where numers\n",
    "        # So when calculate scores looping by where numbers, ignore the out of length tokens\n",
    "        loss_wc = 0\n",
    "        loss_wo = 0\n",
    "        loss_wv = 0\n",
    "        for batch_idx, where_num in enumerate(g_wn):  # iter by batch_size: B\n",
    "\n",
    "            one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"][batch_idx], device=self.device).scatter(0, self.totensor(g_wc[batch_idx]), 1.0)\n",
    "            loss_wc += self.binary_cross_entropy(decoder_outputs[\"wc\"][batch_idx], one_hot_dist)\n",
    "\n",
    "            batch_g_wo = g_wo[batch_idx]  # (where_num,)\n",
    "            batch_wo = decoder_outputs[\"wo\"][batch_idx, :where_num, :]  # (where_num, n_cond_ops)\n",
    "            loss_wo += self.cross_entropy(batch_wo, self.totensor(batch_g_wo))\n",
    "            \n",
    "            batch_g_wv = g_wv_tkns[batch_idx][:where_num]  # (where_num, T_d_i)\n",
    "            batch_wv = [wv[batch_idx] for wv in decoder_outputs[\"wv\"]]  # (where_num, T_d_i, vocab_size)\n",
    "            for wv, g_wv_i in zip(batch_wv, batch_g_wv):\n",
    "                if len(wv) > len(g_wv_i):\n",
    "#                     g_wv_i = g_wv_i + [self.tokenizer_bert.pad_token_id]*(len(wv) - len(g_wv_i))\n",
    "                    wv = wv[:len(g_wv_i)]\n",
    "                elif len(wv) < len(g_wv_i):\n",
    "                    g_wv_i = g_wv_i[:len(wv)] \n",
    "                loss_wv += self.cross_entropy_wv(wv, self.totensor(g_wv_i))\n",
    "        self.pp_wv.update(torch.exp(loss_wv) / batch_size)     \n",
    "        loss = (loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def calculate_metrics(self, decoder_outputs, batch_sqls) -> None:\n",
    "        # Predict tokens\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "        predicts = self.predict_to_dict(decoder_outputs)\n",
    "        p_sc, p_sa, p_wn, p_wo = predicts[\"sc\"], predicts[\"sa\"], predicts[\"wn\"], predicts[\"wo\"]\n",
    "        \n",
    "        p_wo, g_wo = self.pad_empty_predict_gold(p_wo, g_wo, pad_idx=self.n_cond_ops)  # (B, where_col_num)\n",
    "        \n",
    "        acc_sc = self.acc_sc(*map(self.totensor_cpu, [p_sc, g_sc]))\n",
    "        acc_sa = self.acc_sa(*map(self.totensor_cpu, [p_sa, g_sa]))\n",
    "        acc_wn = self.acc_wn(*map(self.totensor_cpu, [p_wn, g_wn]))\n",
    "        \n",
    "        for batch_idx, where_num in enumerate(g_wn):\n",
    "            batch_g_wo = g_wo[batch_idx]  # (where_num_gold,)\n",
    "            batch_wo = p_wo[batch_idx]  # (where_num_predict,)\n",
    "            acc_wo = self.acc_wo(*map(self.totensor_cpu, [batch_wo, batch_g_wo]))\n",
    "        \n",
    "    def pad_empty_predict_gold(self, predict, gold, pad_idx):\n",
    "        res = []\n",
    "        for p, g in zip(predict, gold):            \n",
    "            if len(p) < len(g):\n",
    "                p.extend([pad_idx]*(len(g)-len(p)))\n",
    "            elif len(p) > len(g):\n",
    "                g.extend([pad_idx]*(len(p)-len(g)))\n",
    "\n",
    "            res.append([p, g])\n",
    "\n",
    "        return list(zip(*res))\n",
    "    \n",
    "    def get_batch_data(self, data: List[Dict[str, Any]], table: Dict[str, Dict[str, List[Any]]], start_tkn=\"[S]\", end_tkn=\"[E]\", only_question=False) -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict[str, Any]]): [description]\n",
    "            dbengine (DBEngine): [description]\n",
    "            start_tkn (str, optional): [description]. Defaults to \"[S]\".\n",
    "            end_tkn (str, optional): [description]. Defaults to \"[E]\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[Dict[str, Any]]]: [description]\n",
    "        \"\"\"    \n",
    "        batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "        \n",
    "        tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "        batch_ts = []\n",
    "        for table_id in tid:\n",
    "            table_str = f\"{table_id}\" + \"\".join([\n",
    "                f\"{self.hparams.special_col_tkn}{col}\" for col in table[table_id][\"header\"]\n",
    "            ])\n",
    "            # TODO: [EXP] Experiment for generate column directly\n",
    "            # table_str = f\"{start_tkn}{table_id}{end_tkn}\" + \"\".join([\n",
    "            #     f\"{col_tkn}{start_tkn}{col}{end_tkn}\" for col in dbengine.schema\n",
    "            # ]) \n",
    "            batch_ts.append(table_str)\n",
    "        if only_question:\n",
    "            return batch_qs, batch_ts\n",
    "        \n",
    "        batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "        return batch_qs, batch_ts, batch_sqls\n",
    "    \n",
    "    def compute_all_metrics(self):\n",
    "        acc_sc = self.acc_sc.compute()\n",
    "        acc_sa = self.acc_sa.compute()\n",
    "        acc_wn = self.acc_wn.compute()\n",
    "        acc_wo = self.acc_wo.compute()\n",
    "        pp_wv = self.pp_wv.compute()\n",
    "        return acc_sc, acc_sa, acc_wn, acc_wo, pp_wv\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table_dict[\"train\"], self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=None, \n",
    "            train=True\n",
    "        )\n",
    "        self.calculate_metrics(outputs, batch_sqls)\n",
    "\n",
    "        acc_sc, acc_sa, acc_wn, acc_wo, pp_wv = self.compute_all_metrics()\n",
    "        self.log(\"train_step_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"train_step_acc_sc\", acc_sc, prog_bar=True, logger=True)\n",
    "        self.log(\"train_step_acc_sa\", acc_sa, prog_bar=True, logger=True)\n",
    "        self.log(\"train_step_acc_wn\", acc_wn, prog_bar=True, logger=True)\n",
    "        self.log(\"train_step_acc_wo\", acc_wo, prog_bar=True, logger=True)\n",
    "        self.log(\"train_step_pp_wv\", pp_wv, prog_bar=True, logger=True)\n",
    "\n",
    "        return  {\"loss\": loss}\n",
    "\n",
    "    def train_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "        \n",
    "        \n",
    "        acc_sc, acc_sa, acc_wn, acc_wo, pp_wv = self.compute_all_metrics()\n",
    "        self.log(\"train_loss\", loss, prog_bar=False, logger=True)\n",
    "        self.log(\"train_acc_sc\", acc_sc, prog_bar=False, logger=True)\n",
    "        self.log(\"train_acc_sa\", acc_sa, prog_bar=False, logger=True)\n",
    "        self.log(\"train_acc_wn\", acc_wn, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc_wo\", acc_wo,  prog_bar=False, logger=True)\n",
    "        self.log(\"train_pp_wv\", pp_wv, prog_bar=False, logger=True)\n",
    "    \n",
    "        self.reset_metrics_epoch_end()\n",
    "        # return {\"train_loss\": loss, \"train_acc_sc\": acc_sc, \"train_acc_sa\": acc_sa, \"train_acc_wn\": acc_wn, \"train_acc_wo\": acc_wo, \"train_pp_wv\": pp_wv}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table_dict[\"eval\"], self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=self.hparams.value_tkn_max_len, \n",
    "            train=False\n",
    "        )\n",
    "        self.calculate_metrics(outputs, batch_sqls)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        acc_sc, acc_sa, acc_wn, acc_wo, pp_wv = self.compute_all_metrics()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_sc\", acc_sc, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_sa\", acc_sa, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_wn\", acc_wn, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_wo\", acc_wo, prog_bar=True, logger=True)\n",
    "        self.log(\"val_pp_wv\", pp_wv, prog_bar=True, logger=True)\n",
    "        self.reset_metrics_epoch_end()\n",
    "\n",
    "    def load_data(self, sql_path: Union[Path, str], table_path: Union[Path, str]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        \"\"\"Load data from path\n",
    "\n",
    "        Args:\n",
    "            sql_path (Union[Path, str]): dataset path which contains NL with SQL queries (+answers)\n",
    "            table_path (Union[Path, str]): table information contains table name, header and values\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[Dict[str, Any]], Dict[str, Any]]: [description]\n",
    "        \"\"\"    \n",
    "        path_sql = Path(sql_path)\n",
    "        path_table = Path(table_path)\n",
    "\n",
    "        dataset = self.load_sqls(path_sql)\n",
    "        table = self.load_tables(path_table)\n",
    "\n",
    "        return dataset, table\n",
    "    \n",
    "    def load_sqls(self, path_sql):\n",
    "        dataset = []\n",
    "        with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                x = json.loads(line.strip())\n",
    "                dataset.append(x)\n",
    "        return dataset\n",
    "    \n",
    "    def load_tables(self, path_table):\n",
    "        table = {}\n",
    "        with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                x = json.loads(line.strip())\n",
    "                table[x['id']] = x\n",
    "        return table\n",
    "    \n",
    "    def create_dataloader(self, mode):\n",
    "        num_workers = 0 if os.name == \"nt\" else self.hparams.num_workers\n",
    "        if mode == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = self.hparams.train_batch_size\n",
    "            sql_file = self.hparams.train_sql_file\n",
    "            table_file = self.hparams.train_table_file\n",
    "        else:\n",
    "            shuffle = False\n",
    "            batch_size = self.hparams.eval_batch_size\n",
    "            sql_file = self.hparams.eval_sql_file\n",
    "            table_file = self.hparams.eval_table_file\n",
    "        \n",
    "        dataset, table = self.load_data(sql_file, table_file)\n",
    "        self.table_dict[mode] = table\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            batch_size=batch_size,\n",
    "            dataset=dataset,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if num_workers > 0 else False,\n",
    "            collate_fn=self._collate_fn # now dictionary values are not merged!\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def _collate_fn(self, x):\n",
    "        return x\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"eval\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model_decoder.parameters()),\n",
    "                                       lr=self.hparams.lr, weight_decay=0)\n",
    "        opt_bert = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model_bert.parameters()),\n",
    "                                    lr=self.hparams.lr_bert, weight_decay=0)\n",
    "        \n",
    "        optimizers = [opt, opt_bert]\n",
    "        return optimizers\n",
    "    \n",
    "    def predict_to_dict(self, outputs):        \n",
    "        predicts = {}\n",
    "        predicts[\"sc\"] = self.model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "        predicts[\"sa\"] = self.model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "        predicts[\"wn\"] = self.model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "        predicts[\"wc\"] = self.model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wo\"] = self.model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wv_tkns\"] = self.model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])  # (B, value_tkn_max_len) x where_nums\n",
    "        # internally wv means wv_tkns, will convert to string here using tokenizer\n",
    "        predicts[\"wv\"] = []\n",
    "        for where_idx, wv_tkns in enumerate(predicts[\"wv_tkns\"]): # iter: (B, value_tkn_max_len)\n",
    "            predicts[\"wv\"].append([self.tokenizer_bert.decode(self.totensor(batch_wv)) for batch_wv in wv_tkns])\n",
    "                \n",
    "        predicts[\"wv\"] = list(zip(*predicts[\"wv\"]))\n",
    "        \n",
    "        return predicts\n",
    "\n",
    "    def predict_outputs(self, data, table):\n",
    "        batch_qs, batch_ts = self.get_batch_data(data, table, only_question=True)\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=self.hparams.value_tkn_max_len, train=False)\n",
    "        return self.predict_to_dict(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1c200",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PyTorch Ligthtning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa512529",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64658d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    db_path = \"./private/kospi.db\",\n",
    "    model_bert_path = \"monologg/kobert\",\n",
    "    # Dataloader\n",
    "    train_sql_file = \"./newdata/train/train.jsonl\",\n",
    "    train_table_file = \"./newdata/train/train_table.jsonl\",\n",
    "    train_batch_size = 5,\n",
    "    eval_sql_file = \"./newdata/test/test.jsonl\",\n",
    "    eval_table_file = \"./newdata/test/test_table.jsonl\",\n",
    "    eval_batch_size = 5,\n",
    "    num_workers = 4,\n",
    "    # Model-decoder\n",
    "    hidden_size = 100,\n",
    "    num_layers = 2,\n",
    "    dropout_ratio = 0.3,\n",
    "    max_where_conds = 4,\n",
    "    value_tkn_max_len = 20, \n",
    "    # Tokenizer\n",
    "    max_length = 256,\n",
    "    special_start_tkn = \"[S]\", \n",
    "    special_end_tkn = \"[E]\",\n",
    "    special_col_tkn = \"[COL]\",\n",
    "    # Loss Function\n",
    "    wn_penalty = 2.0,  # scale up for guessing where number\n",
    "    # Optimizer\n",
    "    num_train = 10,\n",
    "    lr = 1e-3,\n",
    "    lr_bert = 1e-5,\n",
    "    # Seed\n",
    "    seed = 88,\n",
    "    # Records\n",
    "    task = \"TEXT2SQL_v1\",\n",
    "    ckpt_path = \"./ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9017f4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 88\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=\"epoch{epoch:02d}-{val_loss:.3f}-{val_acc_sc:.3f}-{val_acc_sa:.3f}-{val_acc_wn:.3f}-{val_acc_wo:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "earlystop_callback = pl.callbacks.EarlyStopping(\"val_loss\", mode=\"min\")\n",
    "pl.seed_everything(args_dict[\"seed\"])\n",
    "model = Text2SQL(**args_dict)\n",
    "logger = pl.loggers.TensorBoardLogger(args_dict[\"ckpt_path\"], name=args_dict[\"task\"])\n",
    "# trainer = pl.Trainer(\n",
    "#     callbacks=[checkpoint_callback, earlystop_callback],\n",
    "#     default_root_dir=args_dict[\"ckpt_path\"],\n",
    "#     max_epochs=args_dict[\"num_train\"],\n",
    "#     deterministic=torch.cuda.is_available(),\n",
    "#     gpus=-1 if torch.cuda.is_available() else None,\n",
    "#     num_sanity_val_steps=0,\n",
    "#     logger=logger\n",
    "# )\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "729e3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 88\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(args_dict[\"seed\"])\n",
    "loader = model.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a540ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5af7ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs, batch_ts, batch_sqls = model.get_batch_data(x, model.table_dict[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15aa1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = model.get_sql_answers(batch_sqls)\n",
    "gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1953980",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_inputs = model.tokenizer_bert(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=model.hparams.max_length, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    "    return_special_tokens_mask=False, \n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "472c9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_outputs = model.model_bert(**encode_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "970a2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_mask, input_db_mask, input_col_mask = model.get_input_mask(encode_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e0fd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded, question_lengths = model.get_decoder_batches(encode_outputs, input_question_mask, pad_idx=model.tokenizer_bert.pad_token_id)\n",
    "db_padded, db_lengths = model.get_decoder_batches(encode_outputs, input_db_mask, pad_idx=model.tokenizer_bert.pad_token_id)\n",
    "col_padded, col_lengths = model.get_decoder_batches(encode_outputs, input_col_mask, pad_idx=model.tokenizer_bert.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ad09f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 16, 768]), torch.Size([5, 112, 768]), torch.Size([5, 14, 768]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_padded.size(), db_padded.size(), col_padded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eefb63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_outputs, at1 = model.model_decoder.select_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, True)\n",
    "select_idxes = g_sc if g_sc else model.model_decoder.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "# select_idxes = self.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "agg_outputs, at2 = model.model_decoder.agg_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, select_idxes, True)\n",
    "\n",
    "where_num_outputs, at3  = model.model_decoder.where_num_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, True)\n",
    "where_nums = g_wn if g_wn else model.model_decoder.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "# where_nums = self.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "where_col_outputs, at4 = model.model_decoder.where_col_decoder(question_padded, db_padded, col_padded, question_lengths, col_lengths, True)\n",
    "where_col_idxes = g_wc if g_wc else model.model_decoder.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "# where_col_idxes = self.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "\n",
    "where_op_outputs, at5 = model.model_decoder.where_op_decoder(question_padded, db_padded, col_padded, question_lengths, where_nums, where_col_idxes, True)\n",
    "where_op_idxes = g_wo if g_wo else model.model_decoder.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "# where_op_idxes = self.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "where_value_outputs, at6 = model.model_decoder.where_value_decoder(question_padded, db_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, None, g_wv_tkns, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45785750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 14, 16, 13, 14]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c420bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e985f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a3bd066d00>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(at2[1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8fdd8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0716, 0.0716, 0.0716, 0.0715, 0.0714, 0.0712, 0.0712, 0.0713, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0714, 0.0000, 0.0000],\n",
       "        [0.0716, 0.0716, 0.0716, 0.0715, 0.0714, 0.0712, 0.0712, 0.0713, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0714, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0716, 0.0715, 0.0714, 0.0712, 0.0712, 0.0713, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0714, 0.0714, 0.0713, 0.0712, 0.0713, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0714, 0.0714, 0.0712, 0.0712, 0.0713, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0714, 0.0714, 0.0713, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0714, 0.0714, 0.0712, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0716, 0.0716, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0714, 0.0714, 0.0714, 0.0713, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0716, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0714, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0713,\n",
       "         0.0714, 0.0715, 0.0716, 0.0716, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0714,\n",
       "         0.0714, 0.0715, 0.0715, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0714,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0715, 0.0000, 0.0000],\n",
       "        [0.0715, 0.0715, 0.0715, 0.0715, 0.0714, 0.0712, 0.0712, 0.0714, 0.0714,\n",
       "         0.0714, 0.0715, 0.0716, 0.0715, 0.0714, 0.0000, 0.0000]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss 구할때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611cc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d528b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79138983",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = model.model_decoder(\n",
    "    question_padded, \n",
    "    db_padded, \n",
    "    col_padded, \n",
    "    question_lengths, \n",
    "    col_lengths, \n",
    "    None, \n",
    "    gold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b73f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc torch.Size([5, 14])\n",
      "sa torch.Size([5, 6])\n",
      "wn torch.Size([5, 5])\n",
      "wc torch.Size([5, 14])\n",
      "wo torch.Size([5, 4, 4])\n",
      "wv\n",
      "torch.Size([5, 5, 8005])\n",
      "torch.Size([5, 5, 8005])\n",
      "torch.Size([5, 1, 8005])\n",
      "torch.Size([5, 1, 8005])\n"
     ]
    }
   ],
   "source": [
    "for k, v in decoder_outputs.items():\n",
    "    if k != \"wv\":\n",
    "        print(k, v.size())\n",
    "    else:\n",
    "        print(k)\n",
    "        for x in v:\n",
    "            print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97199dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ccc3f",
   "metadata": {},
   "source": [
    "## Testing: Execution-guided beam decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00d802ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81066c3e",
   "metadata": {},
   "source": [
    "### SELECT beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bfb8c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_outputs, _ = model_decoder.select_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "select_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebfc002",
   "metadata": {},
   "source": [
    "construct all possible select + (agg) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "667a36b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_size, n_col = select_output.size()\n",
    "\n",
    "select_prob = torch.softmax(select_output, 1)  # prob_sc\n",
    "if n_col < beam_size:\n",
    "    beam_size_max_col = n_col\n",
    "else:\n",
    "    beam_size_max_col = beam_size\n",
    "\n",
    "prob_sc_sa = torch.zeros([batch_size, beam_size_max_col, n_agg_ops])\n",
    "prob_sca = torch.zeros_like(prob_sc_sa)\n",
    "print(prob_sca.size())  # (B, beam-size, n_agg_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "55ed42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc top k: [[14, 5, 2, 4], [2, 18, 0, 19]]\n"
     ]
    }
   ],
   "source": [
    "# beamseacrh\n",
    "_, pr_sc_beam = select_outputs.topk(k=beam_size_max_col) # pr_sc_beam: (B, beam_size)\n",
    "print(f\"sc top k: {pr_sc_beam.tolist()}\")\n",
    "\n",
    "for i_beam in range(beam_size_max_col):\n",
    "    select_idxes = pr_sc_beam[:, i_beam].tolist()  # select one of predicted index\n",
    "    agg_outputs, _ = model_decoder.agg_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, select_idxes)\n",
    "    agg_prob = torch.softmax(agg_outputs, dim=-1)  # prob_sa: (B, n_agg_ops)\n",
    "    prob_sc_sa[:, i_beam, :] = agg_prob\n",
    "    \n",
    "    prob_sc_selected = select_prob[range(batch_size), select_idx]  # (B,)\n",
    "    prob_sca[:, i_beam, :] = (agg_prob.t() * prob_sc_selected).t()  # (n_agg_ops, B) \\odot (1, B) (broadcast) -> (B, max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6dc52e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0503, 0.0503], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_sc_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "53474126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1524, 0.1822, 0.1478, 0.1937, 0.1536, 0.1703],\n",
      "         [0.1514, 0.1820, 0.1485, 0.1943, 0.1518, 0.1720],\n",
      "         [0.1518, 0.1816, 0.1489, 0.1934, 0.1524, 0.1720],\n",
      "         [0.1512, 0.1826, 0.1473, 0.1951, 0.1527, 0.1711]],\n",
      "\n",
      "        [[0.1536, 0.1789, 0.1471, 0.1923, 0.1539, 0.1742],\n",
      "         [0.1520, 0.1789, 0.1463, 0.1936, 0.1533, 0.1758],\n",
      "         [0.1519, 0.1782, 0.1498, 0.1938, 0.1528, 0.1734],\n",
      "         [0.1532, 0.1790, 0.1466, 0.1933, 0.1541, 0.1738]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sc_sa.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c11e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n",
      "tensor([[[0.0077, 0.0092, 0.0074, 0.0097, 0.0077, 0.0086],\n",
      "         [0.0076, 0.0092, 0.0075, 0.0098, 0.0076, 0.0087],\n",
      "         [0.0076, 0.0091, 0.0075, 0.0097, 0.0077, 0.0087],\n",
      "         [0.0076, 0.0092, 0.0074, 0.0098, 0.0077, 0.0086]],\n",
      "\n",
      "        [[0.0077, 0.0090, 0.0074, 0.0097, 0.0077, 0.0088],\n",
      "         [0.0077, 0.0090, 0.0074, 0.0097, 0.0077, 0.0089],\n",
      "         [0.0076, 0.0090, 0.0075, 0.0098, 0.0077, 0.0087],\n",
      "         [0.0077, 0.0090, 0.0074, 0.0097, 0.0078, 0.0087]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sca.size())  # (B, beam_size, prob_sc * prob_agg)\n",
    "print(prob_sca.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b4445c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_multi_dim(tensor, n_topk):\n",
    "    batch_size = tensor.size(0)\n",
    "    values_1d, idxes_1d = tensor.view(batch_size, -1).topk(n_topk)\n",
    "    idxes = np.stack(np.unravel_index(idxes_1d, tensor.size()[1:])).transpose(1, 2, 0)\n",
    "    values = tensor.view(batch_size, -1).gather(1, idxes_1d).numpy()\n",
    "    return idxes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "065264d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First flatten to 1-d\n",
    "if np.prod(prob_sca.shape[1:]) < beam_size:\n",
    "    beam_size_sca = np.prod(prob_sca.shape[1:])\n",
    "else:\n",
    "    beam_size_sca = beam_size\n",
    "# Now as sc_idx is already sorted, re-map them properly.\n",
    "# idxes: [sc_beam_idx, sa_idx] -> sca_idxes: [sc_idx, sa_idx]\n",
    "idxes, values = topk_multi_dim(prob_sca.detach().cpu(), n_topk=beam_size_sca)\n",
    "sc_beam_idxes = idxes[:, :, 0]\n",
    "sc_idxes = np.stack([pr_sc_beam.numpy()[i, sc_beam_idx] for i, sc_beam_idx in enumerate(sc_beam_idxes)]) # (B, beam_size)\n",
    "sca_idxes = np.stack([sc_idxes, idxes[:, :, 1]]).transpose(1, 2, 0)  # (B, beam_size, sc+sa=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c1f2e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4,  3],\n",
       "        [ 5,  3],\n",
       "        [14,  3],\n",
       "        [ 2,  3]],\n",
       "\n",
       "       [[ 0,  3],\n",
       "        [18,  3],\n",
       "        [19,  3],\n",
       "        [ 2,  3]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3732e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sc_sa_pairs(tb, pr_sc, pr_sa):\n",
    "    \"\"\"\n",
    "    tb = single table json\n",
    "    Check whether pr_sc, pr_sa are allowed pairs or not.\n",
    "    agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "    \"\"\"\n",
    "    batch_size = len(pr_sc)\n",
    "    check = [False] * batch_size\n",
    "    \n",
    "    for b, b_pr_sc in enumerate(pr_sc):\n",
    "        if len(tb['types']) <= b_pr_sc:\n",
    "            check[b] = False\n",
    "            continue\n",
    "        b_pr_sa = pr_sa[b]\n",
    "        header_types = tb['types']\n",
    "        header_type_sc = header_types[b_pr_sc].lower()\n",
    "        if header_type_sc == 'text':\n",
    "            if b_pr_sa == 0 or b_pr_sa == 3: \n",
    "                check[b] = True\n",
    "            else:\n",
    "                check[b] = False\n",
    "\n",
    "        elif header_type_sc == 'integer' or header_type_sc == 'float':\n",
    "            check[b] = True\n",
    "        else:\n",
    "            raise Exception(\"New TYPE!!\")\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d06b9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prob_sca, a joint probability\n",
    "beam_idx_sca = [0] * batch_size\n",
    "beam_meet_the_final = [False] * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db6767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pr_sc = sca_idxes[range(batch_size), beam_idx_sca, 0]\n",
    "    pr_sa = sca_idxes[range(batch_size), beam_idx_sca, 1]\n",
    "    check = check_sc_sa_pairs(table[\"receipts\"], pr_sc, pr_sa)\n",
    "    if sum(check) == batch_size:\n",
    "        break\n",
    "    else:\n",
    "        for b, b_check in enumerate(check):\n",
    "            if not b_check: # wrong pair\n",
    "                beam_idx_sca[b] += 1\n",
    "                if beam_idx_sca[b] >= beam_size_sca:\n",
    "                    beam_meet_the_final[b] = True\n",
    "                    beam_idx_sca[b] -= 1\n",
    "            else:\n",
    "                beam_meet_the_final[b] = True\n",
    "\n",
    "    if sum(beam_meet_the_final) == batch_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eb0a8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_sc_best = list(pr_sc)\n",
    "pr_sa_best = list(pr_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c66551",
   "metadata": {},
   "source": [
    "## WHERE clause beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cdefc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_num_outputs, _  = model_decoder.where_num_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "prob_wn = torch.softmax(where_num_outputs, dim=-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1427fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19943994, 0.21269168, 0.19114491, 0.21635418, 0.18036927],\n",
       "       [0.1970151 , 0.21438198, 0.19467372, 0.21400847, 0.17992072]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "004b3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_col_outputs, _ = model_decoder.where_col_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "where_nums = model_decoder.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "prob_wc = torch.sigmoid(where_col_outputs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "edb8d5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49202684, 0.4923531 , 0.49339655, 0.4926352 , 0.4921672 ,\n",
       "        0.49323654, 0.4927438 , 0.4918207 , 0.4921981 , 0.49224687,\n",
       "        0.49229336, 0.49169996, 0.48907605, 0.4890667 , 0.48958477,\n",
       "        0.4892638 , 0.48943475, 0.4897049 , 0.4910051 , 0.49259278],\n",
       "       [0.4946221 , 0.49537787, 0.49438062, 0.49485302, 0.4945096 ,\n",
       "        0.49569654, 0.49484485, 0.49506623, 0.4936429 , 0.49399838,\n",
       "        0.49496365, 0.4929868 , 0.49419218, 0.49251673, 0.49326468,\n",
       "        0.4935392 , 0.4941137 , 0.4930429 , 0.49313435, 0.4940745 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ef5ecbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_wn_max = [max_where_conds]*batch_size\n",
    "where_col_idxes = model_decoder.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fc3c356d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4890667 , 0.48907605, 0.4892638 ], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wc[b, b_where_col_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a589dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_wc_max = np.zeros([batch_size, max_where_conds])\n",
    "for b, b_where_col_idxes in enumerate(where_col_idxes):\n",
    "    prob_wc_max[b, :] = list(prob_wc[b, b_where_col_idxes]) + [0.0]*(max_where_conds - len(b_where_col_idxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0b64cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_op_outputs = model_decoder.where_op_decoder(question_padded, header_padded, col_padded, question_lengths, where_nums, where_col_idxes)\n",
    "where_op_idxes = model_decoder.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "prob_wo = torch.softmax(where_op_outputs, dim=-1).detach().cpu().numpy()  # (B, max_where_num, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773e4fd",
   "metadata": {},
   "source": [
    "writing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "67cd48f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4890667 , 0.48907605, 0.4892638 ], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wc[0, [13, 12, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43456263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ebc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First flatten to 1-d\n",
    "if np.prod(prob_sca.shape[1:]) < beam_size:\n",
    "    beam_size_sca = np.prod(prob_sca.shape[1:])\n",
    "else:\n",
    "    beam_size_sca = beam_size\n",
    "idxs_s, values_s = topk_multi_dim(torch.tensor(prob_sca), n_topk=beam_size_sca, batch_exist=True)\n",
    "# Now as sc_idx is already sorted, re-map them properly.\n",
    "\n",
    "idxs_s = remap_sc_idx(idxs_s, pr_sc_beam) # [sc_beam_idx, sa_idx] -> [sc_idx, sa_idx]\n",
    "idxs_arr = array(idxs_s)\n",
    "# [B, beam_size, remainig dim]\n",
    "# idxs[b][0] gives first probable [sc_idx, sa_idx] pairs.\n",
    "# idxs[b][1] gives of second.\n",
    "\n",
    "# Calculate prob_sca, a joint probability\n",
    "beam_idx_sca = [0] * bS\n",
    "beam_meet_the_final = [False] * bS\n",
    "while True:\n",
    "    pr_sc = idxs_arr[range(bS),beam_idx_sca,0]\n",
    "    pr_sa = idxs_arr[range(bS),beam_idx_sca,1]\n",
    "\n",
    "    # map index properly\n",
    "\n",
    "    check = check_sc_sa_pairs(tb, pr_sc, pr_sa)\n",
    "\n",
    "    if sum(check) == bS:\n",
    "        break\n",
    "    else:\n",
    "        for b, check1 in enumerate(check):\n",
    "            if not check1: # wrong pair\n",
    "                beam_idx_sca[b] += 1\n",
    "                if beam_idx_sca[b] >= beam_size_sca:\n",
    "                    beam_meet_the_final[b] = True\n",
    "                    beam_idx_sca[b] -= 1\n",
    "            else:\n",
    "                beam_meet_the_final[b] = True\n",
    "\n",
    "    if sum(beam_meet_the_final) == bS:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
