{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816bf2a7",
   "metadata": {},
   "source": [
    "# TEXT2SQL with transformers\n",
    "\n",
    "Lee Woo Chul, Jang Ji Soo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236ef46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba49d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os\n",
    "\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69a871-f39c-470d-8789-8bddbb004ae1",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "`NLSQL.jsonl` and `\"table.jsonl` contains the data like following format same with [WikiSQL](https://github.com/salesforce/WikiSQL), Please follow the [link](https://github.com/salesforce/WikiSQL#content-and-format) to see what are the keys mean.\n",
    "\n",
    "```json\n",
    "// example of 'NLSQL.jsonl'\n",
    "{\n",
    "    \"phase\": 1, \n",
    "    \"question\": \"2015 삼성전자 유동자산은 어떻게 돼?\", \n",
    "    \"table_id\": \"receipts\", \n",
    "    \"sql\": {\n",
    "        \"sel\": 16, \n",
    "        \"agg\": 0, \n",
    "        \"conds\": [[10, 0, \"유동자산\"], [3, 0, 2016]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34066108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sql_path, table_path):\n",
    "    path_sql = Path(sql_path)\n",
    "    path_table = Path(table_path)\n",
    "\n",
    "    dataset = []\n",
    "    table = {}\n",
    "    with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            dataset.append(x)\n",
    "\n",
    "    with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            table[x['id']] = x\n",
    "            \n",
    "    return dataset, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a708bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, table = load_data(\"NLSQL.jsonl\", \"table.jsonl\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    dataset=data,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: x # now dictionary values are not merged!\n",
    ")\n",
    "# Load DBEngine\n",
    "db_path = Path(\"./private\")\n",
    "dbengine = DBEngine(db_path / \"samsung_new.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8557c773-e0b9-419b-9d2f-65cee290d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116a4a4e1f8f4cdebbd8af26a8ae5d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test with toy data:   0%|          | 0/21120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch_data in enumerate(tqdm(data_loader, desc=\"Test with toy data\")):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a7f80",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Used BERT in hugging Face with KoBERT\n",
    "\n",
    "- https://github.com/SKTBrain/KoBERT\n",
    "- https://github.com/monologg/KoBERT-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cbc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "def get_bert(model_path: str, device: str, output_hidden_states: bool=False):\n",
    "    special_tokens = [\"[S]\", \"[E]\", \"[COL]\"] # sequence start, sequence end, column tokens\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    config = BertConfig.from_pretrained(model_path)\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.output_hidden_states = output_hidden_states\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d38dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"monologg/kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "model_bert, tokenizer_bert, config_bert = get_bert(model_path=model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7acbed4-ff69-4af6-8457-8396947e5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data, dbengine):\n",
    "    batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "    tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "    batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "    batch_ts = []\n",
    "    for table_id in tid:\n",
    "        dbengine.get_schema_info(table_id)\n",
    "        table_str = f\"{table_id}\" + \"\".join([\n",
    "            f\"[COL]{col}\" for col in dbengine.schema\n",
    "        ]) \n",
    "        batch_ts.append(table_str)\n",
    "    \n",
    "    return batch_qs, batch_sqls, batch_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74243aaf-575d-477b-a819-48fc25431cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs, batch_sqls, batch_ts = get_batch_data(batch_data, dbengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25590739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Input\n",
    "encode_input = tokenizer_bert(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    "    return_special_tokens_mask=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a9e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 삼성전자의 2017년도 영업이익이 어때?[SEP] receipts [COL] index [COL] rcept_no [COL] reprt_code [COL] bsns_year [COL] corp_code [COL] stock_code [COL] fs_div [COL] fs_nm [COL] sj_div [COL] sj_nm [COL] account_nm [COL] thstrm_nm [COL] thstrm_dt [COL] thstrm_amount [COL] frmtrm_nm [COL] frmtrm_dt [COL] frmtrm_amount [COL] bfefrmtrm_nm [COL] bfefrmtrm_dt [COL] bfefrmtrm_amount[SEP][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "# Show an Example of Input\n",
    "print(tokenizer_bert.decode(encode_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "baab9434-fcd8-4ff7-8f42-782f9d08da85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoBertTokenizer.KoBertTokenizer"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a7df8-20e8-4e25-bb2d-a43e9b44efe6",
   "metadata": {},
   "source": [
    "## Prepare for decoder Inputs: Createing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73371609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_input_mask(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    r\"\"\"\n",
    "    input should only contains word tokens:\n",
    "    \"\"\"\n",
    "    start_tkn_mask = input_ids == start_tkn_id\n",
    "    end_tkn_mask = input_ids == end_tkn_id\n",
    "    start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "    return mask.scatter(1, index, False)\n",
    "\n",
    "def get_input_mask_and_answer(encode_input, tokenizer):\n",
    "    r\"\"\"\n",
    "    table -> database table name(id)\n",
    "    header -> database header\n",
    "    \n",
    "    returns:\n",
    "        input_question_mask, input_table_mask, input_header_mask, answer_table_tkns, answer_header_tkns\n",
    "    \"\"\"\n",
    "    batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "    sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "    start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "    \n",
    "    input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "    input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "    input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "    db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "    db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "    col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "    db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "    # split table_mask and header_mask\n",
    "    input_idx = torch.arange(max_length).repeat(batch_size, 1)\n",
    "    db_idx = input_idx[db_mask]\n",
    "    table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "    table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "    start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "    table_end_sep_idx = start_idx[:, 0] - 1\n",
    "    split_size = torch.stack([\n",
    "        table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "    ]).transpose(0, 1)\n",
    "\n",
    "    # Token idx\n",
    "    table_tkn_idx, header_tkn_idx = map(\n",
    "        lambda x: torch.stack(x), \n",
    "        zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "    )\n",
    "\n",
    "    table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "    # Mask include [S] & [E] tokens\n",
    "    table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, table_tkn_idx, True)\n",
    "    header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "    # For Decoder Input, Maskout [S], [E] for table & header  \n",
    "    input_table_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    input_header_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    # [COL] token mask: this is for attention\n",
    "    col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "    input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "    return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a25c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_mask, input_table_mask, input_header_mask, input_col_mask = get_input_mask_and_answer(encode_input, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5412574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tokens for Decoder\n",
      "-------------------------\n",
      "삼성전자의 2017년도 영업이익이 어때? 삼성전자 2019의 이익잉여금은 어때?\n",
      "\n",
      "Table Tokens for Decoder\n",
      "-------------------------\n",
      "receipts receipts\n",
      "\n",
      "Header Tokens for Decoder\n",
      "-------------------------\n",
      "index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount\n",
      "\n",
      "Column(Index of Headers) Tokens for Decoder\n",
      "-------------------------\n",
      "[COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, t in zip(\n",
    "        [input_question_mask, input_table_mask, input_header_mask, input_col_mask], \n",
    "        [\"Question Tokens for Decoder\", \"Table Tokens for Decoder\", \"Header Tokens for Decoder\", \"Column(Index of Headers) Tokens for Decoder\"]\n",
    "    ):\n",
    "    print(t)\n",
    "    print(\"-----\"*5)\n",
    "    print(tokenizer_bert.decode(encode_input[\"input_ids\"][m]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec38c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed to BERT Model\n",
    "encode_outputs = model_bert(**encode_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741735a8-71a8-474f-9379-be39e51f88be",
   "metadata": {},
   "source": [
    "\n",
    "The `encode_outputs` will be selected by 4 types of masks\n",
    "```\n",
    "encode_outputs\n",
    "-> Question\n",
    "-> Table\n",
    "-> Header\n",
    "-> Column(Index of Headers)\n",
    "```\n",
    "\n",
    "And pad batches which has less tokens than max length with \"\\[PAD\\]\"  for Decoder Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f27e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batches: Tuple[torch.Tensor], lengths: List[int], model: BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "    padded = []\n",
    "    max_length = max(lengths)\n",
    "    for x in batches:\n",
    "        if len(x) < max_length:\n",
    "            pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "            padded.append(torch.cat([x, pad_tensor]))\n",
    "        else:\n",
    "            padded.append(x)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "def get_decoder_batches(encode_output, mask, model, pad_idx):\n",
    "    lengths = mask.sum(1)\n",
    "    tensors = encode_output.last_hidden_state[mask, :]\n",
    "    batches = torch.split(tensors, lengths.tolist())\n",
    "    if lengths.ne(lengths.max()).sum().item() != 0:\n",
    "        # pad not same length tokens\n",
    "        tensors_padded = pad(batches, lengths.tolist(), model, pad_idx=pad_idx)\n",
    "    else:\n",
    "        # just stack the splitted tensors\n",
    "        tensors_padded = torch.stack(batches)\n",
    "    return tensors_padded, lengths.tolist()\n",
    "\n",
    "def get_pad_mask(lengths):\n",
    "    batch_size = len(lengths)\n",
    "    max_len = max(lengths)\n",
    "    mask = torch.ones(batch_size, max_len)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca850272",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded, question_lengths = get_decoder_batches(encode_outputs, input_question_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "table_padded, table_lengths = get_decoder_batches(encode_outputs, input_table_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "header_padded, header_lengths = get_decoder_batches(encode_outputs, input_header_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "col_padded, col_lengths = get_decoder_batches(encode_outputs, input_col_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2efe5c0b-e737-4ef6-803c-e0f0ef1dbe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e905945-3e87-480b-aa5f-810921a59410",
   "metadata": {},
   "source": [
    "## Create the Answers for decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8611c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_answers(batch_sqls, tokenizer, end_tkn_idx=1):\n",
    "    \"\"\"\n",
    "    for backward compatibility, separated with get_g\n",
    "    \n",
    "    sc: select column\n",
    "    sa: select agg\n",
    "    wn: where number\n",
    "    wc: where column\n",
    "    wo: where operator\n",
    "    wv: where value\n",
    "    \"\"\"\n",
    "\n",
    "    get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, sql_dict in enumerate(batch_sqls):\n",
    "        g_sc.append( sql_dict[\"sel\"] )\n",
    "        g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "        conds = sql_dict[\"conds\"]\n",
    "        if not sql_dict[\"agg\"] < 0:\n",
    "            g_wn.append( len(conds) )\n",
    "            g_wc.append( get_ith_element(conds, 0) )\n",
    "            g_wo.append( get_ith_element(conds, 1) )\n",
    "            g_wv.append( get_ith_element(conds, 2) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    \n",
    "    # get where value tokenized \n",
    "    end_tkn = tokenizer.additional_special_tokens[end_tkn_idx]\n",
    "    pad_tkn_id = tokenizer.pad_token_id\n",
    "    g_wv_tkns = [[f\"{s}{end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "    g_wv_tkns = [tokenizer(batch_wv, add_special_tokens=False)[\"input_ids\"] for batch_wv in g_wv_tkns]\n",
    "    # add empty list if batch has different where column number\n",
    "    max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "    g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "    temp = []\n",
    "    for batch_wv in list(zip(*g_wv_tkns)):\n",
    "        batch_max_len = max(map(len, batch_wv))\n",
    "        batch_temp = []\n",
    "        for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "            if len(wv_tkns) < batch_max_len:\n",
    "                batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "            else:\n",
    "                batch_temp.append(wv_tkns)\n",
    "        temp.append(batch_temp)\n",
    "    g_wv_tkns = list(zip(*temp))\n",
    "\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0352ed82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 16],\n",
       " [0, 0],\n",
       " [2, 2],\n",
       " [[10, 3], [10, 3]],\n",
       " [[0, 0], [0, 0]],\n",
       " [['영업이익', 2018], ['이익잉여금', 2020]],\n",
       " [([3383, 8003, 1, 1, 1], [554, 115, 8003]),\n",
       "  ([3736, 7144, 6916, 5550, 8003], [554, 127, 8003])])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert, 1)\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d30e9-5b95-450a-8fd6-c7fb59abaf07",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Similar structure in SQLova but a little difference in here.\n",
    "\n",
    "- SQLova is a neural semantic parser translating natural language utterance to SQL query.\n",
    "- Official Github: [https://github.com/naver/sqlova](https://github.com/naver/sqlova)\n",
    "- Paper: [A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization](https://arxiv.org/abs/1902.01069)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PW9oAXfW-ZI-jxGn5q9O_gzUIZnNYaet\" alt=\"Sqlova Decoder Architecture \" width=\"100%\" height=\"auto\">\n",
    "\n",
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72f3ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def wipe_out_pad_tkn_score(self, score, lengths, dim=2):\n",
    "        max_len = max(lengths)\n",
    "        for batch_idx, length in enumerate(lengths):\n",
    "            if length < max_len:\n",
    "                if dim == 2:\n",
    "                    score[batch_idx, :, length:] = -10000000\n",
    "                elif dim == 1:\n",
    "                    score[batch_idx, length:, :] = 0.0\n",
    "                else:\n",
    "                    raise ValueError(f\"`dim` in wipe_out_pad_tkn_score should be 1 or 2\")\n",
    "        return score \n",
    "\n",
    "\n",
    "class C2QAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Column to Question Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, o_c, o_q, q_lengths, c_lengths=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each column tokens, How much related to question tokens?\n",
    "        \n",
    "        o_c: LSTM output of column\n",
    "        o_q: LSTM output of question \n",
    "        \n",
    "        c_lengths: wipe out row length\n",
    "        return context atttended to question tokens\n",
    "        \"\"\"\n",
    "        sqrt_H = torch.sqrt(torch.FloatTensor([o_c.size(-1)], device=o_c.device))  # Apply Attention is All you Need Technique\n",
    "        o_q_transform = self.linear(o_q)  # (B, T_q, H)\n",
    "        score_c2q = torch.bmm(o_c, o_q_transform.transpose(1, 2)) / sqrt_H  # (B, T_c, H) x (B, H, T_q) = (B, T_c, T_q)\n",
    "        score_c2q = self.wipe_out_pad_tkn_score(score_c2q, q_lengths, dim=2)\n",
    "        \n",
    "        prob_c2q = self.softmax(score_c2q)\n",
    "        if c_lengths is not None:\n",
    "            prob_c2q = self.wipe_out_pad_tkn_score(prob_c2q, c_lengths, dim=1)\n",
    "        # prob_c2q: (B, T_c, T_q) -> (B, T_c, T_q, 1)\n",
    "        # o_q: (B, 1, T_q, H)\n",
    "        # p_col2question \\odot o_q = (B, T_c, T_q, 1) \\odot (B, 1, T_q, H) = (B, T_c, T_q, H)\n",
    "        # -> reduce sum to T_q to get context for each column (B, T_c, H)\n",
    "        context = torch.mul(prob_c2q.unsqueeze(3), o_q.unsqueeze(1)).sum(dim=2)\n",
    "        if rt_attn:\n",
    "            attn = prob_c2q\n",
    "        else:\n",
    "            attn = None\n",
    "        return context, attn\n",
    "\n",
    "class SelfAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Self Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, o, lengths, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each o tokens, How much related to o tokens?\n",
    "        \n",
    "        return attended summary of o\n",
    "        \"\"\"\n",
    "        o_transform = self.linear(o)  # (B, T_o, H) -> (B, T_o, 1)\n",
    "        o_transform = self.wipe_out_pad_tkn_score(o_transform, lengths) \n",
    "        o_prob = self.softmax(o_transform)  # (B, T_o, 1)\n",
    "        \n",
    "        o_summary = torch.mul(o, o_prob).sum(1)  # (B, T_o, H) \\odot (B, T_o, 1) -> (B, H)\n",
    "\n",
    "        if rt_attn:\n",
    "            attn = o_prob\n",
    "        else:\n",
    "            attn = None\n",
    "        return o_summary, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19195ef5-5b46-422c-99c7-9450ed5db331",
   "metadata": {},
   "source": [
    "## Decoder Sub Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d44e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectDecoder(nn.Module):\n",
    "    r\"\"\"SELECT Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "\n",
    "class AggDecoder(nn.Module):\n",
    "    r\"\"\"AGG Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], col_lengths: List[int], select_idxes: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        \n",
    "        col_selected = col_context[list(range(batch_size)), select_idxes].unsqueeze(1)  # col_selected: (B, 1, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_selected, o_q, question_lengths, col_lengths, rt_attn)  # (B, 1, H), (B, 1, T_q)\n",
    "        output = self.output_layer(col_q_context.squeeze(1))\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    \n",
    "class WhereNumDecoder(nn.Module):\n",
    "    r\"\"\"WHERE number Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        if self.output_size > self.max_where_conds+1:\n",
    "            # HERE output will be dilivered to cross-entropy loss, not guessing the real number of where clause\n",
    "            raise ValueError(f\"`WhereNumDecoder` only support maximum {max_where_conds} where clause\")\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_self_attn = SelfAttention(2*hidden_size, 1)\n",
    "        self.lstm_q_hidden_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.lstm_q_cell_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        self.context_self_attn = SelfAttention(hidden_size, 1)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "\n",
    "        col_self_attn, col_attn = self.col_self_attn(col_context, col_lengths, rt_attn)  # (B, 2H), (B, T_c)\n",
    "\n",
    "        h_0 = self.lstm_q_hidden_init_linear(col_self_attn)  # (B, 2H)\n",
    "        h_0 = h_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        c_0 = self.lstm_q_cell_init_linear(col_self_attn)  # (B, 2H)\n",
    "        c_0 = c_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded, (h_0, c_0))  # o_q: (B, T_q, H)\n",
    "        o_summary, o_attn = self.context_self_attn(o_q, question_lengths, rt_attn)  # (B, H), (B, T_q)\n",
    "        output = self.output_layer(o_summary)\n",
    "        \n",
    "        return output, (col_attn, o_attn)\n",
    "\n",
    "    \n",
    "class WhereColumnDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Column Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int=1, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "    \n",
    "class WhereOpDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Opperator Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is settled at WhereColumnDecoder, but it can be lower than or equal to `max_where_conds`\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context_padded], dim=2)  # (B, max_where_col_nums, 2H)\n",
    "        output = self.output_layer(vec)  # (B, max_where_col_nums, n_cond_ops)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        return output\n",
    "        \n",
    "    def get_context_padded(self, col_context, where_nums, where_col_idxes):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size, device=col_context.device)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    \n",
    "class WhereValueDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Value Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4, n_cond_ops: int=4,\n",
    "                 start_tkn_id=8002, end_tkn_id=8003, embedding_layer=None) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        \n",
    "        self.start_tkn_id = start_tkn_id\n",
    "        self.end_tkn_id = end_tkn_id\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.where_op_linear = nn.Linear(n_cond_ops, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        if embedding_layer is None:\n",
    "            raise KeyError(\"Must initialize the embedding_layer to BertModel's word embedding layer\")\n",
    "        else:\n",
    "            if not isinstance(embedding_layer, torch.nn.modules.sparse.Embedding):\n",
    "                embedding_layer = embedding_layer.word_embeddings\n",
    "            self.embedding_layer = embedding_layer\n",
    "            vocab_size, bert_hidden_size = embedding_layer.weight.data.size()\n",
    "            self.output_lstm_hidden_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm_cell_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm = nn.LSTM(bert_hidden_size, bert_hidden_size, 1, batch_first=True)\n",
    "            self.output_linear = nn.Linear(bert_hidden_size, vocab_size)\n",
    "            self.output_linear.weight.data = embedding_layer.weight.data\n",
    "\n",
    "        \n",
    "    def forward(self, question_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], where_op_idxes: List[List[int]], value_tkn_max_len=None, g_wv_tkns=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is setted at WhereColumnDecoder\n",
    "        value_tkn_max_len = Test if None else Train\n",
    "        g_wv_tkns = When Train should not be None\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes)  # (B, max_where_col_nums, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        where_op_one_hot_padded = self.get_where_op_one_hot_padded(where_op_idxes, where_nums, where_col_idxes, n_cond_ops=self.n_cond_ops)#.to(o_q.device)  # (B, max_where_col_nums, n_cond_ops)\n",
    "        where_op = self.where_op_linear(where_op_one_hot_padded)  # (B, max_where_col_nums, H)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context_padded, where_op], dim=2)  # (B, max_where_col_nums, 3H)\n",
    "        max_where_col_nums = vec.size(1)\n",
    "        # predict each where_col\n",
    "        total_scores = []\n",
    "        for i in range(max_where_col_nums):\n",
    "            g_wv_tkns_i = torch.LongTensor([g_wv_tkns[b_idx][i] for b_idx in range(batch_size)]) if g_wv_tkns is not None else None  # (B, T_d_i)\n",
    "            vec_i = vec[:, i, :]  # (B, 3H)\n",
    "            \n",
    "            h_0 = self.output_lstm_hidden_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            c_0 = self.output_lstm_cell_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            \n",
    "            scores = self.decode_single_where_col(batch_size, h_0, c_0, value_tkn_max_len=value_tkn_max_len, g_wv_tkns_i=g_wv_tkns_i)  # (B, T_d_i, vocab_size)\n",
    "            total_scores.append(scores)\n",
    "        \n",
    "        # total_scores: [(B, T_d_i, vocab_size)] x max_where_col_nums\n",
    "        return total_scores\n",
    "    \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.start_tkn_id]*batch_size).unsqueeze(1)  # (B, 1)\n",
    "        return sos\n",
    "    \n",
    "    def decode_single_where_col(self, batch_size, h_0, c_0, value_tkn_max_len=None, g_wv_tkns_i=None):\n",
    "        if value_tkn_max_len is None:\n",
    "            # [Training] set the max length to gold token max length (already padded)\n",
    "            max_len = len(g_wv_tkns_i[0])\n",
    "        else:\n",
    "            # [Testing]  don't know the max length\n",
    "            max_len = value_tkn_max_len\n",
    "            \n",
    "        sos = self.start_token(batch_size)  # (B, 1)\n",
    "        emb = self.embedding_layer(sos)  # (B, 1, bert_H)\n",
    "        scores = [] \n",
    "        for i in range(max_len):\n",
    "            o, (h, c) = self.output_lstm(emb, (h_0, c_0))  # h: (1, B, bert_H)  \n",
    "            s = self.output_linear(h[-1, :]) # select last layer if use multiple rnn layers, h: (1, B, bert_H) -> (B, bert_H) -> s: (B, vocab_size)\n",
    "            scores.append(s)\n",
    "            \n",
    "            if g_wv_tkns_i is not None:\n",
    "                # [Training] Teacher Force model\n",
    "                pred = g_wv_tkns_i[:, i]  # (B, )\n",
    "            else:\n",
    "                # [Testing]\n",
    "                pred = s.argmax(1)  # (1,) only for single batch_size\n",
    "                if pred.item() == self.end_tkn_id:\n",
    "                    break\n",
    "                    \n",
    "            emb = self.embedding_layer(pred.unsqueeze(1))  # (B, 1, bert_H)\n",
    "        \n",
    "        return torch.stack(scores).transpose(0, 1).contiguous() # (T_d_i, B, vocab_size) -> (B, T_d_i, vocab_size)\n",
    "        \n",
    "    def get_context_padded(self, col_context: torch.Tensor, where_nums: List[int], where_col_idxes: List[List[int]]):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    \n",
    "    def get_where_op_one_hot_padded(self, where_op_idxes: List[List[int]], where_nums: List[int], where_col_idxes: List[List[int]], n_cond_ops: int):\n",
    "        r\"\"\"\n",
    "        Turn where operation indexs into one hot encoded vectors\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [torch.zeros(where_num, n_cond_ops).scatter(1, torch.LongTensor(batch_col).unsqueeze(1), 1) for where_num, batch_col in zip(where_nums, where_op_idxes)]  \n",
    "        # batches = [(where_col_nums, n_cond_ops), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), n_cond_ops)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab5182-0380-4b7c-a64a-875652f5c26e",
   "metadata": {},
   "source": [
    "## Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a521f21-286e-4e7c-b652-c58f6b1849fa",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "853d1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_bert.hidden_size\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "dropout_ratio = 0.3\n",
    "max_where_conds = 4\n",
    "n_agg_ops = len(dbengine.agg_ops)\n",
    "n_cond_ops = len(dbengine.cond_ops)\n",
    "start_tkn_id = tokenizer_bert.additional_special_tokens_ids[0]\n",
    "end_tkn_id = tokenizer_bert.additional_special_tokens_ids[1]\n",
    "embedding_layer = model_bert.embeddings.word_embeddings\n",
    "train = True\n",
    "if train:\n",
    "    value_tkn_max_len = None\n",
    "else:\n",
    "    value_tkn_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df37ab32-ae22-46fc-87f4-dcd5f5f89410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, value_tkn_max_len, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.select_decoder = SelectDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.agg_decoder = AggDecoder(\n",
    "            input_size, hidden_size, output_size=n_agg_ops, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_num_decoder = WhereNumDecoder(\n",
    "            input_size, hidden_size, output_size=(max_where_conds+1), num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_col_decoder = WhereColumnDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_op_decoder = WhereOpDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_value_decoder = WhereValueDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds, \n",
    "            n_cond_ops=n_cond_ops, start_tkn_id=start_tkn_id, end_tkn_id=end_tkn_id, embedding_layer=embedding_layer\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths, col_lengths, g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns):\n",
    "        decoder_outputs = {}\n",
    "\n",
    "        select_outputs, _ = self.select_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        select_idxes = g_sc if g_sc else predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "        agg_outputs, _ = self.agg_decoder(question_padded, col_padded, question_lengths, col_lengths, select_idxes)\n",
    "\n",
    "        where_num_outputs, _  = self.where_num_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_nums = g_wn if g_wn else predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "        where_col_outputs, _ = self.where_col_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_col_argsort = torch.sigmoid(where_col_outputs).argsort(1)\n",
    "        where_col_idxes = g_wc if g_wc else predict_decoder(\"wc\", where_col_argsort=where_col_argsort, where_nums=where_nums)\n",
    "\n",
    "        where_op_outputs = self.where_op_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes)\n",
    "        where_op_idxes = g_wo if g_wo else predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_value_outputs = self.where_value_decoder(question_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns)\n",
    "\n",
    "        decoder_outputs = {\n",
    "            \"sc\": select_outputs,\n",
    "            \"sa\": agg_outputs,\n",
    "            \"wn\": where_num_outputs,\n",
    "            \"wc\": where_col_outputs,\n",
    "            \"wo\": where_op_outputs,\n",
    "            \"wv\": where_value_outputs\n",
    "        }\n",
    "        \n",
    "        return decoder_outputs\n",
    "        \n",
    "    def predict_decoder(typ, **kwargs):\n",
    "        r\"\"\"\n",
    "        if not using teacher force model will use this function to predict answer\n",
    "        \"\"\"\n",
    "        if typ == \"sc\":  # SELECT column\n",
    "            select_outputs = kwargs[\"select_outputs\"]\n",
    "            return select_outputs.argmax(1).tolist()\n",
    "        elif typ == \"sa\":  # SELECT aggregation operator\n",
    "            # not need actually\n",
    "            agg_outputs = kwargs[\"agg_outputs\"]\n",
    "            return agg_outputs.argmax(1)\n",
    "        elif typ == \"wn\":  # WHERE number\n",
    "            where_num_outputs = kwargs[\"where_num_outputs\"]\n",
    "            return where_num_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wc\":  # WHERE clause column\n",
    "            where_col_argsort = kwargs[\"where_col_argsort\"]\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_col_idxes = [where_col_argsort[b_idx, :w_num].tolist() for b_idx, w_num in enumerate(where_nums)]\n",
    "            return where_col_idxes\n",
    "        elif typ == \"wo\":  # WHERE clause operator\n",
    "            where_op_outputs = kwargs[\"where_op_outputs\"]\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_op_idxes = [where_op_outputs.argmax(2)[b_idx, :w_num].tolist() for b_idx, w_num in enumerate(where_nums)]\n",
    "            return where_op_idxes\n",
    "        elif typ == \"wv\":  # WHERE clause value\n",
    "            # not need actually\n",
    "            where_value_outputs = kwargs[\"where_value_outputs\"]\n",
    "            return [o.argmax(2) for o in where_value_outputs]\n",
    "        else:\n",
    "            raise KeyError(\"`typ` must be in ['sc', 'sa', 'wn', 'wc', 'wo', 'wv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21781a87-d1f6-43af-b87f-30ce37205fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, value_tkn_max_len, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4d6fd89-557a-45b8-92ef-8a1c84b49859",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = model(question_padded, header_padded, col_padded, question_lengths, col_lengths, g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7a5fc94-d857-4d6d-be0b-abbed5f8f7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[0.0293, 0.0333, 0.0291, 0.0303, 0.0326, 0.0241, 0.0281, 0.0251, 0.0200,\n",
       "          0.0293, 0.0290, 0.0245, 0.0304, 0.0330, 0.0350, 0.0344, 0.0328, 0.0284,\n",
       "          0.0287, 0.0300],\n",
       "         [0.0249, 0.0244, 0.0189, 0.0174, 0.0193, 0.0153, 0.0147, 0.0188, 0.0083,\n",
       "          0.0152, 0.0211, 0.0284, 0.0196, 0.0206, 0.0239, 0.0259, 0.0250, 0.0226,\n",
       "          0.0247, 0.0204]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[-0.1265, -0.0473,  0.0327,  0.0128, -0.0155,  0.0326],\n",
       "         [-0.0944, -0.0563,  0.0321,  0.0297, -0.0045,  0.0324]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[ 0.0133, -0.0735,  0.0406, -0.0394,  0.0236],\n",
       "         [ 0.0189, -0.0583,  0.0404, -0.0418,  0.0382]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[ 9.3779e-03,  3.8128e-03,  1.5921e-03,  2.8032e-03, -2.9681e-03,\n",
       "          -2.8121e-03, -3.1518e-03, -1.6126e-03,  2.7844e-03,  6.7169e-03,\n",
       "           4.4253e-03,  4.8969e-03,  6.2904e-03,  5.4647e-03,  2.8376e-03,\n",
       "           3.4574e-03,  3.9735e-03,  2.1127e-03,  5.3105e-03,  4.3461e-05],\n",
       "         [ 7.0859e-03,  2.8699e-03,  2.5553e-03,  4.3631e-03,  3.0086e-03,\n",
       "           7.8377e-03,  7.6928e-03,  4.0429e-03,  2.8130e-03,  2.6239e-03,\n",
       "          -2.6636e-04, -5.0939e-04,  3.6777e-03,  8.6452e-03, -5.3523e-04,\n",
       "           2.6243e-03,  2.4482e-04, -3.6540e-03, -2.3949e-03, -7.4172e-03]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[ 0.0053,  0.0688,  0.1008,  0.0200],\n",
       "          [ 0.0044,  0.0632,  0.0927,  0.0184]],\n",
       " \n",
       "         [[ 0.0083,  0.0711,  0.0914, -0.0027],\n",
       "          [ 0.0092,  0.0662,  0.0918, -0.0072]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[ 0.1128,  0.0258, -0.0035,  ...,  0.0149,  0.0070, -0.0188],\n",
       "           [ 0.1049,  0.0261,  0.0030,  ...,  0.0153,  0.0094, -0.0192],\n",
       "           [ 0.1066,  0.0211, -0.0037,  ...,  0.0159,  0.0040, -0.0156],\n",
       "           [ 0.1087,  0.0328, -0.0001,  ...,  0.0150,  0.0069, -0.0142],\n",
       "           [ 0.1087,  0.0328, -0.0001,  ...,  0.0150,  0.0069, -0.0142]],\n",
       "  \n",
       "          [[ 0.1126,  0.0322, -0.0039,  ...,  0.0145,  0.0075, -0.0215],\n",
       "           [ 0.1096,  0.0508, -0.0035,  ...,  0.0196,  0.0037, -0.0176],\n",
       "           [ 0.0989,  0.0313, -0.0051,  ...,  0.0086,  0.0074, -0.0192],\n",
       "           [ 0.1134,  0.0290, -0.0003,  ...,  0.0196,  0.0087, -0.0213],\n",
       "           [ 0.1110,  0.0026, -0.0056,  ...,  0.0136,  0.0073, -0.0143]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[ 0.1112,  0.0274, -0.0034,  ...,  0.0159,  0.0057, -0.0196],\n",
       "           [ 0.1114,  0.0494,  0.0056,  ...,  0.0136,  0.0032, -0.0194],\n",
       "           [ 0.1063,  0.0346,  0.0003,  ...,  0.0193,  0.0091, -0.0111]],\n",
       "  \n",
       "          [[ 0.1137,  0.0382, -0.0013,  ...,  0.0167,  0.0072, -0.0224],\n",
       "           [ 0.1140,  0.0600,  0.0077,  ...,  0.0144,  0.0047, -0.0223],\n",
       "           [ 0.1233,  0.0495,  0.0044,  ...,  0.0169,  0.0086, -0.0170]]],\n",
       "         grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec147ee-0c67-48fd-9494-ffae33815592",
   "metadata": {},
   "source": [
    "# Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20fdbd9d-ae01-49a5-bc9a-9a0280e15ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac72d5-e7c7-484b-b022-90cb1e87895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningDataModule):\n",
    "    def __init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c4a07-ee09-458b-ac82-a4d0f48049f9",
   "metadata": {},
   "source": [
    "## Traning\n",
    "\n",
    "Stil Working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8cb4a-e3c8-4fc4-a492-7ea955cf74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr_bert = 1e-5\n",
    "\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=lr, weight_decay=0)\n",
    "opt_bert = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_bert.parameters()),\n",
    "                            lr=lr_bert, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347e1b6",
   "metadata": {},
   "source": [
    "## Testing: Execution-guided beam decoding\n",
    "\n",
    "Stil Working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1bf2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d964f",
   "metadata": {},
   "source": [
    "select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b836ddaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_output, _ = select_decoder(question_padded, header_padded, col_padded, question_lengths)\n",
    "select_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0dc2d",
   "metadata": {},
   "source": [
    "construct all possible select + (agg) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "545035ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_size, n_col = select_output.size()\n",
    "\n",
    "select_prob = torch.softmax(select_output, 1)  # prob_sc\n",
    "if n_col < beam_size:\n",
    "    beam_size_max_col = n_col\n",
    "else:\n",
    "    beam_size_max_col = beam_size\n",
    "\n",
    "prob_sc_sa = torch.zeros([batch_size, beam_size_max_col, n_agg_ops])\n",
    "prob_sca = torch.zeros_like(prob_sc_sa)\n",
    "print(prob_sca.size())  # (B, beam-size, n_agg_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a40f72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc top k: [[19, 18, 0, 14], [3, 4, 6, 5]]\n"
     ]
    }
   ],
   "source": [
    "# beamseacrh\n",
    "_, pr_sc_beam = select_output.topk(k=beam_size_max_col)\n",
    "print(f\"sc top k: {pr_sc_beam.tolist()}\")\n",
    "\n",
    "for i_beam in range(beam_size_max_col):\n",
    "    select_idx = pr_sc_beam[:, i_beam].tolist() # pr_sc\n",
    "    agg_output, _ = agg_decoder(question_padded, col_padded, question_lengths, select_idx)\n",
    "    agg_prob = torch.softmax(agg_output, dim=-1)  # prob_sa: (B, n_agg_ops)\n",
    "    prob_sc_sa[:, i_beam, :] = agg_prob\n",
    "    \n",
    "    prob_sc_selected = select_prob[range(batch_size), select_idx]  # (B,)\n",
    "    prob_sca[:, i_beam, :] = (agg_prob.t() * prob_sc_selected).t()  # (n_agg_ops, B) \\odot (1, B) (broadcast) -> (B, max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9846ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1765, 0.1692, 0.1639, 0.1756, 0.1588, 0.1561],\n",
      "         [0.1777, 0.1687, 0.1647, 0.1742, 0.1588, 0.1558],\n",
      "         [0.1764, 0.1690, 0.1643, 0.1751, 0.1593, 0.1558],\n",
      "         [0.1779, 0.1693, 0.1647, 0.1732, 0.1581, 0.1568]],\n",
      "\n",
      "        [[0.1778, 0.1697, 0.1638, 0.1724, 0.1561, 0.1601],\n",
      "         [0.1778, 0.1712, 0.1638, 0.1742, 0.1554, 0.1577],\n",
      "         [0.1789, 0.1702, 0.1634, 0.1741, 0.1557, 0.1578],\n",
      "         [0.1774, 0.1712, 0.1640, 0.1729, 0.1564, 0.1581]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sc_sa.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95fe0af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n",
      "tensor([[[0.0088, 0.0085, 0.0082, 0.0088, 0.0079, 0.0078],\n",
      "         [0.0089, 0.0084, 0.0082, 0.0087, 0.0079, 0.0078],\n",
      "         [0.0088, 0.0084, 0.0082, 0.0088, 0.0080, 0.0078],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0079, 0.0078]],\n",
      "\n",
      "        [[0.0089, 0.0085, 0.0082, 0.0086, 0.0078, 0.0080],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0086, 0.0078, 0.0079]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sca.size())  # (B, beam_size, prob_sc(beam size selected) * prob_agg)\n",
    "print(prob_sca.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2ba5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_multi_dim(tensor, n_topk):\n",
    "    batch_size = tensor.size(0)\n",
    "    values_1d, idxes_1d = tensor.view(batch_size, -1).topk(n_topk)\n",
    "    idxes = np.stack(np.unravel_index(idxes_1d, tensor.size()[1:])).transpose(1, 2, 0)\n",
    "    values = tensor.view(batch_size, -1).gather(1, idxes_1d).numpy()\n",
    "    return idxes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d77e68e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First flatten to 1-d\n",
    "if np.prod(prob_sca.shape[1:]) < beam_size:\n",
    "    beam_size_sca = np.prod(prob_sca.shape[1:])\n",
    "else:\n",
    "    beam_size_sca = beam_size\n",
    "# Now as sc_idx is already sorted, re-map them properly.\n",
    "# idxes: [sc_beam_idx, sa_idx] -> sca_idxes: [sc_idx, sa_idx]\n",
    "idxes, values = topk_multi_dim(prob_sca.detach().cpu(), n_topk=beam_size_sca)\n",
    "sc_beam_idxes = idxes[:, :, 0]\n",
    "sc_idxes = np.stack([pr_sc_beam.numpy()[i, sc_beam_idx] for i, sc_beam_idx in enumerate(sc_beam_idxes)])\n",
    "sca_idxes = np.stack([sc_idxes, idxes[:, :, 1]]).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c8cf77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[14,  0],\n",
       "        [18,  0],\n",
       "        [19,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 6,  0],\n",
       "        [ 3,  0],\n",
       "        [ 4,  0],\n",
       "        [ 5,  0]]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca_idxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f12bd",
   "metadata": {},
   "source": [
    "writing ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba4d9b-f82e-49e5-a833-a76cdbfeca6f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
