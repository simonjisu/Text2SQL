{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7a52ae",
   "metadata": {},
   "source": [
    "# TEXT2SQL with transformers\n",
    "\n",
    "Lee Woo Chul, Jang Ji Soo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58b81e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c44f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os\n",
    "\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")\n",
    "\n",
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f198f",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "`NLSQL.jsonl` and `\"table.jsonl` contains the data like following format same with [WikiSQL](https://github.com/salesforce/WikiSQL), Please follow the [link](https://github.com/salesforce/WikiSQL#content-and-format) to see what are the keys mean.\n",
    "\n",
    "```json\n",
    "// example of 'NLSQL.jsonl'\n",
    "{\n",
    "    \"phase\": 1, \n",
    "    \"question\": \"2015 삼성전자 유동자산은 어떻게 돼?\", \n",
    "    \"table_id\": \"receipts\", \n",
    "    \"sql\": {\n",
    "        \"sel\": 16, \n",
    "        \"agg\": 0, \n",
    "        \"conds\": [[10, 0, \"유동자산\"], [3, 0, 2016]]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbdcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sql_path, table_path):\n",
    "    path_sql = Path(sql_path)\n",
    "    path_table = Path(table_path)\n",
    "\n",
    "    dataset = []\n",
    "    table = {}\n",
    "    with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            dataset.append(x)\n",
    "\n",
    "    with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            x = json.loads(line.strip())\n",
    "            table[x['id']] = x\n",
    "            \n",
    "    return dataset, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c461129",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, table = load_data(\"NLSQL.jsonl\", \"table.jsonl\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    dataset=data,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=lambda x: x # now dictionary values are not merged!\n",
    ")\n",
    "# Load DBEngine\n",
    "db_path = Path(\"./private\")\n",
    "dbengine = DBEngine(db_path / \"samsung_new.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba748c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec758d7129b421ca960aa94c8843507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test with toy data:   0%|          | 0/21120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, batch_data in enumerate(tqdm(data_loader, desc=\"Test with toy data\")):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec744a",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Used BERT in hugging Face with KoBERT\n",
    "\n",
    "- https://github.com/SKTBrain/KoBERT\n",
    "- https://github.com/monologg/KoBERT-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384008f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert(model_path: str, output_hidden_states: bool=False):\n",
    "    special_tokens = [\"[S]\", \"[E]\", \"[COL]\"] # sequence start, sequence end, column tokens\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    config = BertConfig.from_pretrained(model_path)\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.output_hidden_states = output_hidden_states\n",
    "    \n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25429ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"monologg/kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "model_bert, tokenizer_bert, config_bert = get_bert(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313a2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch_data(data, dbengine):\n",
    "#     batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "#     tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "#     batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "#     batch_ts = []\n",
    "#     for table_id in tid:\n",
    "#         dbengine.get_schema_info(table_id)\n",
    "#         table_str = f\"{table_id}\" + \"\".join([\n",
    "#             f\"[COL]{col}\" for col in dbengine.schema\n",
    "#         ]) \n",
    "#         batch_ts.append(table_str)\n",
    "    \n",
    "#     return batch_qs, batch_sqls, batch_ts\n",
    "\n",
    "def get_batch_data(data: List[Dict[str, Any]], table: Dict[str, Dict[str, List[Any]]], start_tkn=\"[S]\", end_tkn=\"[E]\") -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
    "   \n",
    "    batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "    tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "    batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "    batch_ts = []\n",
    "    for table_id in tid:\n",
    "        table_str = f\"{table_id}\" + \"\".join([\n",
    "            f\"[COL]{col}\" for col in table[table_id][\"header\"]\n",
    "        ])\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # table_str = f\"{start_tkn}{table_id}{end_tkn}\" + \"\".join([\n",
    "        #     f\"{col_tkn}{start_tkn}{col}{end_tkn}\" for col in dbengine.schema\n",
    "        # ]) \n",
    "        batch_ts.append(table_str)\n",
    "\n",
    "    return batch_qs, batch_ts, batch_sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a79d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qs, batch_ts, batch_sqls = get_batch_data(batch_data, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e6fe737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Input\n",
    "encode_input = tokenizer_bert(\n",
    "    batch_qs, batch_ts, \n",
    "    max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "    return_attention_mask=True, \n",
    "    return_special_tokens_mask=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b2ee929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 삼성전자의 2017년의 유동자산은 어떻게 돼?[SEP] receipts [COL] index [COL] rcept_no [COL] reprt_code [COL] bsns_year [COL] corp_code [COL] stock_code [COL] fs_div [COL] fs_nm [COL] sj_div [COL] sj_nm [COL] account_nm [COL] thstrm_nm [COL] thstrm_dt [COL] thstrm_amount [COL] frmtrm_nm [COL] frmtrm_dt [COL] frmtrm_amount [COL] bfefrmtrm_nm [COL] bfefrmtrm_dt [COL] bfefrmtrm_amount[SEP]\n"
     ]
    }
   ],
   "source": [
    "# Show an Example of Input\n",
    "print(tokenizer_bert.decode(encode_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e91d2",
   "metadata": {},
   "source": [
    "## Prepare for decoder Inputs: Createing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3fec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_input_mask(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    r\"\"\"\n",
    "    input should only contains word tokens:\n",
    "    \"\"\"\n",
    "    start_tkn_mask = input_ids == start_tkn_id\n",
    "    end_tkn_mask = input_ids == end_tkn_id\n",
    "    start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "    return mask.scatter(1, index, False)\n",
    "\n",
    "def get_input_mask_and_answer(encode_input, tokenizer):\n",
    "    r\"\"\"\n",
    "    table -> database table name(id)\n",
    "    header -> database header\n",
    "    \n",
    "    returns:\n",
    "        input_question_mask, input_table_mask, input_header_mask, answer_table_tkns, answer_header_tkns\n",
    "    \"\"\"\n",
    "    batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "    sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "    start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "    \n",
    "    input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "    input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "    input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "    db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "    db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "    col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "    db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "    # split table_mask and header_mask\n",
    "    input_idx = torch.arange(max_length).repeat(batch_size, 1)\n",
    "    db_idx = input_idx[db_mask]\n",
    "    table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "    table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "    start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "    table_end_sep_idx = start_idx[:, 0] - 1\n",
    "    split_size = torch.stack([\n",
    "        table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "    ]).transpose(0, 1)\n",
    "\n",
    "    # Token idx\n",
    "    table_tkn_idx, header_tkn_idx = map(\n",
    "        lambda x: torch.stack(x), \n",
    "        zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "    )\n",
    "\n",
    "    table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "    # Mask include [S] & [E] tokens\n",
    "    table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, table_tkn_idx, True)\n",
    "    header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "    # For Decoder Input, Maskout [S], [E] for table & header  \n",
    "    input_table_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    input_header_mask = get_decoder_input_mask(\n",
    "        encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "    )\n",
    "    # [COL] token mask: this is for attention\n",
    "    col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "    input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "    return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6684726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question_mask, input_table_mask, input_header_mask, input_col_mask = get_input_mask_and_answer(encode_input, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a3205f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Tokens for Decoder\n",
      "-------------------------\n",
      "삼성전자의 2017년의 유동자산은 어떻게 돼? 50기 삼성전자 비유동부채는 어떻게 돼?\n",
      "\n",
      "Table Tokens for Decoder\n",
      "-------------------------\n",
      "receipts receipts\n",
      "\n",
      "Header Tokens for Decoder\n",
      "-------------------------\n",
      "index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount index rcept_no reprt_code bsns_year corp_code stock_code fs_div fs_nm sj_div sj_nm account_nm thstrm_nm thstrm_dt thstrm_amount frmtrm_nm frmtrm_dt frmtrm_amount bfefrmtrm_nm bfefrmtrm_dt bfefrmtrm_amount\n",
      "\n",
      "Column(Index of Headers) Tokens for Decoder\n",
      "-------------------------\n",
      "[COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL] [COL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, t in zip(\n",
    "        [input_question_mask, input_table_mask, input_header_mask, input_col_mask], \n",
    "        [\"Question Tokens for Decoder\", \"Table Tokens for Decoder\", \"Header Tokens for Decoder\", \"Column(Index of Headers) Tokens for Decoder\"]\n",
    "    ):\n",
    "    print(t)\n",
    "    print(\"-----\"*5)\n",
    "    print(tokenizer_bert.decode(encode_input[\"input_ids\"][m]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b339bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed to BERT Model\n",
    "encode_outputs = model_bert(**encode_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9d7fa",
   "metadata": {},
   "source": [
    "\n",
    "The `encode_outputs` will be selected by 4 types of masks\n",
    "```\n",
    "encode_outputs\n",
    "-> Question\n",
    "-> Table\n",
    "-> Header\n",
    "-> Column(Index of Headers)\n",
    "```\n",
    "\n",
    "And pad batches which has less tokens than max length with \"\\[PAD\\]\"  for Decoder Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d83a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batches: Tuple[torch.Tensor], lengths: List[int], model: BertModel, pad_idx: int=1) -> torch.Tensor:\n",
    "    padded = []\n",
    "    max_length = max(lengths)\n",
    "    for x in batches:\n",
    "        if len(x) < max_length:\n",
    "            pad_tensor = model.embeddings.word_embeddings(torch.LongTensor([pad_idx]*(max_length - len(x))))\n",
    "            padded.append(torch.cat([x, pad_tensor]))\n",
    "        else:\n",
    "            padded.append(x)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "def get_decoder_batches(encode_output, mask, model, pad_idx):\n",
    "    lengths = mask.sum(1)\n",
    "    tensors = encode_output.last_hidden_state[mask, :]\n",
    "    batches = torch.split(tensors, lengths.tolist())\n",
    "    if lengths.ne(lengths.max()).sum().item() != 0:\n",
    "        # pad not same length tokens\n",
    "        tensors_padded = pad(batches, lengths.tolist(), model, pad_idx=pad_idx)\n",
    "    else:\n",
    "        # just stack the splitted tensors\n",
    "        tensors_padded = torch.stack(batches)\n",
    "    return tensors_padded, lengths.tolist()\n",
    "\n",
    "# def get_pad_mask(lengths):\n",
    "#     batch_size = len(lengths)\n",
    "#     max_len = max(lengths)\n",
    "#     mask = torch.ones(batch_size, max_len)\n",
    "#     for i, l in enumerate(lengths):\n",
    "#         mask[i, :l] = 0\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6a870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded, question_lengths = get_decoder_batches(encode_outputs, input_question_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "table_padded, table_lengths = get_decoder_batches(encode_outputs, input_table_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "header_padded, header_lengths = get_decoder_batches(encode_outputs, input_header_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)\n",
    "col_padded, col_lengths = get_decoder_batches(encode_outputs, input_col_mask, model_bert, pad_idx=tokenizer_bert.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b8cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed5e59",
   "metadata": {},
   "source": [
    "## Create the Answers for decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "458d2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_answers(batch_sqls, tokenizer, end_tkn_idx=1):\n",
    "    \"\"\"\n",
    "    for backward compatibility, separated with get_g\n",
    "    \n",
    "    sc: select column\n",
    "    sa: select agg\n",
    "    wn: where number\n",
    "    wc: where column\n",
    "    wo: where operator\n",
    "    wv: where value\n",
    "    \"\"\"\n",
    "\n",
    "    get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, sql_dict in enumerate(batch_sqls):\n",
    "        g_sc.append( sql_dict[\"sel\"] )\n",
    "        g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "        conds = sql_dict[\"conds\"]\n",
    "        if not sql_dict[\"agg\"] < 0:\n",
    "            g_wn.append( len(conds) )\n",
    "            g_wc.append( get_ith_element(conds, 0) )\n",
    "            g_wo.append( get_ith_element(conds, 1) )\n",
    "            g_wv.append( get_ith_element(conds, 2) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    \n",
    "    # get where value tokenized \n",
    "    end_tkn = tokenizer.additional_special_tokens[end_tkn_idx]\n",
    "    pad_tkn_id = tokenizer.pad_token_id\n",
    "    g_wv_tkns = [[f\"{s}{end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "    g_wv_tkns = [tokenizer(batch_wv, add_special_tokens=False)[\"input_ids\"] for batch_wv in g_wv_tkns]\n",
    "    # add empty list if batch has different where column number\n",
    "    max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "    g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "    temp = []\n",
    "    for batch_wv in list(zip(*g_wv_tkns)):\n",
    "        batch_max_len = max(map(len, batch_wv))\n",
    "        batch_temp = []\n",
    "        for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "            if len(wv_tkns) < batch_max_len:\n",
    "                batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "            else:\n",
    "                batch_temp.append(wv_tkns)\n",
    "        temp.append(batch_temp)\n",
    "    g_wv_tkns = list(zip(*temp))\n",
    "\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f64ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 16],\n",
       " [0, 0],\n",
       " [2, 2],\n",
       " [[10, 3], [10, 3]],\n",
       " [[0, 0], [0, 0]],\n",
       " [['유동자산', 2018], ['비유동부채', 2019]],\n",
       " [([3574, 5872, 7162, 8003, 1, 1], [554, 115, 8003]),\n",
       "  ([2514, 7063, 5872, 6398, 7405, 8003], [554, 116, 8003])])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert, 1)\n",
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478db0f",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Similar structure in SQLova but a little difference in here.\n",
    "\n",
    "- SQLova is a neural semantic parser translating natural language utterance to SQL query.\n",
    "- Official Github: [https://github.com/naver/sqlova](https://github.com/naver/sqlova)\n",
    "- Paper: [A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization](https://arxiv.org/abs/1902.01069)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PW9oAXfW-ZI-jxGn5q9O_gzUIZnNYaet\" alt=\"Sqlova Decoder Architecture \" width=\"50%\" height=\"auto\">\n",
    "\n",
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e428f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AttentionBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def wipe_out_pad_tkn_score(self, score, lengths, dim=2):\n",
    "        max_len = max(lengths)\n",
    "        for batch_idx, length in enumerate(lengths):\n",
    "            if length < max_len:\n",
    "                if dim == 2:\n",
    "                    score[batch_idx, :, length:] = -10000000\n",
    "                elif dim == 1:\n",
    "                    score[batch_idx, length:, :] = 0.0\n",
    "                else:\n",
    "                    raise ValueError(f\"`dim` in wipe_out_pad_tkn_score should be 1 or 2\")\n",
    "        return score \n",
    "\n",
    "\n",
    "class C2QAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Column to Question Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, o_c, o_q, q_lengths, c_lengths=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each column tokens, How much related to question tokens?\n",
    "        \n",
    "        o_c: LSTM output of column\n",
    "        o_q: LSTM output of question \n",
    "        \n",
    "        c_lengths: wipe out row length\n",
    "        return context atttended to question tokens\n",
    "        \"\"\"\n",
    "        sqrt_H = np.sqrt(o_c.size(-1))# torch.sqrt(torch.FloatTensor([o_c.size(-1)]))  # Apply Attention is All you Need Technique\n",
    "        o_q_transform = self.linear(o_q)  # (B, T_q, H)\n",
    "        score_c2q = torch.bmm(o_c, o_q_transform.transpose(1, 2)) / sqrt_H  # (B, T_c, H) x (B, H, T_q) = (B, T_c, T_q)\n",
    "        score_c2q = self.wipe_out_pad_tkn_score(score_c2q, q_lengths, dim=2)\n",
    "        \n",
    "        prob_c2q = self.softmax(score_c2q)\n",
    "        if c_lengths is not None:\n",
    "            prob_c2q = self.wipe_out_pad_tkn_score(prob_c2q, c_lengths, dim=1)\n",
    "        # prob_c2q: (B, T_c, T_q) -> (B, T_c, T_q, 1)\n",
    "        # o_q: (B, 1, T_q, H)\n",
    "        # p_col2question \\odot o_q = (B, T_c, T_q, 1) \\odot (B, 1, T_q, H) = (B, T_c, T_q, H)\n",
    "        # -> reduce sum to T_q to get context for each column (B, T_c, H)\n",
    "        context = torch.mul(prob_c2q.unsqueeze(3), o_q.unsqueeze(1)).sum(dim=2)\n",
    "        if rt_attn:\n",
    "            attn = prob_c2q\n",
    "        else:\n",
    "            attn = None\n",
    "        return context, attn\n",
    "\n",
    "class SelfAttention(AttentionBase):\n",
    "    r\"\"\"Decoder Self Attention Module\"\"\"\n",
    "    def __init__(self, in_features, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, o, lengths, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        Calculate for each o tokens, How much related to o tokens?\n",
    "        \n",
    "        return attended summary of o\n",
    "        \"\"\"\n",
    "        o_transform = self.linear(o)  # (B, T_o, H) -> (B, T_o, 1)\n",
    "        o_transform = self.wipe_out_pad_tkn_score(o_transform, lengths) \n",
    "        o_prob = self.softmax(o_transform)  # (B, T_o, 1)\n",
    "        \n",
    "        o_summary = torch.mul(o, o_prob).sum(1)  # (B, T_o, H) \\odot (B, T_o, 1) -> (B, H)\n",
    "\n",
    "        if rt_attn:\n",
    "            attn = o_prob\n",
    "        else:\n",
    "            attn = None\n",
    "        return o_summary, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf572e3b",
   "metadata": {},
   "source": [
    "## Decoder Sub Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3034f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectDecoder(nn.Module):\n",
    "    r\"\"\"SELECT Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "\n",
    "class AggDecoder(nn.Module):\n",
    "    r\"\"\"AGG Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], select_idxes: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        \n",
    "        col_selected = col_context[list(range(batch_size)), select_idxes].unsqueeze(1)  # col_selected: (B, 1, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_selected, o_q, question_lengths, col_lengths, rt_attn)  # (B, 1, H), (B, 1, T_q)\n",
    "        output = self.output_layer(col_q_context.squeeze(1))\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    \n",
    "class WhereNumDecoder(nn.Module):\n",
    "    r\"\"\"WHERE number Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        if self.output_size > self.max_where_conds+1:\n",
    "            # HERE output will be dilivered to cross-entropy loss, not guessing the real number of where clause\n",
    "            raise ValueError(f\"`WhereNumDecoder` only support maximum {max_where_conds} where clause\")\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_self_attn = SelfAttention(2*hidden_size, 1)\n",
    "        self.lstm_q_hidden_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.lstm_q_cell_init_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "        self.context_self_attn = SelfAttention(hidden_size, 1)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "\n",
    "        col_self_attn, col_attn = self.col_self_attn(col_context, col_lengths, rt_attn)  # (B, 2H), (B, T_c)\n",
    "\n",
    "        h_0 = self.lstm_q_hidden_init_linear(col_self_attn)  # (B, 2H)\n",
    "        h_0 = h_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        c_0 = self.lstm_q_cell_init_linear(col_self_attn)  # (B, 2H)\n",
    "        c_0 = c_0.view(batch_size, 2*self.num_layers, -1).transpose(0, 1).contiguous()  # (B, n_direc*num_layers, H/2) -> (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded, (h_0, c_0))  # o_q: (B, T_q, H)\n",
    "        o_summary, o_attn = self.context_self_attn(o_q, question_lengths, rt_attn)  # (B, H), (B, T_q)\n",
    "        output = self.output_layer(o_summary)\n",
    "        \n",
    "        return output, (col_attn, o_attn)\n",
    "\n",
    "    \n",
    "class WhereColumnDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Column Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int=1, num_layers: int=2, dropout_ratio:float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "\n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], col_lengths: List[int], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict column index\n",
    "        \"\"\"\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_q_context, attn = self.col2question_attn(col_context, o_q, question_lengths, col_lengths, rt_attn)  # (B, T_c, H), (B, T_c, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context], dim=2)  # (B, T_c, 2H)\n",
    "        output = self.output_layer(vec)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        \n",
    "        return output.squeeze(-1), attn\n",
    "    \n",
    "    \n",
    "class WhereOpDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Opperator Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2*hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is settled at WhereColumnDecoder, but it can be lower than or equal to `max_where_conds`\n",
    "        \"\"\"\n",
    "        device = col_padded.device\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes, device)  # (B, max_where_col_nums, H)\n",
    "        \n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        \n",
    "        vec = torch.cat([col_q_context, col_context_padded], dim=2)  # (B, max_where_col_nums, 2H)\n",
    "        output = self.output_layer(vec)  # (B, max_where_col_nums, n_cond_ops)\n",
    "        # TODO: add penalty for padded header(column) information\n",
    "        return output\n",
    "        \n",
    "    def get_context_padded(self, col_context, where_nums, where_col_idxes, device: str=\"cpu\"):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums) \n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size, device=device)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    \n",
    "class WhereValueDecoder(nn.Module):\n",
    "    r\"\"\"WHERE Value Decoder\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int=2, dropout_ratio: float=0.3, max_where_conds: int=4, n_cond_ops: int=4,\n",
    "                 start_tkn_id=8002, end_tkn_id=8003, embedding_layer=None) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.max_where_conds = max_where_conds\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        \n",
    "        self.start_tkn_id = start_tkn_id\n",
    "        self.end_tkn_id = end_tkn_id\n",
    "        \n",
    "        self.lstm_q = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        self.lstm_h = nn.LSTM(input_size, int(hidden_size / 2), num_layers, dropout=dropout_ratio, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.col_context_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.where_op_linear = nn.Linear(n_cond_ops, hidden_size)\n",
    "        self.col2question_attn = C2QAttention(hidden_size, hidden_size)\n",
    "        if embedding_layer is None:\n",
    "            raise KeyError(\"Must initialize the embedding_layer to BertModel's word embedding layer\")\n",
    "        else:\n",
    "            if not isinstance(embedding_layer, torch.nn.modules.sparse.Embedding):\n",
    "                embedding_layer = embedding_layer.word_embeddings\n",
    "            self.embedding_layer = embedding_layer\n",
    "            vocab_size, bert_hidden_size = embedding_layer.weight.data.size()\n",
    "            self.output_lstm_hidden_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm_cell_init_linear = nn.Linear(3*hidden_size, bert_hidden_size)\n",
    "            self.output_lstm = nn.LSTM(bert_hidden_size, bert_hidden_size, 1, batch_first=True)\n",
    "            self.output_linear = nn.Linear(bert_hidden_size, vocab_size)\n",
    "            self.output_linear.weight.data = embedding_layer.weight.data\n",
    "        \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths: List[int], where_nums: List[int], where_col_idxes: List[List[int]], where_op_idxes: List[List[int]], value_tkn_max_len=None, g_wv_tkns=None, rt_attn=False):\n",
    "        r\"\"\"\n",
    "        predict agg index\n",
    "        select_prob: selected argmax indices of select_output score\n",
    "        max_where_col_nums is setted at WhereColumnDecoder\n",
    "        value_tkn_max_len = Train if None else Test\n",
    "        g_wv_tkns = When Train should not be None\n",
    "        \n",
    "        \"\"\"\n",
    "        device = col_padded.device\n",
    "        batch_size, n_col, _ = col_padded.size()\n",
    "        o_q, (h_q, c_q) = self.lstm_q(question_padded)  # o_q: (B, T_q, H)\n",
    "        o_c, (h_c, c_c) = self.lstm_h(col_padded)  # o_c: (B, T_c, H)\n",
    "        o_h, (h_h, c_h) = self.lstm_h(header_padded)  # h_h: (n_direc*num_layers, B, H/2)\n",
    "        \n",
    "        header_summary = torch.cat([h for h in h_h[-2:]], dim=1).unsqueeze(1).repeat(1, n_col, 1)  # (B, T_c, H)\n",
    "        col_context = torch.cat([o_c, header_summary], dim=2)  # (B, T_c, 2H)\n",
    "        col_context = self.col_context_linear(col_context)  # (B, T_c, H)\n",
    "        col_context_padded = self.get_context_padded(col_context, where_nums, where_col_idxes, device)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        col_q_context, attn = self.col2question_attn(col_context_padded, o_q, question_lengths, where_nums, rt_attn)  # (B, max_where_col_nums, H), (B, max_where_col_nums, T_q)\n",
    "        where_op_one_hot_padded = self.get_where_op_one_hot_padded(\n",
    "            where_op_idxes, where_nums, where_col_idxes, n_cond_ops=self.n_cond_ops, device=device)  # (B, max_where_col_nums, n_cond_ops)\n",
    "\n",
    "        where_op = self.where_op_linear(where_op_one_hot_padded)  # (B, max_where_col_nums, H)\n",
    "\n",
    "        vec = torch.cat([col_q_context, col_context_padded, where_op], dim=2)  # (B, max_where_col_nums, 3H)\n",
    "        max_where_col_nums = vec.size(1)\n",
    "        # predict each where_col\n",
    "        total_scores = []\n",
    "        for i in range(max_where_col_nums):\n",
    "            g_wv_tkns_i = torch.LongTensor([g_wv_tkns[b_idx][i] for b_idx in range(batch_size)]).to(device) if g_wv_tkns is not None else None  # (B, T_d_i)\n",
    "            vec_i = vec[:, i, :]  # (B, 3H)\n",
    "            \n",
    "            h_0 = self.output_lstm_hidden_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            c_0 = self.output_lstm_cell_init_linear(vec_i).unsqueeze(1).transpose(0, 1).contiguous()  # (B, 3H) -> (B, bert_H) -> (1, B, bert_H)\n",
    "            \n",
    "            scores = self.decode_single_where_col(batch_size, h_0, c_0, value_tkn_max_len=value_tkn_max_len, g_wv_tkns_i=g_wv_tkns_i, device=device)  # (B, T_d_i, vocab_size)\n",
    "            total_scores.append(scores)\n",
    "        \n",
    "        # total_scores: [(B, T_d_i, vocab_size)] x max_where_col_nums\n",
    "        return total_scores\n",
    "    \n",
    "    def start_token(self, batch_size, device):\n",
    "        sos = torch.LongTensor([self.start_tkn_id]*batch_size).unsqueeze(1).to(device)  # (B, 1)\n",
    "        return sos\n",
    "    \n",
    "    def decode_single_where_col(self, batch_size, h_0, c_0, value_tkn_max_len=None, g_wv_tkns_i=None, device: str=\"cpu\"):\n",
    "        if value_tkn_max_len is None:\n",
    "            # [Training] set the max length to gold token max length (already padded)\n",
    "            max_len = len(g_wv_tkns_i[0])\n",
    "        else:\n",
    "            # [Testing]  don't know the max length\n",
    "            max_len = value_tkn_max_len\n",
    "            \n",
    "        # Version2: left_batch_size = batch_size\n",
    "        \n",
    "        sos = self.start_token(batch_size, device)  # (B, 1)\n",
    "        emb = self.embedding_layer(sos)  # (B, 1, bert_H)\n",
    "        scores = [] \n",
    "        for i in range(max_len):\n",
    "            o, (h, c) = self.output_lstm(emb, (h_0, c_0))  # h: (1, B, bert_H)  \n",
    "            s = self.output_linear(h[-1, :]) # select last layer if use multiple rnn layers, h: (1, B, bert_H) -> (B, bert_H) -> s: (B, vocab_size)\n",
    "            scores.append(s)\n",
    "            if g_wv_tkns_i is not None:\n",
    "                # [Training] Teacher Force model\n",
    "                pred = g_wv_tkns_i[:, i]  # (B, )\n",
    "            else:\n",
    "                # [Testing]\n",
    "                pred = s.argmax(1)  # (B, )\n",
    "                if (pred == self.end_tkn_id).sum() == batch_size:  # all stop\n",
    "                    break\n",
    "                # Version2: Seperate all tokens\n",
    "                # if (pred == dd.end_tkn_id).sum() == left_batch_size:  # all stop\n",
    "                #     scores.append(s)\n",
    "                #     break\n",
    "                # else:\n",
    "                #     stop_mask = pred == dd.end_tkn_id\n",
    "                #     pred = pred[~stop_mask]\n",
    "                #     scores.append(pred)\n",
    "                #     left_batch_size -= stop_mask.sum().item()\n",
    "                    \n",
    "            emb = self.embedding_layer(pred.unsqueeze(1))  # (B, 1, bert_H)\n",
    "            \n",
    "        return torch.stack(scores).transpose(0, 1).contiguous() # (T_d_i, B, vocab_size) -> (B, T_d_i, vocab_size)\n",
    "        \n",
    "    def get_context_padded(self, col_context: torch.Tensor, where_nums: List[int], where_col_idxes: List[List[int]], device: str=\"cpu\"):\n",
    "        r\"\"\"\n",
    "        Select the where column index and pad if some batch doesn't match the max length of tensor\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        batch_size, n_col, hidden_size = col_context.size()\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [col_context[i, batch_col] for i, batch_col in enumerate(where_col_idxes)]  # [(where_col_nums, hidden_size), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), hidden_size, device=device)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "            \n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)\n",
    "    \n",
    "    def get_where_op_one_hot_padded(self, where_op_idxes: List[List[int]], where_nums: List[int], where_col_idxes: List[List[int]], n_cond_ops: int, device: str=\"cpu\"):\n",
    "        r\"\"\"\n",
    "        Turn where operation indexs into one hot encoded vectors\n",
    "        In case for have different where column lengths\n",
    "        \"\"\"\n",
    "        max_where_col_nums = max(where_nums)\n",
    "        batches = [\n",
    "            torch.zeros(where_num, n_cond_ops).scatter(1, torch.LongTensor(batch_col).unsqueeze(1), 1).to(device) \n",
    "            for where_num, batch_col in zip(where_nums, where_op_idxes)\n",
    "        ]  \n",
    "        # batches = [(where_col_nums, n_cond_ops), ...]  len = B\n",
    "        batches_padded = []\n",
    "        for b in batches:\n",
    "            where_col_nums = b.size(0)\n",
    "            if where_col_nums < max_where_col_nums:\n",
    "                b_padded = torch.cat([b, torch.zeros((max_where_col_nums-where_col_nums), n_cond_ops, device=device)], dim=0)\n",
    "            else:\n",
    "                b_padded = b\n",
    "            batches_padded.append(b_padded)  # (max_where_col_nums, hidden_size)\n",
    "        return torch.stack(batches_padded) # (B, max_where_col_nums, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f5595",
   "metadata": {},
   "source": [
    "## Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8804f",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11c99714",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_bert.hidden_size\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "dropout_ratio = 0.3\n",
    "max_where_conds = 4\n",
    "n_agg_ops = len(dbengine.agg_ops)\n",
    "n_cond_ops = len(dbengine.cond_ops)\n",
    "start_tkn_id = tokenizer_bert.additional_special_tokens_ids[0]\n",
    "end_tkn_id = tokenizer_bert.additional_special_tokens_ids[1]\n",
    "embedding_layer = model_bert.embeddings.word_embeddings\n",
    "train = True\n",
    "if train:\n",
    "    value_tkn_max_len = None\n",
    "else:\n",
    "    value_tkn_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e21db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.max_where_conds = max_where_conds\n",
    "        \n",
    "        self.select_decoder = SelectDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.agg_decoder = AggDecoder(\n",
    "            input_size, hidden_size, output_size=n_agg_ops, num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_num_decoder = WhereNumDecoder(\n",
    "            input_size, hidden_size, output_size=(max_where_conds+1), num_layers=num_layers, dropout_ratio=dropout_ratio\n",
    "        )\n",
    "        self.where_col_decoder = WhereColumnDecoder(\n",
    "            input_size, hidden_size, output_size=1, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_op_decoder = WhereOpDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds\n",
    "        )\n",
    "        self.where_value_decoder = WhereValueDecoder(\n",
    "            input_size, hidden_size, output_size=n_cond_ops, num_layers=num_layers, dropout_ratio=dropout_ratio, max_where_conds=max_where_conds, \n",
    "            n_cond_ops=n_cond_ops, start_tkn_id=start_tkn_id, end_tkn_id=end_tkn_id, embedding_layer=embedding_layer\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=None, gold=None):\n",
    "        \"\"\"\n",
    "        # Outputs Size\n",
    "        # sc = (B, T_c)\n",
    "        # sa = (B, n_agg_ops)\n",
    "        # wn = (B, max_where_conds+1)\n",
    "        # wc = (B, T_c): binary\n",
    "        # wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        # wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        if gold is None:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = [None] * 6\n",
    "        else:\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "        decoder_outputs = {}\n",
    "\n",
    "        select_outputs, _ = self.select_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        select_idxes = g_sc if g_sc else self.predict_decoder(\"sc\", select_outputs=select_outputs)\n",
    "\n",
    "        agg_outputs, _ = self.agg_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, select_idxes)\n",
    "\n",
    "        where_num_outputs, _  = self.where_num_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_nums = g_wn if g_wn else self.predict_decoder(\"wn\", where_num_outputs=where_num_outputs)\n",
    "\n",
    "        where_col_outputs, _ = self.where_col_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths)\n",
    "        where_col_idxes = g_wc if g_wc else self.predict_decoder(\"wc\", where_col_outputs=where_col_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_op_outputs = self.where_op_decoder(question_padded, header_padded, col_padded, question_lengths, where_nums, where_col_idxes)\n",
    "        where_op_idxes = g_wo if g_wo else self.predict_decoder(\"wo\", where_op_outputs=where_op_outputs, where_nums=where_nums)\n",
    "\n",
    "        where_value_outputs = self.where_value_decoder(question_padded, header_padded, col_padded, question_lengths, where_nums, where_col_idxes, where_op_idxes, value_tkn_max_len, g_wv_tkns)\n",
    "        \n",
    "        decoder_outputs = {\n",
    "            \"sc\": select_outputs,  # cross entropy\n",
    "            \"sa\": agg_outputs,  # cross entropy\n",
    "            \"wn\": where_num_outputs,  # cross entropy\n",
    "            \"wc\": where_col_outputs,  # binary cross entropy\n",
    "            \"wo\": where_op_outputs,  # cross entropy\n",
    "            \"wv\": where_value_outputs  # cross entropy\n",
    "        }\n",
    "        \n",
    "        return decoder_outputs\n",
    "        \n",
    "    def predict_decoder(self, typ, **kwargs):\n",
    "        r\"\"\"\n",
    "        if not using teacher force model will use this function to predict answer\n",
    "        # Outputs Size\n",
    "        # sc = (B, T_c)\n",
    "        # sa = (B, n_agg_ops)\n",
    "        # wn = (B, max_where_conds+1)\n",
    "        # wc = (B, T_c): binary\n",
    "        # wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        # wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        if typ == \"sc\":  # SELECT column\n",
    "            select_outputs = kwargs[\"select_outputs\"]\n",
    "            return select_outputs.argmax(1).tolist()\n",
    "        elif typ == \"sa\":  # SELECT aggregation operator\n",
    "            agg_outputs = kwargs[\"agg_outputs\"]\n",
    "            return agg_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wn\":  # WHERE number\n",
    "            where_num_outputs = kwargs[\"where_num_outputs\"]\n",
    "            return where_num_outputs.argmax(1).tolist()\n",
    "        elif typ == \"wc\":  # WHERE clause column\n",
    "            where_col_outputs = kwargs[\"where_col_outputs\"]\n",
    "            where_col_argsort = torch.sigmoid(where_col_outputs).argsort(1)\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_col_idxes = [where_col_argsort[b_idx, :w_num].tolist() for b_idx, w_num in enumerate(where_nums)]\n",
    "            return where_col_idxes\n",
    "        elif typ == \"wo\":  # WHERE clause operator\n",
    "            where_op_outputs = kwargs[\"where_op_outputs\"]\n",
    "            where_nums = kwargs[\"where_nums\"]\n",
    "            where_op_idxes = []\n",
    "            for b_idx, w_num in enumerate(where_nums):\n",
    "                if w_num == 0:  # means no where number\n",
    "                    where_op_idxes.append([])\n",
    "                else:\n",
    "                    where_op_idxes.append(where_op_outputs.argmax(2)[b_idx, :w_num].tolist())\n",
    "            return where_op_idxes\n",
    "        elif typ == \"wv\":  # WHERE clause value\n",
    "            where_value_outputs = kwargs[\"where_value_outputs\"]\n",
    "            return [o.argmax(2).tolist() for o in where_value_outputs]  # iter with each where clause\n",
    "        else:\n",
    "            raise KeyError(\"`typ` must be in ['sc', 'sa', 'wn', 'wc', 'wo', 'wv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "911dfeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts = {}\n",
    "# predicts[\"sc\"] = model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "# predicts[\"sa\"] = model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "# predicts[\"wn\"] = model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "# predicts[\"wc\"] = model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "# predicts[\"wo\"] = model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "# predicts[\"wv\"] = model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e0ee6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decoder = Decoder(input_size, hidden_size, num_layers, dropout_ratio, max_where_conds, n_agg_ops, n_cond_ops, start_tkn_id, end_tkn_id, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b88a2",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5fd0ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[-0.0512, -0.0510, -0.0482, -0.0414, -0.0402, -0.0447, -0.0397, -0.0437,\n",
       "          -0.0454, -0.0486, -0.0494, -0.0447, -0.0496, -0.0523, -0.0460, -0.0452,\n",
       "          -0.0435, -0.0446, -0.0462, -0.0415],\n",
       "         [-0.0356, -0.0358, -0.0307, -0.0365, -0.0369, -0.0346, -0.0382, -0.0346,\n",
       "          -0.0403, -0.0408, -0.0399, -0.0368, -0.0418, -0.0408, -0.0395, -0.0387,\n",
       "          -0.0391, -0.0409, -0.0380, -0.0352]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[-0.0188,  0.1266,  0.0655, -0.0132, -0.0502,  0.0563],\n",
       "         [-0.0298,  0.1227,  0.0649, -0.0285, -0.0581,  0.0507]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[-0.0960, -0.1163, -0.0221,  0.0192, -0.0908],\n",
       "         [-0.1013, -0.1192, -0.0132,  0.0161, -0.0888]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[0.0350, 0.0374, 0.0367, 0.0411, 0.0455, 0.0437, 0.0417, 0.0375, 0.0384,\n",
       "          0.0391, 0.0335, 0.0313, 0.0306, 0.0383, 0.0422, 0.0378, 0.0342, 0.0409,\n",
       "          0.0386, 0.0340],\n",
       "         [0.0405, 0.0388, 0.0406, 0.0399, 0.0335, 0.0339, 0.0285, 0.0283, 0.0309,\n",
       "          0.0297, 0.0346, 0.0367, 0.0399, 0.0466, 0.0443, 0.0389, 0.0397, 0.0358,\n",
       "          0.0373, 0.0359]], grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[-0.0202,  0.0559, -0.0811, -0.0427],\n",
       "          [-0.0188,  0.0540, -0.0788, -0.0418]],\n",
       " \n",
       "         [[-0.0333,  0.0514, -0.0870, -0.0416],\n",
       "          [-0.0247,  0.0419, -0.0789, -0.0384]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[-2.0390e-02,  8.9074e-02,  1.2735e-02,  ..., -1.2009e-02,\n",
       "             1.5616e-02,  6.3630e-02],\n",
       "           [-1.7755e-02,  9.4309e-02,  1.0585e-02,  ..., -7.9875e-03,\n",
       "             1.1124e-02,  6.1733e-02],\n",
       "           [-2.8637e-02,  1.1760e-01,  1.3108e-02,  ..., -9.3284e-03,\n",
       "             2.0039e-02,  6.6331e-02],\n",
       "           [-4.5703e-02,  9.0344e-02,  7.3450e-03,  ..., -1.7495e-03,\n",
       "             1.7380e-02,  7.3881e-02],\n",
       "           [-2.9933e-02,  1.0067e-01,  1.2342e-02,  ..., -9.7471e-03,\n",
       "             1.3514e-02,  6.4732e-02],\n",
       "           [-3.1742e-02,  8.4713e-02,  1.0558e-02,  ..., -1.1470e-02,\n",
       "             2.1957e-02,  5.3663e-02]],\n",
       "  \n",
       "          [[-2.0189e-02,  8.8495e-02,  1.5937e-02,  ..., -1.2467e-02,\n",
       "             1.6013e-02,  6.3457e-02],\n",
       "           [-1.7741e-02,  5.6674e-02,  1.2618e-02,  ..., -2.0548e-02,\n",
       "             1.2977e-02,  5.8991e-02],\n",
       "           [-2.4200e-02,  1.0584e-01,  1.3338e-02,  ..., -9.2853e-03,\n",
       "             8.8180e-03,  6.1853e-02],\n",
       "           [-2.8307e-02,  1.1718e-01,  1.6315e-02,  ..., -9.8632e-03,\n",
       "             2.0495e-02,  6.6221e-02],\n",
       "           [-2.5099e-02,  8.6722e-02,  6.2094e-03,  ..., -1.5358e-02,\n",
       "             1.7551e-02,  6.5207e-02],\n",
       "           [ 8.5663e-05,  9.5736e-02,  9.2046e-03,  ..., -1.3076e-02,\n",
       "             1.3459e-02,  7.0156e-02]]], grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-0.0260,  0.0835,  0.0132,  ..., -0.0147,  0.0132,  0.0635],\n",
       "           [-0.0275,  0.0880,  0.0106,  ..., -0.0162,  0.0163,  0.0655],\n",
       "           [-0.0352,  0.0902,  0.0146,  ..., -0.0097,  0.0087,  0.0656]],\n",
       "  \n",
       "          [[-0.0175,  0.0816,  0.0152,  ..., -0.0150,  0.0142,  0.0635],\n",
       "           [-0.0192,  0.0864,  0.0127,  ..., -0.0164,  0.0173,  0.0656],\n",
       "           [-0.0242,  0.0933,  0.0171,  ..., -0.0112,  0.0087,  0.0690]]],\n",
       "         grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = get_sql_answers(batch_sqls, tokenizer_bert)\n",
    "gold = g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns\n",
    "decoder_outputs = model_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=None, gold=gold)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcbe29",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e5f157a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sc': tensor([[-0.0386, -0.0402, -0.0374, -0.0357, -0.0362, -0.0359, -0.0380, -0.0341,\n",
       "          -0.0347, -0.0388, -0.0379, -0.0317, -0.0339, -0.0398, -0.0451, -0.0404,\n",
       "          -0.0395, -0.0334, -0.0362, -0.0391],\n",
       "         [-0.0357, -0.0366, -0.0337, -0.0350, -0.0371, -0.0358, -0.0382, -0.0352,\n",
       "          -0.0339, -0.0367, -0.0299, -0.0351, -0.0377, -0.0393, -0.0331, -0.0350,\n",
       "          -0.0384, -0.0396, -0.0394, -0.0433]], grad_fn=<SqueezeBackward1>),\n",
       " 'sa': tensor([[-0.0166,  0.1289,  0.0615, -0.0191, -0.0575,  0.0516],\n",
       "         [-0.0289,  0.1171,  0.0701, -0.0318, -0.0553,  0.0462]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wn': tensor([[-0.0913, -0.1125, -0.0182,  0.0102, -0.0842],\n",
       "         [-0.1004, -0.1137, -0.0206,  0.0189, -0.0867]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'wc': tensor([[0.0510, 0.0406, 0.0425, 0.0470, 0.0503, 0.0506, 0.0495, 0.0474, 0.0475,\n",
       "          0.0401, 0.0496, 0.0439, 0.0417, 0.0420, 0.0513, 0.0436, 0.0520, 0.0519,\n",
       "          0.0538, 0.0535],\n",
       "         [0.0381, 0.0357, 0.0371, 0.0318, 0.0298, 0.0344, 0.0340, 0.0341, 0.0325,\n",
       "          0.0277, 0.0275, 0.0275, 0.0307, 0.0345, 0.0352, 0.0421, 0.0450, 0.0366,\n",
       "          0.0368, 0.0331]], grad_fn=<SqueezeBackward1>),\n",
       " 'wo': tensor([[[-0.0247,  0.0497, -0.0833, -0.0458],\n",
       "          [-0.0250,  0.0487, -0.0796, -0.0455],\n",
       "          [-0.0225,  0.0490, -0.0785, -0.0476]],\n",
       " \n",
       "         [[-0.0255,  0.0483, -0.0808, -0.0447],\n",
       "          [-0.0239,  0.0492, -0.0821, -0.0373],\n",
       "          [-0.0202,  0.0453, -0.0766, -0.0465]]], grad_fn=<AddBackward0>),\n",
       " 'wv': [tensor([[[-0.0195,  0.1309,  0.0230,  ..., -0.0379,  0.0222,  0.0519],\n",
       "           [-0.0342,  0.1399,  0.0186,  ..., -0.0416,  0.0217,  0.0549],\n",
       "           [-0.0342,  0.1399,  0.0186,  ..., -0.0416,  0.0217,  0.0549],\n",
       "           ...,\n",
       "           [-0.0342,  0.1399,  0.0186,  ..., -0.0416,  0.0217,  0.0549],\n",
       "           [-0.0342,  0.1399,  0.0186,  ..., -0.0416,  0.0217,  0.0549],\n",
       "           [-0.0342,  0.1399,  0.0186,  ..., -0.0416,  0.0217,  0.0549]],\n",
       "  \n",
       "          [[-0.0036,  0.1318,  0.0240,  ..., -0.0386,  0.0251,  0.0527],\n",
       "           [-0.0183,  0.1411,  0.0196,  ..., -0.0425,  0.0247,  0.0559],\n",
       "           [-0.0183,  0.1411,  0.0196,  ..., -0.0425,  0.0247,  0.0559],\n",
       "           ...,\n",
       "           [-0.0183,  0.1411,  0.0196,  ..., -0.0425,  0.0247,  0.0559],\n",
       "           [-0.0183,  0.1411,  0.0196,  ..., -0.0425,  0.0247,  0.0559],\n",
       "           [-0.0183,  0.1411,  0.0196,  ..., -0.0425,  0.0247,  0.0559]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-0.0111,  0.1311,  0.0222,  ..., -0.0396,  0.0249,  0.0551],\n",
       "           [-0.0257,  0.1402,  0.0179,  ..., -0.0434,  0.0244,  0.0582],\n",
       "           [-0.0257,  0.1402,  0.0179,  ..., -0.0434,  0.0244,  0.0582],\n",
       "           ...,\n",
       "           [-0.0257,  0.1402,  0.0179,  ..., -0.0434,  0.0244,  0.0582],\n",
       "           [-0.0257,  0.1402,  0.0179,  ..., -0.0434,  0.0244,  0.0582],\n",
       "           [-0.0257,  0.1402,  0.0179,  ..., -0.0434,  0.0244,  0.0582]],\n",
       "  \n",
       "          [[ 0.0019,  0.1342,  0.0234,  ..., -0.0381,  0.0272,  0.0550],\n",
       "           [-0.0126,  0.1434,  0.0190,  ..., -0.0421,  0.0267,  0.0581],\n",
       "           [-0.0126,  0.1434,  0.0190,  ..., -0.0421,  0.0267,  0.0581],\n",
       "           ...,\n",
       "           [-0.0126,  0.1434,  0.0190,  ..., -0.0421,  0.0267,  0.0581],\n",
       "           [-0.0126,  0.1434,  0.0190,  ..., -0.0421,  0.0267,  0.0581],\n",
       "           [-0.0126,  0.1434,  0.0190,  ..., -0.0421,  0.0267,  0.0581]]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  tensor([[[-0.0124,  0.1317,  0.0232,  ..., -0.0394,  0.0255,  0.0542],\n",
       "           [-0.0267,  0.1406,  0.0187,  ..., -0.0432,  0.0250,  0.0573],\n",
       "           [-0.0267,  0.1406,  0.0187,  ..., -0.0432,  0.0250,  0.0573],\n",
       "           ...,\n",
       "           [-0.0267,  0.1406,  0.0187,  ..., -0.0432,  0.0250,  0.0573],\n",
       "           [-0.0267,  0.1406,  0.0187,  ..., -0.0432,  0.0250,  0.0573],\n",
       "           [-0.0267,  0.1406,  0.0187,  ..., -0.0432,  0.0250,  0.0573]],\n",
       "  \n",
       "          [[-0.0067,  0.1355,  0.0242,  ..., -0.0389,  0.0240,  0.0527],\n",
       "           [-0.0215,  0.1448,  0.0199,  ..., -0.0427,  0.0236,  0.0558],\n",
       "           [-0.0215,  0.1448,  0.0199,  ..., -0.0427,  0.0236,  0.0558],\n",
       "           ...,\n",
       "           [-0.0215,  0.1448,  0.0199,  ..., -0.0427,  0.0236,  0.0558],\n",
       "           [-0.0215,  0.1448,  0.0199,  ..., -0.0427,  0.0236,  0.0558],\n",
       "           [-0.0215,  0.1448,  0.0199,  ..., -0.0427,  0.0236,  0.0558]]],\n",
       "         grad_fn=<CopyBackwards>)]}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = model_decoder(question_padded, header_padded, col_padded, question_lengths, col_lengths, value_tkn_max_len=20, gold=None)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5474b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228ad1b",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82723614",
   "metadata": {},
   "source": [
    "TRAIN for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "47bd9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = \\\n",
    "    ([13, 16],\n",
    "     [0, 0],\n",
    "     [3, 1],\n",
    "     [[10, 3, 3], [10]],\n",
    "     [[0, 0, 0], [0]],\n",
    "     [['유동부채', 2018, 2018], ['유동자산']],\n",
    "     [([3574, 5872, 6398, 7405, 8003], [554, 115, 8003], [554, 115, 8003]),\n",
    "      ([3574, 5872, 7162, 8003, 1], [1, 1, 1], [1, 1, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7881af6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(124.2673, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "binary_cross_entropy = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "vocab_size = len(tokenizer_bert)\n",
    "wn_penalty = 2.0  # scale up for guessing where number\n",
    "wv_penalty = 5.0\n",
    "# Outputs Size\n",
    "# sc = (B, T_c)\n",
    "# sa = (B, n_agg_ops)\n",
    "# wn = (B, 5)\n",
    "# wc = (B, T_c): binary\n",
    "# wo = (B, max_where_col_nums, n_cond_ops)\n",
    "# wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "batch_size = decoder_outputs[\"sc\"].size(0)\n",
    "loss_sc = cross_entropy(decoder_outputs[\"sc\"], torch.LongTensor(g_sc))\n",
    "loss_sa = cross_entropy(decoder_outputs[\"sa\"], torch.LongTensor(g_sa))\n",
    "loss_wn = cross_entropy(decoder_outputs[\"wn\"], torch.LongTensor(g_wn)) * wn_penalty\n",
    "\n",
    "# need consider: might have different length of where numers\n",
    "# So when calculate scores looping by where numbers, ignore the out of length tokens\n",
    "loss_wc = 0\n",
    "loss_wo = 0\n",
    "loss_wv = 0\n",
    "for batch_idx, where_num in enumerate(g_wn):\n",
    "    one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"][batch_idx]).scatter(0, torch.LongTensor(g_wc[batch_idx]), 1.0)\n",
    "    loss_wc += binary_cross_entropy(decoder_outputs[\"wc\"][batch_idx], one_hot_dist)\n",
    "    \n",
    "    batch_g_wo = g_wo[batch_idx]  # (where_num,)\n",
    "    batch_wo = decoder_outputs[\"wo\"][batch_idx, :where_num, :]  # (where_num, n_cond_ops)\n",
    "    loss_wo += cross_entropy(batch_wo, torch.LongTensor(batch_g_wo))\n",
    "    \n",
    "    batch_g_wv = g_wv_tkns[batch_idx][:where_num]  # (where_num, T_d_i)\n",
    "    batch_wv = torch.stack([wv[batch_idx] for wv in decoder_outputs[\"wv\"]])  # (where_num, value_tkn_max_len, vocab_size)\n",
    "    for wv, g_wv_i in zip(batch_wv_temp, batch_g_wv_temp):  # will by where_num\n",
    "        if wv.size(0) > len(g_wv_i):\n",
    "            wv_penalty = 1.0\n",
    "            wv = wv[:len(g_wv_i), :]  # (T_d_gold, vocab_size)\n",
    "        elif wv.size(0) < len(g_wv_i):\n",
    "            # giving penalty if not generate enought tokens\n",
    "            wv_penalty = 5.0\n",
    "            g_wv_i = g_wv_i[:len(wv)]  # (T_d_predict,)\n",
    "        else:\n",
    "            wv_penalty = 1.0\n",
    "        # now have the same T_d size, ignore all over lengthed\n",
    "        loss_wv += cross_entropy(wv, torch.LongTensor(g_wv_i)) * wv_penalty\n",
    "    \n",
    "total_loss = (loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv) / batch_size\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06332fb",
   "metadata": {},
   "source": [
    "# Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c7e57a-5a72-45e4-83f2-5fa852ebead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version: 1.8.1\n",
      "Transfomers Version: 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "from typing import Tuple, Dict, List, Union, Any\n",
    "import os \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dbengine import DBEngine\n",
    "# multiprocessing lib doesn’t have it implemented on Windows\n",
    "# https://discuss.pytorch.org/t/cant-pickle-local-object-dataloader-init-locals-lambda/31857/14\n",
    "num_workers = 0 if os.name == \"nt\" else 4\n",
    "\n",
    "print(f\"PyTroch Version: {torch.__version__}\")\n",
    "print(f\"Transfomers Version: {transformers.__version__}\")\n",
    "\n",
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertModel, BertConfig\n",
    "import torchmetrics\n",
    "\n",
    "class Perplexity(torchmetrics.Metric):\n",
    "    def __init__(self, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "        self.add_state(\"pp\", default=torch.FloatTensor([0]), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, pp: torch.Tensor):\n",
    "        self.pp += pp\n",
    "\n",
    "    def compute(self):\n",
    "        return self.pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5a6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQL(pl.LightningModule):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.dbengine = DBEngine(Path(self.hparams.db_path))\n",
    "        self.n_agg_ops = len(self.dbengine.agg_ops)\n",
    "        self.n_cond_ops = len(self.dbengine.cond_ops)\n",
    "        # Encoder\n",
    "        self.model_bert, self.tokenizer_bert, self.config_bert = self.get_bert(model_path=self.hparams.model_bert_path)\n",
    "        # Decoder\n",
    "        self.model_decoder = Decoder(\n",
    "            input_size=self.config_bert.hidden_size, \n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            dropout_ratio=self.hparams.dropout_ratio,\n",
    "            max_where_conds=self.hparams.max_where_conds,\n",
    "            n_agg_ops=self.n_agg_ops,\n",
    "            n_cond_ops=self.n_cond_ops,\n",
    "            start_tkn_id = self.tokenizer_bert.additional_special_tokens_ids[0],\n",
    "            end_tkn_id = self.tokenizer_bert.additional_special_tokens_ids[1],\n",
    "            embedding_layer = self.model_bert.embeddings.word_embeddings\n",
    "        )\n",
    "        \n",
    "        # Loss function & Metrics\n",
    "        self.vocab_size = len(self.tokenizer_bert)\n",
    "        self.totensor = lambda x: torch.LongTensor(x).to(self.device)\n",
    "        self.create_metrics()\n",
    "\n",
    "    def get_bert(self, model_path: str, output_hidden_states: bool=False):\n",
    "        self.special_tokens = [self.hparams.special_start_tkn, self.hparams.special_end_tkn, self.hparams.special_col_tkn] # sequence start, sequence end, column tokens\n",
    "        tokenizer = KoBertTokenizer.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=self.special_tokens)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        model = BertModel.from_pretrained(model_path)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.output_hidden_states = output_hidden_states\n",
    "\n",
    "        return model, tokenizer, config\n",
    "    \n",
    "    def create_metrics(self):\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.binary_cross_entropy = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "        \n",
    "        self.acc_sc = torchmetrics.Accuracy()\n",
    "        self.acc_sa = torchmetrics.Accuracy(num_classes=self.n_agg_ops)\n",
    "        self.acc_wn = torchmetrics.Accuracy(num_classes=self.hparams.max_where_conds+1)\n",
    "        self.acc_wo = torchmetrics.Accuracy(num_classes=self.n_cond_ops+1) # add one to calculate if where number is missing\n",
    "        self.pp_wv = Perplexity()\n",
    "\n",
    "    def reset_metrics_epoch_end(self):\n",
    "        self.acc_sc.reset()\n",
    "        self.acc_sa.reset()\n",
    "        self.acc_wn.reset()\n",
    "        self.acc_wo.reset()\n",
    "        self.pp_wv.reset()\n",
    "        \n",
    "    def forward(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls, value_tkn_max_len, train)\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "        gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "        loss = self.calculate_loss(outputs, gold)  # when calculate loss must need gold answer\n",
    "        return loss, outputs\n",
    "            \n",
    "    def forward_outputs(self, batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=None, train=True):\n",
    "        # --- Get Answer & Variables ---\n",
    "        if train:\n",
    "            assert value_tkn_max_len is None, \"In train phase, `value_tkn_max_len` must be None\"\n",
    "            assert batch_sqls is not None, \"In train phase, `batch_sqls` must not be None\"\n",
    "            g_sc, g_sa, g_wn, g_wc, g_wo, _, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "            gold = [g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns]\n",
    "        else:\n",
    "            assert value_tkn_max_len is not None, \"In validation Phase, `value_tkn_max_len` must not be None\"\n",
    "            gold = None\n",
    "            value_tkn_max_len = value_tkn_max_len\n",
    "            \n",
    "        # --- Get Inputs for Encoder --- \n",
    "        encode_inputs = self.tokenizer_bert(\n",
    "            batch_qs, batch_ts, \n",
    "            max_length=512, padding=True, truncation=True, return_tensors=\"pt\", \n",
    "            return_attention_mask=True, \n",
    "            return_special_tokens_mask=False, \n",
    "        ).to(self.device)  # encode_input doesn't return the cuda device\n",
    "        \n",
    "        # --- Forward Encoder ---\n",
    "        encode_outputs = self.model_bert(**encode_inputs)\n",
    "        \n",
    "        # --- Get Inputs for Decoder ---\n",
    "        input_question_mask, input_table_mask, input_header_mask, input_col_mask = self.get_input_mask_and_answer(encode_inputs, self.tokenizer_bert)\n",
    "        question_padded, question_lengths = self.get_decoder_batches(encode_outputs, input_question_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        # table_padded, table_lengths = self.get_decoder_batches(encode_outputs, input_table_mask, pad_idx=self.tokenizer_bert.pad_token_id)  # Not used yet\n",
    "        header_padded, header_lengths = self.get_decoder_batches(encode_outputs, input_header_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        col_padded, col_lengths = self.get_decoder_batches(encode_outputs, input_col_mask, pad_idx=self.tokenizer_bert.pad_token_id)\n",
    "        \n",
    "        # --- Forward Decoder ---\n",
    "        decoder_outputs = self.model_decoder(\n",
    "            question_padded, \n",
    "            header_padded, \n",
    "            col_padded, \n",
    "            question_lengths, \n",
    "            col_lengths, \n",
    "            value_tkn_max_len, \n",
    "            gold\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "    def get_input_mask_and_answer(self, encode_input: transformers.tokenization_utils_base.BatchEncoding, tokenizer: KoBertTokenizer) -> Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        In this code 'table' means database table name(id), 'header' means database header, 'col' means index of header \n",
    "\n",
    "        Args:\n",
    "            encode_input (transformers.tokenization_utils_base.BatchEncoding): [description]\n",
    "            tokenizer (KoBertTokenizer): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.BoolTensor, torch.BoolTensor, torch.BoolTensor, torch.BoolTensor]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, max_length = encode_input[\"input_ids\"].size()\n",
    "        sep_tkn_mask = encode_input[\"input_ids\"] == tokenizer.sep_token_id\n",
    "        start_tkn_id, end_tkn_id, col_tkn_id = tokenizer.additional_special_tokens_ids\n",
    "\n",
    "        input_question_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 0, encode_input[\"attention_mask\"].bool())\n",
    "        input_question_mask = torch.bitwise_and(input_question_mask, ~sep_tkn_mask) # [SEP] mask out\n",
    "        input_question_mask[:, 0] = False  # [CLS] mask out\n",
    "\n",
    "        db_mask = torch.bitwise_and(encode_input[\"token_type_ids\"] == 1, encode_input[\"attention_mask\"].bool())\n",
    "        db_mask = torch.bitwise_xor(db_mask, sep_tkn_mask)\n",
    "        col_tkn_mask = encode_input[\"input_ids\"] == col_tkn_id\n",
    "        db_mask = torch.bitwise_and(db_mask, ~col_tkn_mask)\n",
    "        # split table_mask and header_mask\n",
    "        input_idx = torch.arange(max_length).repeat(batch_size, 1).to(self.device)\n",
    "        db_idx = input_idx[db_mask]\n",
    "        table_header_tkn_idx = db_idx[db_idx > 0]\n",
    "        table_start_idx = table_header_tkn_idx.view(batch_size, -1)[:, 0] + 1\n",
    "        start_idx = table_header_tkn_idx[1:][table_header_tkn_idx.diff() == 2].view(batch_size, -1)\n",
    "        table_end_sep_idx = start_idx[:, 0] - 1\n",
    "        split_size = torch.stack([\n",
    "            table_end_sep_idx-table_start_idx+1, table_header_tkn_idx.view(batch_size, -1).size(1)-(table_end_sep_idx-table_start_idx+1)\n",
    "        ]).transpose(0, 1)\n",
    "\n",
    "        # Token idx\n",
    "        table_tkn_idx, header_tkn_idx = map(\n",
    "            lambda x: torch.stack(x).to(self.device), \n",
    "            zip(*[torch.split(x, size.tolist()) for x, size in zip(table_header_tkn_idx.view(batch_size, -1), split_size)])\n",
    "        )\n",
    "\n",
    "        table_tkn_idx = table_tkn_idx[:, 1:]\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # If [EXP], `table_tkn_mask` and `header_tkn_mask` should include [S] & [E] tokens\n",
    "        table_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool, device=self.device).scatter(1, table_tkn_idx, True)\n",
    "        header_tkn_mask = torch.zeros_like(encode_input[\"input_ids\"], dtype=torch.bool, device=self.device).scatter(1, header_tkn_idx, True)\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # For Decoder Input, Maskout [S], [E] for table & header -> will be done automatically\n",
    "        input_table_mask = self.get_decoder_input_mask(\n",
    "            encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        )\n",
    "        input_header_mask = self.get_decoder_input_mask(\n",
    "            encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        )\n",
    "\n",
    "        # [COL] token mask: this is for attention\n",
    "        col_tkn_idx = input_idx[col_tkn_mask].view(batch_size, -1)\n",
    "        input_col_mask = torch.zeros_like(encode_input[\"input_ids\"], device=self.device, dtype=torch.bool).scatter(1, col_tkn_idx, True)\n",
    "\n",
    "        # TODO: [EXP] Experiment for generate column directly\n",
    "        # For Answer, Maskout [S] for table & header \n",
    "        # answer_table_tkns = get_answer(\n",
    "        #     encode_input[\"input_ids\"], table_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        # )\n",
    "        # answer_header_tkns = get_answer(\n",
    "        #     encode_input[\"input_ids\"], header_tkn_mask, batch_size, start_tkn_id, end_tkn_id\n",
    "        # )\n",
    "\n",
    "        return input_question_mask, input_table_mask, input_header_mask, input_col_mask # , answer_table_tkns, answer_header_tkns    \n",
    "\n",
    "    ## Masks\n",
    "    # TODO: [EXP] Experiment for generate column directly\n",
    "    # def get_answer(input_ids, mask, batch_size, start_tkn_id, end_tkn_id):\n",
    "    #     r\"\"\"\n",
    "    #     answer should include end token: [E]\n",
    "    #     \"\"\"\n",
    "    #     masked_input_ids = input_ids[mask]\n",
    "    #     start_tkn_mask = masked_input_ids == start_tkn_id\n",
    "    #     end_tkn_mask = masked_input_ids == end_tkn_id\n",
    "    #     table_col_length = masked_input_ids.view(batch_size, -1).size(1)\n",
    "    #     start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "    #     index = torch.arange(table_col_length).repeat(batch_size)[start_end_mask].view(batch_size, -1, 2)\n",
    "    #     tkn_lengths = index[:, :, 1] - index[:, :, 0]\n",
    "    #     answer_col_tkns = [x.split(tkn_length.tolist()) for x, tkn_length in zip(\n",
    "    #         masked_input_ids[~start_tkn_mask].view(batch_size, -1), tkn_lengths)]\n",
    "    #     return answer_col_tkns\n",
    "\n",
    "    def get_decoder_input_mask(self, input_ids: torch.Tensor, mask: torch.BoolTensor, batch_size: int, start_tkn_id: int, end_tkn_id: int) -> torch.BoolTensor:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): [description]\n",
    "            mask (torch.BoolTensor): [description]\n",
    "            batch_size (int): [description]\n",
    "            start_tkn_id (int): [description]\n",
    "            end_tkn_id (int): [description]\n",
    "\n",
    "        Returns:\n",
    "            torch.BoolTensor: [description]\n",
    "        \"\"\"    \n",
    "        start_tkn_mask = input_ids == start_tkn_id\n",
    "        end_tkn_mask = input_ids == end_tkn_id\n",
    "        start_end_mask = torch.bitwise_or(start_tkn_mask, end_tkn_mask)\n",
    "        index = torch.arange(input_ids.size(1)).repeat(batch_size)[start_end_mask.view(-1)].view(batch_size, -1)\n",
    "        return mask.scatter(1, index, False)\n",
    "    \n",
    "    def get_decoder_batches(self, encode_output: transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions, mask: torch.BoolTensor, pad_idx: int) -> Tuple[torch.Tensor, List[int]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            encode_output (transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions): [description]\n",
    "            mask (torch.BoolTensor): [description]\n",
    "            model (BertModel): [description]\n",
    "            pad_idx (int): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, List[int]]: [description]\n",
    "        \"\"\"    \n",
    "        lengths = mask.sum(1)\n",
    "        tensors = encode_output.last_hidden_state[mask, :]\n",
    "        batches = torch.split(tensors, lengths.tolist())\n",
    "        if lengths.ne(lengths.max()).sum().item() != 0:\n",
    "            # pad not same length tokens\n",
    "            tensors_padded = self.pad(batches, lengths.tolist(), pad_idx=pad_idx)\n",
    "        else:\n",
    "            # just stack the splitted tensors\n",
    "            tensors_padded = torch.stack(batches)\n",
    "        return tensors_padded, lengths.tolist()\n",
    "\n",
    "    def pad(self, batches: Tuple[torch.Tensor], lengths: List[int], pad_idx: int=1) -> torch.Tensor:\n",
    "        \"\"\"Pad for decoder inputs\n",
    "\n",
    "        Args:\n",
    "            batches (Tuple[torch.Tensor]): [description]\n",
    "            lengths (List[int]): [description]\n",
    "            model (transformers.models.bert.modeling_bert.BertModel): [description]\n",
    "            pad_idx (int, optional): [description]. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [description]\n",
    "        \"\"\"       \n",
    "        padded = []\n",
    "        max_length = max(lengths)\n",
    "        for x in batches:\n",
    "            if len(x) < max_length:\n",
    "                idxes = self.totensor([pad_idx]*(max_length - len(x)))\n",
    "                pad_tensor = self.model_bert.embeddings.word_embeddings(idxes)\n",
    "                padded.append(torch.cat([x, pad_tensor]))\n",
    "            else:\n",
    "                padded.append(x)\n",
    "        return torch.stack(padded)\n",
    "\n",
    "    def get_sql_answers(self, batch_sqls: List[Dict[str, Any]]):\n",
    "        \"\"\"[summary]\n",
    "        sc: select column\n",
    "        sa: select agg\n",
    "        wn: where number\n",
    "        wc: where column\n",
    "        wo: where operator\n",
    "        wv: where value\n",
    "\n",
    "        Args:\n",
    "            batch_sqls (List[Dict[str, Any]]): [description]\n",
    "            tokenizer (KoBertTokenizer): [description]\n",
    "\n",
    "        Raises:\n",
    "            EnvironmentError: [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        get_ith_element = lambda li, i: [x[i] for x in li]\n",
    "        g_sc = []\n",
    "        g_sa = []\n",
    "        g_wn = []\n",
    "        g_wc = []\n",
    "        g_wo = []\n",
    "        g_wv = []\n",
    "        for b, sql_dict in enumerate(batch_sqls):\n",
    "            g_sc.append( sql_dict[\"sel\"] )\n",
    "            g_sa.append( sql_dict[\"agg\"])\n",
    "\n",
    "            conds = sql_dict[\"conds\"]\n",
    "            if not sql_dict[\"agg\"] < 0:\n",
    "                g_wn.append( len(conds) )\n",
    "                g_wc.append( get_ith_element(conds, 0) )\n",
    "                g_wo.append( get_ith_element(conds, 1) )\n",
    "                g_wv.append( get_ith_element(conds, 2) )\n",
    "            else:\n",
    "                raise EnvironmentError\n",
    "\n",
    "        # get where value tokenized \n",
    "        pad_tkn_id = self.tokenizer_bert.pad_token_id\n",
    "        g_wv_tkns = [[f\"{s}{self.hparams.special_end_tkn}\" for s in batch_wv] for batch_wv in g_wv]\n",
    "        g_wv_tkns = [self.tokenizer_bert(batch_wv, add_special_tokens=False)[\"input_ids\"] for batch_wv in g_wv_tkns]\n",
    "        # add empty list if batch has different where column number\n",
    "        max_where_cols = max([len(batch_wv) for batch_wv in g_wv_tkns])\n",
    "        g_wv_tkns = [batch_wv + [[]]*(max_where_cols-len(batch_wv)) if len(batch_wv) < max_where_cols else batch_wv for batch_wv in g_wv_tkns]\n",
    "        temp = []\n",
    "        for batch_wv in list(zip(*g_wv_tkns)):\n",
    "            batch_max_len = max(map(len, batch_wv))\n",
    "            batch_temp = []\n",
    "            for wv_tkns in batch_wv:  # iter by number of where clause\n",
    "                if len(wv_tkns) < batch_max_len:\n",
    "                    batch_temp.append(wv_tkns + [pad_tkn_id]*(batch_max_len - len(wv_tkns)))\n",
    "                else:\n",
    "                    batch_temp.append(wv_tkns)\n",
    "            temp.append(batch_temp)\n",
    "        g_wv_tkns = list(zip(*temp))\n",
    "        \n",
    "        return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns\n",
    "        \n",
    "        \n",
    "    def calculate_loss(self, decoder_outputs, gold):\n",
    "        \"\"\"\n",
    "        # Outputs Size\n",
    "        sc = (B, T_c)\n",
    "        sa = (B, n_agg_ops)\n",
    "        wn = (B, 5)\n",
    "        wc = (B, T_c): binary\n",
    "        wo = (B, max_where_col_nums, n_cond_ops)\n",
    "        wv = [(B, T_d_i, vocab_size)] x max_where_col_nums / T_d_i = may have different length for answer\n",
    "        \"\"\"\n",
    "        # Loss Calculation\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv_tkns = gold\n",
    "        # g_wv_tkns = list(zip(*g_wv_tkns))  # (B, where_col_num, T_d_i) -> (where_col_num, B, T_d_i)\n",
    "\n",
    "        batch_size = decoder_outputs[\"sc\"].size(0)\n",
    "        loss_sc = self.cross_entropy(decoder_outputs[\"sc\"], self.totensor(g_sc))\n",
    "        loss_sa = self.cross_entropy(decoder_outputs[\"sa\"], self.totensor(g_sa))\n",
    "        loss_wn = self.cross_entropy(decoder_outputs[\"wn\"], self.totensor(g_wn)) * self.hparams.wn_penalty\n",
    "\n",
    "        # need consider: might have different length of where numers\n",
    "        # So when calculate scores looping by where numbers, ignore the out of length tokens\n",
    "        loss_wc = 0\n",
    "        loss_wo = 0\n",
    "        loss_wv = 0\n",
    "        for batch_idx, where_num in enumerate(g_wn):  # iter by batch_size: B\n",
    "\n",
    "            one_hot_dist = torch.zeros_like(decoder_outputs[\"wc\"][batch_idx], device=self.device).scatter(0, self.totensor(g_wc[batch_idx]), 1.0)\n",
    "            loss_wc += self.binary_cross_entropy(decoder_outputs[\"wc\"][batch_idx], one_hot_dist)\n",
    "\n",
    "            batch_g_wo = g_wo[batch_idx]  # (where_num_gold,)\n",
    "            batch_wo = decoder_outputs[\"wo\"][batch_idx, :where_num, :]  # (where_num_predict, n_cond_ops)\n",
    "            if (len(batch_wo) == 0 and where_num != 0):\n",
    "                # if predict nothing where clause and answer is not, what loss should be added?\n",
    "                # simply giving big loss will be enough?\n",
    "                loss_wo += loss_wn * 100\n",
    "            else:\n",
    "                give_wo_penalty = False\n",
    "                if len(batch_wo) > len(batch_g_wo): \n",
    "                    wo_penalty = self.hparams.wo_penalty / 2\n",
    "                    give_wo_penalty = True\n",
    "                    batch_wo = batch_wo[:len(batch_g_wo), :]  # (where_num_predict, n_cond_ops)\n",
    "                elif len(batch_wo) < len(batch_g_wo):\n",
    "                    # giving penalty if not guessed right where numbers\n",
    "                    # It becomes problem when reduce the gold tokens but predicted corrected \n",
    "                    # Then `loss_wo_base` will be 0, if simply multiply by `loss_wv_base` to loss_base will be zero\n",
    "                    wo_penalty = self.hparams.wo_penalty\n",
    "                    give_wo_penalty = True\n",
    "                    batch_g_wo = batch_g_wo[:len(batch_wo)]  # (where_num_gold,)\n",
    "                else:\n",
    "                    wo_penalty = 1.0\n",
    "                    give_wo_penalty = False\n",
    "                loss_wo_base = self.cross_entropy(batch_wo, self.totensor(batch_g_wo))\n",
    "                if give_wo_penalty:\n",
    "                    loss_wo += loss_wo_base + loss_wn * wo_penalty\n",
    "                else:\n",
    "                    loss_wo += loss_wo_base\n",
    "            \n",
    "            batch_g_wv = g_wv_tkns[batch_idx][:where_num]  # (where_num_gold, T_d_i)\n",
    "            batch_wv = [wv[batch_idx] for wv in decoder_outputs[\"wv\"]]  # (where_num_predict, T_d_i, vocab_size)\n",
    "            if len(batch_wo) == 0 and where_num != 0:\n",
    "                # if predict nothing where clause and answer is not, what loss should be added?\n",
    "                loss_wv += loss_wn * 100\n",
    "            else:\n",
    "                for wv, g_wv_i in zip(batch_wv, batch_g_wv):  # will iter by where_num\n",
    "                    give_wv_penalty = False\n",
    "                    if len(wv) > len(g_wv_i):\n",
    "                        wv_penalty = self.hparams.wo_penalty / 2\n",
    "                        give_wv_penalty = True\n",
    "                        wv = wv[:len(g_wv_i), :]  # (T_d_gold, vocab_size)\n",
    "                    elif len(wv) < len(g_wv_i):\n",
    "                        # giving penalty if not generate enough tokens\n",
    "                        # It becomes problem when reduce the gold tokens but predicted corrected \n",
    "                        # Then `loss_wv_base` will be 0, if simply multiply by `loss_wv_base` to loss_base will be zero\n",
    "                        wv_penalty = self.hparams.wo_penalty\n",
    "                        give_wv_penalty = True\n",
    "                        g_wv_i = g_wv_i[:len(wv)]  # (T_d_predict,)\n",
    "                    else:\n",
    "                        wv_penalty = 1.0\n",
    "                        give_wv_penalty = False\n",
    "                    # now have the same T_d size, ignore all over lengthed\n",
    "                    loss_wv_base = self.cross_entropy(wv, self.totensor(g_wv_i))\n",
    "                    self.pp_wv(torch.exp(loss_wv_base))\n",
    "                    if give_wv_penalty:\n",
    "                        loss_wv += loss_wv_base + loss_wn * wv_penalty\n",
    "                    else:\n",
    "                        loss_wv += loss_wv_base\n",
    "        loss = (loss_sc + loss_sa + loss_wn + loss_wc + loss_wo + loss_wv) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def calculate_metrics(self, decoder_outputs, batch_sqls) -> None:\n",
    "        # Predict tokens\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv, g_wv_tkns = self.get_sql_answers(batch_sqls)\n",
    "        predicts = self.predict_to_dict(decoder_outputs)\n",
    "        p_sc, p_sa, p_wn, p_wc, p_wo, p_wv, p_wv_tkns = predicts[\"sc\"], predicts[\"sa\"], predicts[\"wn\"], predicts[\"wc\"], predicts[\"wo\"], predicts[\"wv\"], predicts[\"wv_tkns\"]\n",
    "        \n",
    "        p_wo, g_wo = self.pad_empty_predict_gold(p_wo, g_wo, pad_idx=self.n_cond_ops)  # (B, where_col_num)\n",
    "        \n",
    "        acc_ac = self.acc_sc(*map(self.totensor, [predicts[\"sc\"], g_sc]))\n",
    "        acc_sa = self.acc_sa(*map(self.totensor, [predicts[\"sa\"], g_sa]))\n",
    "        acc_wn = self.acc_wn(*map(self.totensor, [predicts[\"wn\"], g_wn]))\n",
    "        \n",
    "        for batch_idx, where_num in enumerate(g_wn):\n",
    "            batch_g_wo = g_wo[batch_idx]  # (where_num_gold,)\n",
    "            batch_wo = p_wo[batch_idx]  # (where_num_predict,)\n",
    "            acc_wo = self.acc_wo(*map(self.totensor, [predicts[\"wo\"], g_wo]))\n",
    "        \n",
    "    def pad_empty_predict_gold(self, predict, gold, pad_idx):\n",
    "        res = []\n",
    "        for p, g in zip(predict, gold):            \n",
    "            if len(p) < len(g):\n",
    "                p.extend([pad_idx]*(len(g)-len(p)))\n",
    "            elif len(p) > len(g):\n",
    "                g.extend([pad_idx]*(len(p)-len(g)))\n",
    "\n",
    "            res.append([p, g])\n",
    "\n",
    "        return list(zip(*res))\n",
    "    \n",
    "    def get_batch_data(self, data: List[Dict[str, Any]], table: Dict[str, Dict[str, List[Any]]], start_tkn=\"[S]\", end_tkn=\"[E]\") -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict[str, Any]]): [description]\n",
    "            dbengine (DBEngine): [description]\n",
    "            start_tkn (str, optional): [description]. Defaults to \"[S]\".\n",
    "            end_tkn (str, optional): [description]. Defaults to \"[E]\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[Dict[str, Any]]]: [description]\n",
    "        \"\"\"    \n",
    "        batch_qs = [jsonl[\"question\"] for jsonl in data]\n",
    "        tid = [jsonl[\"table_id\"] for jsonl in data]\n",
    "        batch_sqls = [jsonl[\"sql\"] for jsonl in data]\n",
    "        batch_ts = []\n",
    "        for table_id in tid:\n",
    "            table_str = f\"{table_id}\" + \"\".join([\n",
    "                f\"{self.hparams.special_col_tkn}{col}\" for col in table[table_id][\"header\"]\n",
    "            ])\n",
    "            # TODO: [EXP] Experiment for generate column directly\n",
    "            # table_str = f\"{start_tkn}{table_id}{end_tkn}\" + \"\".join([\n",
    "            #     f\"{col_tkn}{start_tkn}{col}{end_tkn}\" for col in dbengine.schema\n",
    "            # ]) \n",
    "            batch_ts.append(table_str)\n",
    "\n",
    "        return batch_qs, batch_ts, batch_sqls\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table, self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=None, \n",
    "            train=True\n",
    "        )\n",
    "        self.calculate_metrics(outputs, batch_sqls)\n",
    "\n",
    "        return  {\"loss\": loss}  \n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "        \n",
    "        acc_sc = self.acc_sc.compute()\n",
    "        acc_sa = self.acc_sa.compute()\n",
    "        acc_wn = self.acc_wn.compute()\n",
    "        acc_wo = self.acc_wo.compute()\n",
    "        pp_wv = self.pp_wv.compute() / len(outputs)\n",
    "        \n",
    "        self.reset_metrics_epoch_end()\n",
    "        return {\"loss\": loss, \"acc_sc\": acc_sc, \"acc_sa\": acc_sa, \"acc_wn\": acc_wn, \"acc_wo\": acc_wo, \"pp_wv\": pp_wv}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table, self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        loss, outputs = self(\n",
    "            batch_qs=batch_qs, \n",
    "            batch_ts=batch_ts, \n",
    "            batch_sqls=batch_sqls, \n",
    "            value_tkn_max_len=self.hparams.value_tkn_max_len, \n",
    "            train=False\n",
    "        )\n",
    "        self.calculate_metrics(outputs, batch_sqls)\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "        \n",
    "        acc_sc = self.acc_sc.compute()\n",
    "        acc_sa = self.acc_sa.compute()\n",
    "        acc_wn = self.acc_wn.compute()\n",
    "        acc_wo = self.acc_wo.compute()\n",
    "        pp_wv = self.pp_wv.compute() / len(outputs)\n",
    "        \n",
    "        self.reset_metrics_epoch_end()\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc_sc\", acc_sc, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc_sa\", acc_sa, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc_wn\", acc_wn, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc_wo\", acc_wo, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_pp_wv\", pp_wv, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def load_data(self, sql_path: Union[Path, str], table_path: Union[Path, str]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        \"\"\"Load data from path\n",
    "\n",
    "        Args:\n",
    "            sql_path (Union[Path, str]): dataset path which contains NL with SQL queries (+answers)\n",
    "            table_path (Union[Path, str]): table information contains table name, header and values\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[Dict[str, Any]], Dict[str, Any]]: [description]\n",
    "        \"\"\"    \n",
    "        path_sql = Path(sql_path)\n",
    "        path_table = Path(table_path)\n",
    "\n",
    "        dataset = []\n",
    "        table = {}\n",
    "        with path_sql.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                x = json.loads(line.strip())\n",
    "                dataset.append(x)\n",
    "\n",
    "        with path_table.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                x = json.loads(line.strip())\n",
    "                table[x['id']] = x\n",
    "\n",
    "        return dataset, table\n",
    "    \n",
    "    def create_dataloader(self, mode):\n",
    "        num_workers = 0 if os.name == \"nt\" else self.hparams.num_workers\n",
    "        if mode == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = self.hparams.train_batch_size\n",
    "            sql_file = self.hparams.train_sql_file\n",
    "            table_file = self.hparams.train_table_file\n",
    "        else:\n",
    "            shuffle = False\n",
    "            batch_size = self.hparams.eval_batch_size\n",
    "            sql_file = self.hparams.eval_sql_file\n",
    "            table_file = self.hparams.eval_table_file\n",
    "        \n",
    "        dataset, self.table = self.load_data(sql_file, table_file)\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            batch_size=batch_size,\n",
    "            dataset=dataset,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=lambda x: x # now dictionary values are not merged!\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(mode=\"eval\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model_decoder.parameters()),\n",
    "                                       lr=self.hparams.lr, weight_decay=0)\n",
    "        opt_bert = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model_bert.parameters()),\n",
    "                                    lr=self.hparams.lr_bert, weight_decay=0)\n",
    "        \n",
    "        optimizers = [opt, opt_bert]\n",
    "        return optimizers\n",
    "    \n",
    "    def predict_to_dict(self, outputs):        \n",
    "        predicts = {}\n",
    "        predicts[\"sc\"] = self.model_decoder.predict_decoder(\"sc\", select_outputs=outputs[\"sc\"])\n",
    "        predicts[\"sa\"] = self.model_decoder.predict_decoder(\"sa\", agg_outputs=outputs[\"sa\"])\n",
    "        predicts[\"wn\"] = self.model_decoder.predict_decoder(\"wn\", where_num_outputs=outputs[\"wn\"])\n",
    "        predicts[\"wc\"] = self.model_decoder.predict_decoder(\"wc\", where_col_outputs=outputs[\"wc\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wo\"] = self.model_decoder.predict_decoder(\"wo\", where_op_outputs=outputs[\"wo\"], where_nums=predicts[\"wn\"])\n",
    "        predicts[\"wv_tkns\"] = self.model_decoder.predict_decoder(\"wv\", where_value_outputs=outputs[\"wv\"])  # (B, value_tkn_max_len) x where_nums\n",
    "        # internally wv means wv_tkns, will convert to string here using tokenizer\n",
    "        predicts[\"wv\"] = []\n",
    "        for where_idx, wv_tkns in enumerate(predicts[\"wv_tkns\"]): # iter: (B, value_tkn_max_len)\n",
    "            predicts[\"wv\"].append([self.tokenizer_bert.decode(self.totensor(batch_wv)) for batch_wv in wv_tkns])\n",
    "                \n",
    "        predicts[\"wv\"] = list(zip(*predicts[\"wv\"]))\n",
    "        \n",
    "        return predicts\n",
    "\n",
    "    def predict_outputs(self, batch):\n",
    "        batch_qs, batch_ts, batch_sqls = self.get_batch_data(batch, self.table, self.hparams.special_start_tkn, self.hparams.special_end_tkn)\n",
    "        outputs = self.forward_outputs(batch_qs, batch_ts, batch_sqls=None, value_tkn_max_len=self.hparams.value_tkn_max_len, train=False)\n",
    "        return self.predict_to_dict(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37771a3d-bb59-474b-aba5-7cccd533babb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PyTorch Ligthtning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27369ff-1d13-45f6-b4fb-8f09fbf66655",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6eddb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    db_path = \"./private/samsung_new.db\",\n",
    "    model_bert_path = \"monologg/kobert\",\n",
    "    # Dataloader\n",
    "    train_sql_file = \"./NLSQL_train_toy.jsonl\",\n",
    "    train_table_file = \"./table_train.jsonl\",\n",
    "    train_batch_size = 5,\n",
    "    eval_sql_file = \"./NLSQL_test_toy.jsonl\",\n",
    "    eval_table_file = \"./table_test.jsonl\",\n",
    "    eval_batch_size = 5,\n",
    "    num_workers = 4,\n",
    "    # Model-decoder\n",
    "    hidden_size = 100,\n",
    "    num_layers = 2,\n",
    "    dropout_ratio = 0.3,\n",
    "    max_where_conds = 4,\n",
    "    value_tkn_max_len = 20, \n",
    "    # Tokenizer\n",
    "    special_start_tkn = \"[S]\", \n",
    "    special_end_tkn = \"[E]\",\n",
    "    special_col_tkn = \"[COL]\",\n",
    "    # Loss Function\n",
    "    wn_penalty = 2.0,  # scale up for guessing where number\n",
    "    wo_penalty = 4.0,\n",
    "    wv_penalty = 5.0,  # giving penalty if not generate enough tokens, if generate more than answer lenght will give 1/2 of it\n",
    "    # Optimizer\n",
    "    num_train = 10,\n",
    "    lr = 1e-3,\n",
    "    lr_bert = 1e-5,\n",
    "    # Seed\n",
    "    seed = 88,\n",
    "    # Records\n",
    "    task = \"TEXT2SQL_v1\",\n",
    "    ckpt_path = \"./ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e463dca3-02d4-446a-b65f-5491db7d813d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 88\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type              | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model_bert           | BertModel         | 92.2 M\n",
      "1 | model_decoder        | Decoder           | 22.5 M\n",
      "2 | cross_entropy        | CrossEntropyLoss  | 0     \n",
      "3 | binary_cross_entropy | BCEWithLogitsLoss | 0     \n",
      "4 | acc_sc               | Accuracy          | 0     \n",
      "5 | acc_sa               | Accuracy          | 0     \n",
      "6 | acc_wn               | Accuracy          | 0     \n",
      "7 | acc_wo               | Accuracy          | 0     \n",
      "8 | pp_wv                | Perplexity        | 0     \n",
      "-----------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "434.292   Total estimated model params size (MB)\n",
      "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeaaa7a6e4840c4ab69cb80b48e2286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=\"epoch{epoch:02d}-{val_loss:.3f}-{val_acc_sc:.3f}-{val_acc_sa:.3f}-{val_acc_wn:.3f}-{val_acc_wo:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "earlystop_callback = pl.callbacks.EarlyStopping(\"val_loss\", mode=\"min\")\n",
    "pl.seed_everything(args_dict[\"seed\"])\n",
    "model = Text2SQL(**args_dict)\n",
    "logger = pl.loggers.TensorBoardLogger(args_dict[\"ckpt_path\"], name=args_dict[\"task\"])\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, earlystop_callback],\n",
    "    default_root_dir=args_dict[\"ckpt_path\"],\n",
    "    max_epochs=args_dict[\"num_train\"],\n",
    "    deterministic=torch.cuda.is_available(),\n",
    "    gpus=-1 if torch.cuda.is_available() else None,\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d134f-a5fa-4aac-93c8-8f0c1edc4d1b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bb28342",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    db_path = \"./data/samsung_new.db\",\n",
    "    model_bert_path = \"monologg/kobert\",\n",
    "    # Dataloader\n",
    "    train_sql_file = \"./data/NLSQL_train.jsonl\",\n",
    "    train_table_file = \"./data/table_train.jsonl\",\n",
    "    train_batch_size = 16,\n",
    "    eval_sql_file = \"./data/NLSQL_test.jsonl\",\n",
    "    eval_table_file = \"./data/table_test.jsonl\",\n",
    "    eval_batch_size = 16,\n",
    "    num_workers = 4,\n",
    "    # Model-decoder\n",
    "    hidden_size = 100,\n",
    "    num_layers = 2,\n",
    "    dropout_ratio = 0.3,\n",
    "    max_where_conds = 4,\n",
    "    value_tkn_max_len = 20, \n",
    "    # Tokenizer\n",
    "    special_start_tkn = \"[S]\", \n",
    "    special_end_tkn = \"[E]\",\n",
    "    special_col_tkn = \"[COL]\",\n",
    "    # Loss Function\n",
    "    wn_penalty = 2.0,  # scale up for guessing where number\n",
    "    wo_penalty = 4.0,\n",
    "    wv_penalty = 5.0,  # giving penalty if not generate enough tokens, if generate more than answer lenght will give 1/2 of it\n",
    "    # Optimizer\n",
    "    num_train = 1,\n",
    "    lr = 1e-3,\n",
    "    lr_bert = 1e-5,\n",
    "    # Seed\n",
    "    seed = 88,\n",
    "    # Records\n",
    "    task = \"TEXT2SQL_v1\",\n",
    "    tensorboard_path = \"./ckpt_tb\",\n",
    "    ckpt_path = \"./ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6dded",
   "metadata": {},
   "source": [
    "## Testing: Execution-guided beam decoding\n",
    "\n",
    "Stil Working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ee7dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373276f",
   "metadata": {},
   "source": [
    "select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bae8b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_output, _ = select_decoder(question_padded, header_padded, col_padded, question_lengths)\n",
    "select_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93b463",
   "metadata": {},
   "source": [
    "construct all possible select + (agg) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c65dce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_size, n_col = select_output.size()\n",
    "\n",
    "select_prob = torch.softmax(select_output, 1)  # prob_sc\n",
    "if n_col < beam_size:\n",
    "    beam_size_max_col = n_col\n",
    "else:\n",
    "    beam_size_max_col = beam_size\n",
    "\n",
    "prob_sc_sa = torch.zeros([batch_size, beam_size_max_col, n_agg_ops])\n",
    "prob_sca = torch.zeros_like(prob_sc_sa)\n",
    "print(prob_sca.size())  # (B, beam-size, n_agg_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2249a1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc top k: [[19, 18, 0, 14], [3, 4, 6, 5]]\n"
     ]
    }
   ],
   "source": [
    "# beamseacrh\n",
    "_, pr_sc_beam = select_output.topk(k=beam_size_max_col)\n",
    "print(f\"sc top k: {pr_sc_beam.tolist()}\")\n",
    "\n",
    "for i_beam in range(beam_size_max_col):\n",
    "    select_idx = pr_sc_beam[:, i_beam].tolist() # pr_sc\n",
    "    agg_output, _ = agg_decoder(question_padded, col_padded, question_lengths, select_idx)\n",
    "    agg_prob = torch.softmax(agg_output, dim=-1)  # prob_sa: (B, n_agg_ops)\n",
    "    prob_sc_sa[:, i_beam, :] = agg_prob\n",
    "    \n",
    "    prob_sc_selected = select_prob[range(batch_size), select_idx]  # (B,)\n",
    "    prob_sca[:, i_beam, :] = (agg_prob.t() * prob_sc_selected).t()  # (n_agg_ops, B) \\odot (1, B) (broadcast) -> (B, max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf53bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1765, 0.1692, 0.1639, 0.1756, 0.1588, 0.1561],\n",
      "         [0.1777, 0.1687, 0.1647, 0.1742, 0.1588, 0.1558],\n",
      "         [0.1764, 0.1690, 0.1643, 0.1751, 0.1593, 0.1558],\n",
      "         [0.1779, 0.1693, 0.1647, 0.1732, 0.1581, 0.1568]],\n",
      "\n",
      "        [[0.1778, 0.1697, 0.1638, 0.1724, 0.1561, 0.1601],\n",
      "         [0.1778, 0.1712, 0.1638, 0.1742, 0.1554, 0.1577],\n",
      "         [0.1789, 0.1702, 0.1634, 0.1741, 0.1557, 0.1578],\n",
      "         [0.1774, 0.1712, 0.1640, 0.1729, 0.1564, 0.1581]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sc_sa.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4ac4c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6])\n",
      "tensor([[[0.0088, 0.0085, 0.0082, 0.0088, 0.0079, 0.0078],\n",
      "         [0.0089, 0.0084, 0.0082, 0.0087, 0.0079, 0.0078],\n",
      "         [0.0088, 0.0084, 0.0082, 0.0088, 0.0080, 0.0078],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0079, 0.0078]],\n",
      "\n",
      "        [[0.0089, 0.0085, 0.0082, 0.0086, 0.0078, 0.0080],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0085, 0.0082, 0.0087, 0.0078, 0.0079],\n",
      "         [0.0089, 0.0086, 0.0082, 0.0086, 0.0078, 0.0079]]])\n"
     ]
    }
   ],
   "source": [
    "print(prob_sca.size())  # (B, beam_size, prob_sc(beam size selected) * prob_agg)\n",
    "print(prob_sca.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8b3e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_multi_dim(tensor, n_topk):\n",
    "    batch_size = tensor.size(0)\n",
    "    values_1d, idxes_1d = tensor.view(batch_size, -1).topk(n_topk)\n",
    "    idxes = np.stack(np.unravel_index(idxes_1d, tensor.size()[1:])).transpose(1, 2, 0)\n",
    "    values = tensor.view(batch_size, -1).gather(1, idxes_1d).numpy()\n",
    "    return idxes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72e1fded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First flatten to 1-d\n",
    "if np.prod(prob_sca.shape[1:]) < beam_size:\n",
    "    beam_size_sca = np.prod(prob_sca.shape[1:])\n",
    "else:\n",
    "    beam_size_sca = beam_size\n",
    "# Now as sc_idx is already sorted, re-map them properly.\n",
    "# idxes: [sc_beam_idx, sa_idx] -> sca_idxes: [sc_idx, sa_idx]\n",
    "idxes, values = topk_multi_dim(prob_sca.detach().cpu(), n_topk=beam_size_sca)\n",
    "sc_beam_idxes = idxes[:, :, 0]\n",
    "sc_idxes = np.stack([pr_sc_beam.numpy()[i, sc_beam_idx] for i, sc_beam_idx in enumerate(sc_beam_idxes)])\n",
    "sca_idxes = np.stack([sc_idxes, idxes[:, :, 1]]).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e29b9fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[14,  0],\n",
       "        [18,  0],\n",
       "        [19,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 6,  0],\n",
       "        [ 3,  0],\n",
       "        [ 4,  0],\n",
       "        [ 5,  0]]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca_idxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fae83e",
   "metadata": {},
   "source": [
    "writing ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1956169",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
