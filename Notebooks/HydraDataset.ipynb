{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e68271",
   "metadata": {},
   "source": [
    "# TEXT2SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fb98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import transformers\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "pretrained_weights = {\n",
    "    (\"bert\", \"base\"): \"bert-base-uncased\",\n",
    "    (\"bert\", \"large\"): \"bert-large-uncased-whole-word-masking\",\n",
    "    (\"roberta\", \"base\"): \"roberta-base\",\n",
    "    (\"roberta\", \"large\"): \"roberta-large\",\n",
    "    (\"albert\", \"xlarge\"): \"albert-xlarge-v2\"\n",
    "}\n",
    "\n",
    "def read_jsonl(jsonl):\n",
    "    for line in open(jsonl, encoding=\"utf8\"):\n",
    "        sample = json.loads(line.rstrip())\n",
    "        yield sample\n",
    "\n",
    "def read_conf(conf_path):\n",
    "    config = {}\n",
    "    for line in open(conf_path, encoding=\"utf8\"):\n",
    "        if line.strip() == \"\" or line[0] == \"#\":\n",
    "             continue\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        config[fields[0]] = fields[1]\n",
    "    config[\"train_data_path\"] =  os.path.abspath(config[\"train_data_path\"])\n",
    "    config[\"dev_data_path\"] =  os.path.abspath(config[\"dev_data_path\"])\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_base_model(config):\n",
    "    weights_name = pretrained_weights[(config[\"base_class\"], config[\"base_name\"])]\n",
    "    if config[\"base_class\"] == \"bert\":\n",
    "        return transformers.BertModel.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"roberta\":\n",
    "        return transformers.RobertaModel.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"albert\":\n",
    "        return transformers.AlbertModel.from_pretrained(weights_name)\n",
    "    else:\n",
    "        raise Exception(\"base_class {0} not supported\".format(config[\"base_class\"]))\n",
    "\n",
    "def create_tokenizer(config):\n",
    "    weights_name = pretrained_weights[(config[\"base_class\"], config[\"base_name\"])]\n",
    "    if config[\"base_class\"] == \"bert\":\n",
    "        return transformers.BertTokenizer.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"roberta\":\n",
    "        return transformers.RobertaTokenizer.from_pretrained(weights_name)\n",
    "    elif config[\"base_class\"] == \"albert\":\n",
    "        return transformers.AlbertTokenizer.from_pretrained(weights_name)\n",
    "    else:\n",
    "        raise Exception(\"base_class {0} not supported\".format(config[\"base_class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e5df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\n\" or c == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(c)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_punctuation(c):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(c)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "        (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(c)\n",
    "    if cat.startswith(\"P\") or cat.startswith(\"S\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def basic_tokenize(doc):\n",
    "    doc_tokens = []\n",
    "    char_to_word = []\n",
    "    word_to_char_start = []\n",
    "    prev_is_whitespace = True\n",
    "    prev_is_punc = False\n",
    "    prev_is_num = False\n",
    "    for pos, c in enumerate(doc):\n",
    "        if is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "            prev_is_punc = False\n",
    "        else:\n",
    "            if prev_is_whitespace or is_punctuation(c) or prev_is_punc or (prev_is_num and not str(c).isnumeric()):\n",
    "                doc_tokens.append(c)\n",
    "                word_to_char_start.append(pos)\n",
    "            else:\n",
    "                doc_tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "            prev_is_punc = is_punctuation(c)\n",
    "            prev_is_num = str(c).isnumeric()\n",
    "        char_to_word.append(len(doc_tokens) - 1)\n",
    "\n",
    "    return doc_tokens, char_to_word, word_to_char_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47060e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(tables):\n",
    "    schema, headers, colTypes, naturalMap = {}, {}, {}, {}\n",
    "    for table in tables:\n",
    "        values = [set() for _ in range(len(table[\"header\"]))]\n",
    "        for row in table[\"rows\"]:\n",
    "            for i, value in enumerate(row):\n",
    "                values[i].add(str(value).lower())\n",
    "        columns = {column: values[i] for i, column in enumerate(table[\"header\"])}\n",
    "\n",
    "        trans = {\"text\": \"string\", \"real\": \"real\", \"integer\": \"integer\"}\n",
    "        colTypes[table[\"id\"]] = {col:trans[ty.lower()] for ty, col in zip(table[\"types\"], table[\"header\"])}\n",
    "        schema[table[\"id\"]] = columns\n",
    "        naturalMap[table[\"id\"]] = {col: col for col in columns}\n",
    "        headers[table[\"id\"]] = table[\"header\"]\n",
    "\n",
    "    return schema, headers, colTypes, naturalMap\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     data_path = os.path.join(\"WikiSQL\", \"data\")\n",
    "#     for phase in [\"train\", \"dev\", \"test\"]:\n",
    "#         src_file = os.path.join(data_path, phase + \".jsonl\")\n",
    "#         schema_file = os.path.join(data_path, phase + \".tables.jsonl\")\n",
    "#         output_file = os.path.join(\"data\", \"wiki\" + phase + \".jsonl\")\n",
    "#         schema, headers, colTypes, naturalMap = get_schema(utils.read_jsonl(schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "160c6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLExample(object):\n",
    "    def __init__(self,\n",
    "                 qid,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 column_meta,\n",
    "                 agg=None,\n",
    "                 select=None,\n",
    "                 conditions=None,\n",
    "                 tokens=None,\n",
    "                 char_to_word=None,\n",
    "                 word_to_char_start=None,\n",
    "                 value_start_end=None,\n",
    "                 valid=True):\n",
    "        self.keys = [\"qid\", \"question\", \"table_id\", \"column_meta\", \"agg\", \"select\", \"conditions\", \"tokens\", \"char_to_word\", \"word_to_char_start\", \"value_start_end\", \"valid\"]\n",
    "        self.qid = qid\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.column_meta = column_meta\n",
    "        self.agg = agg\n",
    "        self.select = select\n",
    "        self.conditions = conditions\n",
    "        self.valid = valid\n",
    "        if tokens is None:\n",
    "            self.tokens, self.char_to_word, self.word_to_char_start = basic_tokenize(question)\n",
    "            self.value_start_end = {}\n",
    "            if conditions is not None and len(conditions) > 0:\n",
    "                cur_start = None\n",
    "                for cond in conditions:\n",
    "                    value = cond[-1]\n",
    "                    value_tokens, _, _ = basic_tokenize(value)\n",
    "                    val_len = len(value_tokens)\n",
    "                    for i in range(len(self.tokens)):\n",
    "                        if \" \".join(self.tokens[i:i+val_len]).lower() != \" \".join(value_tokens).lower():\n",
    "                            continue\n",
    "                        s = self.word_to_char_start[i]\n",
    "                        e = len(question) if i + val_len >= len(self.word_to_char_start) else self.word_to_char_start[i + val_len]\n",
    "                        recovered_answer_text = question[s:e].strip()\n",
    "                        if value.lower() == recovered_answer_text.lower():\n",
    "                            cur_start = i\n",
    "                            break\n",
    "\n",
    "                    if cur_start is None:\n",
    "                        self.valid = False\n",
    "                        # print([value, value_tokens, question, self.tokens])\n",
    "                        # for c in question:\n",
    "                        #     print((c, ord(c), unicodedata.category(c)))\n",
    "                        # raise Exception()\n",
    "                    else:\n",
    "                        self.value_start_end[value] = (cur_start, cur_start + val_len)\n",
    "        else:\n",
    "            self.tokens, self.char_to_word, self.word_to_char_start, self.value_start_end = tokens, char_to_word, word_to_char_start, value_start_end\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for k in self.keys:\n",
    "            s += f\"{k}: \"\n",
    "            s += f\"{self.__dict__[k]}\\n\"\n",
    "        return s\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_json(s):\n",
    "        d = json.loads(s)\n",
    "        keys = [\"qid\", \"question\", \"table_id\", \"column_meta\", \"agg\", \"select\", \"conditions\", \"tokens\", \"char_to_word\", \"word_to_char_start\", \"value_start_end\", \"valid\"]\n",
    "\n",
    "        return SQLExample(*[d[k] for k in keys])\n",
    "\n",
    "    def dump_to_json(self):\n",
    "        d = {}\n",
    "        d[\"qid\"] = self.qid\n",
    "        d[\"question\"] = self.question\n",
    "        d[\"table_id\"] = self.table_id\n",
    "        d[\"column_meta\"] = self.column_meta\n",
    "        d[\"agg\"] = self.agg\n",
    "        d[\"select\"] = self.select\n",
    "        d[\"conditions\"] = self.conditions\n",
    "        d[\"tokens\"] = self.tokens\n",
    "        d[\"char_to_word\"] = self.char_to_word\n",
    "        d[\"word_to_char_start\"] = self.word_to_char_start\n",
    "        d[\"value_start_end\"] = self.value_start_end\n",
    "        d[\"valid\"] = self.valid\n",
    "\n",
    "        return json.dumps(d)\n",
    "\n",
    "    def output_SQ(self, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        agg_text = agg_ops[self.agg]\n",
    "        select_text = self.column_meta[self.select][0]\n",
    "        cond_texts = []\n",
    "        for wc, op, value_text in self.conditions:\n",
    "            column_text = self.column_meta[wc][0]\n",
    "            op_text = cond_ops[op]\n",
    "            cond_texts.append(column_text + op_text + value_text)\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "        return sq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960bae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path().absolute().parent / \"src\" / \"data\"\n",
    "src_file = data_path / \"train\" / \"train.jsonl\"\n",
    "schema_file = data_path / \"train\" / \"train.tables.jsonl\"\n",
    "output_file = data_path / \"train\" / \"hydra_train.jsonl\"\n",
    "schema, headers, colTypes, naturalMap = get_schema(read_jsonl(schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa94598-f8ef-4375-a670-f2e7979f0c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_file.open(\"r\", encoding=\"utf-8\").readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b71bbd-6e2e-484f-b102-0bbe196040ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82865"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((data_path / \"test\" / \"test.jsonl\").open(\"r\", encoding=\"utf-8\").readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9a0ee6f1-b425-4a60-96d2-0557d63a2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikisql\n",
    "data_path = Path().absolute().parent.parent / \"wikisqldata\"\n",
    "src_file = data_path / \"train.jsonl\"\n",
    "schema_file = data_path / \"train.tables.jsonl\"\n",
    "output_file = data_path / \"hydra_train.jsonl\"\n",
    "schema, headers, colTypes, naturalMap = get_schema(read_jsonl(schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3e00d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing C:\\Users\\simon\\Desktop\\Codes\\wikisqldata\\train.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d8f250c2af4e7cae5109415f62ab12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "print(\"processing {0}...\".format(src_file))\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    loader = tqdm(read_jsonl(src_file))\n",
    "    for raw_sample in loader:\n",
    "        table_id = raw_sample[\"table_id\"]\n",
    "        sql = raw_sample[\"sql\"]\n",
    "\n",
    "        cur_schema = schema[table_id]\n",
    "        header = headers[table_id]\n",
    "        cond_col_values = {header[cond[0]]: str(cond[2]) for cond in sql[\"conds\"]}\n",
    "        column_meta = []\n",
    "        for col in header:\n",
    "            if col in cond_col_values:\n",
    "                column_meta.append((col, colTypes[table_id][col], cond_col_values[col]))\n",
    "            else:\n",
    "                detected_val = None\n",
    "                # for cond_col_val in cond_col_values.values():\n",
    "                #     if cond_col_val.lower() in cur_schema[col]:\n",
    "                #         detected_val = cond_col_val\n",
    "                #         break\n",
    "                column_meta.append((col, colTypes[table_id][col], detected_val))\n",
    "    \n",
    "        example = SQLExample(\n",
    "            cnt,\n",
    "            raw_sample[\"question\"],\n",
    "            table_id,\n",
    "            column_meta,\n",
    "            sql[\"agg\"],\n",
    "            int(sql[\"sel\"]),\n",
    "            [(int(cond[0]), cond[1], str(cond[2])) for cond in sql[\"conds\"]])\n",
    "\n",
    "        f.write(example.dump_to_json() + \"\\n\")\n",
    "        cnt += 1\n",
    "        loader.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c06f3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path().absolute().parent / \"src/data/test\"\n",
    "src_file = data_path / \"test.jsonl\"\n",
    "schema_file = data_path / \"test.tables.jsonl\"\n",
    "output_file = data_path / \"hydra_test.jsonl\"\n",
    "schema, headers, colTypes, naturalMap = get_schema(read_jsonl(schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0284a650-8b53-46eb-b8b5-d7f4bab2d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikisql\n",
    "data_path = Path().absolute().parent.parent / \"wikisqldata\"\n",
    "src_file = data_path / \"dev.jsonl\"\n",
    "schema_file = data_path / \"dev.tables.jsonl\"\n",
    "output_file = data_path / \"hydra_dev.jsonl\"\n",
    "schema, headers, colTypes, naturalMap = get_schema(read_jsonl(schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "48abb1e4-3e89-447d-8a43-a5404f929465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing C:\\Users\\simon\\Desktop\\Codes\\wikisqldata\\dev.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe64dda1b6514beaa5438b6c481745f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "print(\"processing {0}...\".format(src_file))\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    loader = tqdm(read_jsonl(src_file))\n",
    "    for raw_sample in loader:\n",
    "        table_id = raw_sample[\"table_id\"]\n",
    "        sql = raw_sample[\"sql\"]\n",
    "\n",
    "        cur_schema = schema[table_id]\n",
    "        header = headers[table_id]\n",
    "        cond_col_values = {header[cond[0]]: str(cond[2]) for cond in sql[\"conds\"]}\n",
    "        column_meta = []\n",
    "        for col in header:\n",
    "            if col in cond_col_values:\n",
    "                column_meta.append((col, colTypes[table_id][col], cond_col_values[col]))\n",
    "            else:\n",
    "                detected_val = None\n",
    "                # for cond_col_val in cond_col_values.values():\n",
    "                #     if cond_col_val.lower() in cur_schema[col]:\n",
    "                #         detected_val = cond_col_val\n",
    "                #         break\n",
    "                column_meta.append((col, colTypes[table_id][col], detected_val))\n",
    "    \n",
    "        example = SQLExample(\n",
    "            cnt,\n",
    "            raw_sample[\"question\"],\n",
    "            table_id,\n",
    "            column_meta,\n",
    "            sql[\"agg\"],\n",
    "            int(sql[\"sel\"]),\n",
    "            [(int(cond[0]), cond[1], str(cond[2])) for cond in sql[\"conds\"]])\n",
    "\n",
    "        f.write(example.dump_to_json() + \"\\n\")\n",
    "        cnt += 1\n",
    "        loader.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610df6e-1b6c-4731-b746-50a2f6567acc",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2da9be24-e3be-4a4a-b33f-1ceaba66972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeature(object):\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 tokens,\n",
    "                 word_to_char_start,\n",
    "                 word_to_subword,\n",
    "                 subword_to_word,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids):\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.tokens = tokens\n",
    "        self.word_to_char_start = word_to_char_start\n",
    "        self.word_to_subword = word_to_subword\n",
    "        self.subword_to_word = subword_to_word\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "        self.columns = None\n",
    "        self.agg = None\n",
    "        self.select = None\n",
    "        self.where_num = None\n",
    "        self.where = None\n",
    "        self.op = None\n",
    "        self.value_start = None\n",
    "        self.value_end = None\n",
    "        \n",
    "        self.keys = [\n",
    "            \"question\", \"table_id\", \"tokens\", \"word_to_char_start\", \"word_to_subword\", \"subword_to_word\", \"input_ids\", \"input_mask\", \"segment_ids\",\n",
    "            \"columns\", \"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"\n",
    "        ]\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for k in self.keys:\n",
    "            s += f\"{k}: \"\n",
    "#             s_res = self.__dict__[k]\n",
    "#             if k == \"tokens\":\n",
    "#                 s_res = [[tkn for tkn in s if tkn != \"[PAD]\"] for s in s_res ]\n",
    "#             if k == \"input_ids\":\n",
    "#                 s_res = [[tkn for tkn in s if tkn != 0] for s in s_res ]\n",
    "            s += f\"{self.__dict__[k]}\\n\"\n",
    "        return s\n",
    "    \n",
    "    def output_SQ(self, agg = None, sel = None, conditions = None, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        if agg is None and sel is None and conditions is None:\n",
    "            sel = np.argmax(self.select)\n",
    "            agg = self.agg[sel]\n",
    "            conditions = []\n",
    "            for i in range(len(self.where)):\n",
    "                if self.where[i] == 0:\n",
    "                    continue\n",
    "                conditions.append((i, self.op[i], self.value_start[i], self.value_end[i]))\n",
    "\n",
    "        agg_text = agg_ops[agg]\n",
    "        select_text = self.columns[sel]\n",
    "        cond_texts = []\n",
    "        for wc, op, vs, ve in conditions:\n",
    "            column_text = self.columns[wc]\n",
    "            op_text = cond_ops[op]\n",
    "            word_start, word_end = self.subword_to_word[wc][vs], self.subword_to_word[wc][ve]\n",
    "            char_start = self.word_to_char_start[word_start]\n",
    "            char_end = len(self.question) if word_end + 1 >= len(self.word_to_char_start) else self.word_to_char_start[word_end + 1]\n",
    "            value_span_text = self.question[char_start:char_end]\n",
    "            cond_texts.append(column_text + op_text + value_span_text.rstrip())\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "\n",
    "        return sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9e8ecf91-35b6-482b-9bcc-4d07ebc7b32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"model_type\":\"pytorch\",\n",
    "\n",
    "    \"SAVE\":1,\n",
    "    \"train_data_path\":\"data/wikitrain.jsonl\",\n",
    "    \"dev_data_path\":\"data/wikidev.jsonl\",\n",
    "    \"test_data_path\":\"data/wikitest.jsonl\",\n",
    "\n",
    "    \"base_class\":\"bert\",\n",
    "    \"base_name\":\"base\",\n",
    "    \"max_total_length\":96,\n",
    "    \"where_column_num\":4,\n",
    "    \"op_num\":4,\n",
    "    \"agg_num\":6,\n",
    "\n",
    "    \"drop_rate\":0.2,\n",
    "    \"learning_rate\":3e-5,\n",
    "    \"decay\":0.01,\n",
    "    \"epochs\":5,\n",
    "    \"batch_size\":256,\n",
    "    \"num_warmup_steps\":400,\n",
    "}\n",
    "model = create_base_model(config)\n",
    "tokenizer = create_tokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf93f1-3e2e-4ef2-b5c9-4df32d5a5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = HydraFeaturizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6832182c-83fd-4781-ae9c-b7e329ed43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path().absolute().parent.parent / \"wikisqldata\"\n",
    "train_data_path = data_path / \"hydra_train.jsonl\"\n",
    "include_label = True\n",
    "model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "if include_label:\n",
    "    for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "        model_inputs[k] = []\n",
    "\n",
    "pos = []\n",
    "input_features = []\n",
    "for line in open(train_data_path, encoding=\"utf8\"):\n",
    "    example = SQLExample.load_from_json(line)\n",
    "    if not example.valid and include_label == True:\n",
    "        continue\n",
    "    input_feature = get_input_feature(example, config)\n",
    "    if include_label:\n",
    "        success = fill_label_feature(example, input_feature, config)\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "    # sq = input_feature.output_SQ()\n",
    "    input_features.append(input_feature)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "54abc049-273b-490b-a292-cd8532913b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid: 0\n",
      "question: Tell me what the notes are for South Australia \n",
      "table_id: 1-1000181-1\n",
      "column_meta: [['State/territory', 'string', None], ['Text/background colour', 'string', None], ['Format', 'string', None], ['Current slogan', 'string', 'SOUTH AUSTRALIA'], ['Current series', 'string', None], ['Notes', 'string', None]]\n",
      "agg: 0\n",
      "select: 5\n",
      "conditions: [[3, 0, 'SOUTH AUSTRALIA']]\n",
      "tokens: ['Tell', 'me', 'what', 'the', 'notes', 'are', 'for', 'South', 'Australia']\n",
      "char_to_word: [0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "word_to_char_start: [0, 5, 8, 13, 17, 23, 27, 31, 37]\n",
      "value_start_end: {'SOUTH AUSTRALIA': [7, 9]}\n",
      "valid: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79473e-1a31-43e6-a9c0-5b49dd8da02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c63280ef-c527-4816-bf82-28257ca6a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_total_length = config[\"max_total_length\"]\n",
    "input_feature = InputFeature(\n",
    "    example.question,\n",
    "    example.table_id,\n",
    "    [],\n",
    "    example.word_to_char_start,\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2e3d9ad0-856a-4b74-a5c0-952b2e71fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me what the notes are for south australia\n",
      "tell me what the notes are for south australia\n",
      "tell me what the notes are for south australia\n",
      "tell me what the notes are for south australia\n",
      "tell me what the notes are for south australia\n",
      "tell me what the notes are for south australia\n"
     ]
    }
   ],
   "source": [
    "for column, col_type, _ in example.column_meta:\n",
    "    # get query tokens\n",
    "    tokens = []\n",
    "    word_to_subword = []\n",
    "    subword_to_word = []\n",
    "    for i, query_token in enumerate(example.tokens):\n",
    "        sub_tokens = tokenizer.tokenize(query_token)\n",
    "        cur_pos = len(tokens)\n",
    "        if len(sub_tokens) > 0:\n",
    "            word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "            tokens.extend(sub_tokens)\n",
    "            subword_to_word.extend([i] * len(sub_tokens))\n",
    "    tokenize_result = tokenizer(\n",
    "        col_type + \" \" + column,\n",
    "        tokens,\n",
    "        max_length=max_total_length,\n",
    "        truncation=\"longest_first\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    input_ids = tokenize_result[\"input_ids\"]\n",
    "    input_mask = tokenize_result[\"attention_mask\"]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    column_token_length = 0\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id == tokenizer.sep_token_id:\n",
    "            column_token_length = i + 1\n",
    "            break\n",
    "    segment_ids = [0] * max_total_length\n",
    "    for i in range(column_token_length, max_total_length):\n",
    "        if input_mask[i] == 0:\n",
    "            break\n",
    "        segment_ids[i] = 1\n",
    "    \n",
    "    print(tokenizer.decode(np.array(input_ids)*np.array(segment_ids), skip_special_tokens=True))\n",
    "    \n",
    "    subword_to_word = [0] * column_token_length + subword_to_word\n",
    "    word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "    \n",
    "    input_feature.tokens.append(tokens)\n",
    "    input_feature.word_to_subword.append(word_to_subword)\n",
    "    input_feature.subword_to_word.append(subword_to_word)\n",
    "    input_feature.input_ids.append(input_ids)\n",
    "    input_feature.input_mask.append(input_mask)\n",
    "    input_feature.segment_ids.append(segment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fda95-c347-4207-9eb0-3c45c9d358df",
   "metadata": {},
   "source": [
    "```\n",
    "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2129: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
    "  warnings.warn(\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a8201-da53-42bc-bed2-bd094caf760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "727511ca-e35d-4dcb-9a2e-3fb1643d7a19",
   "metadata": {},
   "source": [
    "# Korean Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2141af57-26d1-43c9-9f9d-515a19e17f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBertTokenizer import KoBertTokenizer\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import ElectraTokenizer, ElectraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bb4e121b-5d46-4736-a529-484194c2a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(model_type: str, special_tokens=None):\n",
    "    model_path_dict = {\n",
    "        \"kobert\": \"monologg/kobert\",\n",
    "        \"koelectra\": \"monologg/koelectra-base-v3-discriminator\"\n",
    "    }\n",
    "    cls_dict = {\n",
    "        \"kobert\": (KoBertTokenizer, BertModel),\n",
    "        \"koelectra\": (ElectraTokenizer, ElectraModel)\n",
    "    }\n",
    "    model_path = model_path_dict[model_type]\n",
    "    tokenizer_cls, model_cls = cls_dict[model_type]\n",
    "    tokenizer = tokenizer_cls.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    \n",
    "    model = model_cls.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6714febc-ce55-4f6e-ac2f-dabfbf42c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_feature(example: SQLExample, config):\n",
    "    max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "    input_feature = InputFeature(\n",
    "        example.question,\n",
    "        example.table_id,\n",
    "        [],\n",
    "        example.word_to_char_start,\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        []\n",
    "    )\n",
    "\n",
    "    for column, col_type, _ in example.column_meta:\n",
    "        # get query tokens\n",
    "        tokens = []\n",
    "        word_to_subword = []\n",
    "        subword_to_word = []\n",
    "        for column, col_type, _ in example.column_meta:\n",
    "            # get query tokens\n",
    "            tokens = []\n",
    "            word_to_subword = []\n",
    "            subword_to_word = []\n",
    "            for i, query_token in enumerate(example.tokens):\n",
    "                sub_tokens = tokenizer.tokenize(query_token)\n",
    "                cur_pos = len(tokens)\n",
    "                if len(sub_tokens) > 0:\n",
    "                    word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "                    tokens.extend(sub_tokens)\n",
    "                    subword_to_word.extend([i] * len(sub_tokens))\n",
    "            tokenize_result = tokenizer(\n",
    "                col_type + \" \" + column,\n",
    "                tokens,\n",
    "                max_length=max_total_length,\n",
    "                truncation=\"longest_first\",\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            input_ids = tokenize_result[\"input_ids\"]\n",
    "            input_mask = tokenize_result[\"attention_mask\"]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            column_token_length = 0\n",
    "            for i, token_id in enumerate(input_ids):\n",
    "                if token_id == tokenizer.sep_token_id:\n",
    "                    column_token_length = i + 1\n",
    "                    break\n",
    "            segment_ids = [0] * max_total_length\n",
    "            for i in range(column_token_length, max_total_length):\n",
    "                if input_mask[i] == 0:\n",
    "                    break\n",
    "                segment_ids[i] = 1\n",
    "\n",
    "            subword_to_word = [0] * column_token_length + subword_to_word\n",
    "            word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "            \n",
    "            assert len(input_ids) == max_total_length\n",
    "            assert len(input_mask) == max_total_length\n",
    "            assert len(segment_ids) == max_total_length\n",
    "            \n",
    "            input_feature.tokens.append(tokens)\n",
    "            input_feature.word_to_subword.append(word_to_subword)\n",
    "            input_feature.subword_to_word.append(subword_to_word)\n",
    "            input_feature.input_ids.append(input_ids)\n",
    "            input_feature.input_mask.append(input_mask)\n",
    "            input_feature.segment_ids.append(segment_ids)\n",
    "\n",
    "    return input_feature\n",
    "\n",
    "def fill_label_feature(example: SQLExample, input_feature: InputFeature, config):\n",
    "    max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "    columns = [c[0] for c in example.column_meta]\n",
    "    col_num = len(columns)\n",
    "    input_feature.columns = columns\n",
    "\n",
    "    input_feature.agg = [0] * col_num\n",
    "    input_feature.agg[example.select] = example.agg\n",
    "    input_feature.where_num = [len(example.conditions)] * col_num\n",
    "\n",
    "    input_feature.select = [0] * len(columns)\n",
    "    input_feature.select[example.select] = 1\n",
    "\n",
    "    input_feature.where = [0] * len(columns)\n",
    "    input_feature.op = [0] * len(columns)\n",
    "    input_feature.value_start = [0] * len(columns)\n",
    "    input_feature.value_end = [0] * len(columns)\n",
    "\n",
    "    for colidx, op, _ in example.conditions:\n",
    "        input_feature.where[colidx] = 1\n",
    "        input_feature.op[colidx] = op\n",
    "    for colidx, column_meta in enumerate(example.column_meta):\n",
    "        if column_meta[-1] == None:\n",
    "            continue\n",
    "        se = example.value_start_end[column_meta[-1]]\n",
    "        try:\n",
    "            s = input_feature.word_to_subword[colidx][se[0]][0]\n",
    "            input_feature.value_start[colidx] = s\n",
    "            e = input_feature.word_to_subword[colidx][se[1]-1][1]-1\n",
    "            input_feature.value_end[colidx] = e\n",
    "\n",
    "            assert s < max_total_length and input_feature.input_mask[colidx][s] == 1\n",
    "            assert e < max_total_length and input_feature.input_mask[colidx][e] == 1\n",
    "\n",
    "        except:\n",
    "            print(\"value span is out of range\")\n",
    "            return False\n",
    "\n",
    "    # feature_sq = input_feature.output_SQ(return_str=False)\n",
    "    # example_sq = example.output_SQ(return_str=False)\n",
    "    # if feature_sq != example_sq:\n",
    "    #     print(example.qid, feature_sq, example_sq)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0127742c-219b-4963-895e-716226a30ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def get_model_tokenizer(model_type: str, special_tokens=None):\n",
    "    model_path_dict = {\n",
    "        \"kobert\": \"monologg/kobert\",\n",
    "        \"koelectra\": \"monologg/koelectra-base-v3-discriminator\"\n",
    "    }\n",
    "    cls_dict = {\n",
    "        \"kobert\": (KoBertTokenizer, BertModel),\n",
    "        \"koelectra\": (ElectraTokenizer, ElectraModel)\n",
    "    }\n",
    "    model_path = model_path_dict[model_type]\n",
    "    tokenizer_cls, model_cls = cls_dict[model_type]\n",
    "    tokenizer = tokenizer_cls.from_pretrained(model_path, add_special_tokens=True, additional_special_tokens=special_tokens)\n",
    "    \n",
    "    model = model_cls.from_pretrained(model_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_type = \"kobert\"\n",
    "device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "phase = \"train\"\n",
    "data_path = Path().absolute().parent / \"src\" / \"data\"\n",
    "train_data_path = data_path / phase / \"hydra_train.jsonl\"\n",
    "\n",
    "# Tokenizer treats company code as known\n",
    "with (data_path / \"company_codes.txt\").open(\"r\", encoding=\"utf-8\") as file:\n",
    "    company_codes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "model, tokenizer = get_model_tokenizer(model_type=model_type, special_tokens=company_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b9f72d37-470f-44d0-9b8a-31d0b93b2187",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6bf44d5eef4e07b6bf5724d72d759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-251-8596d7c28236>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minclude_label\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0minput_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minclude_label\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_label_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-b5d2b7bdbace>\u001b[0m in \u001b[0;36mget_input_feature\u001b[1;34m(example, config)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0msubword_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_token\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0msub_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[0mcur_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msub_text\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0msub_text\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                         \u001b[0mtokenized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_on_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                         \u001b[0mtokenized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_token\u001b[1;34m(tok, text)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0msplit_on_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m             \u001b[0mtok_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_special_tokens_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0msplit_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "include_label = True\n",
    "model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "if include_label:\n",
    "    for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "        model_inputs[k] = []\n",
    "\n",
    "pos = []\n",
    "input_features = []\n",
    "it = tqdm(open(train_data_path, encoding=\"utf-8\"))\n",
    "for line in it:\n",
    "    example = SQLExample.load_from_json(line)\n",
    "    if not example.valid and include_label == True:\n",
    "        continue\n",
    "    input_feature = get_input_feature(example, config)\n",
    "    if include_label:\n",
    "        success = fill_label_feature(example, input_feature, config)\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "    # sq = input_feature.output_SQ()\n",
    "    input_features.append(input_feature)\n",
    "    \n",
    "    cur_start = len(model_inputs[\"input_ids\"])\n",
    "    cur_sample_num = len(input_feature.input_ids)\n",
    "    pos.append((cur_start, cur_start + cur_sample_num))\n",
    "    \n",
    "    model_inputs[\"input_ids\"].extend(input_feature.input_ids)\n",
    "    model_inputs[\"input_mask\"].extend(input_feature.input_mask)\n",
    "    model_inputs[\"segment_ids\"].extend(input_feature.segment_ids)\n",
    "    if include_label:\n",
    "        model_inputs[\"agg\"].extend(input_feature.agg)\n",
    "        model_inputs[\"select\"].extend(input_feature.select)\n",
    "        model_inputs[\"where_num\"].extend(input_feature.where_num)\n",
    "        model_inputs[\"where\"].extend(input_feature.where)\n",
    "        model_inputs[\"op\"].extend(input_feature.op)\n",
    "        model_inputs[\"value_start\"].extend(input_feature.value_start)\n",
    "        model_inputs[\"value_end\"].extend(input_feature.value_end)\n",
    "    \n",
    "    it.update()\n",
    "    \n",
    "for k in model_inputs:\n",
    "    model_inputs[k] = np.array(model_inputs[k], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "07cde226-98a6-4681-89be-69b15f83f0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InputFeature at 0x1c4ee37bc40>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with (data_path / phase / \"hydra_train_preprocessed.pickle\").open(\"wb\") as file:\n",
    "    pickle.dump(model_inputs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0c5879c1-c3e2-4116-ac35-41ff51aae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydraFeaturizer(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = create_tokenizer(config)\n",
    "        self.colType2token = {\n",
    "            \"string\": \"[unused1]\",\n",
    "            \"real\": \"[unused2]\"}\n",
    "\n",
    "    def get_input_feature(self, example: SQLExample, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        input_feature = InputFeature(\n",
    "            example.question,\n",
    "            example.table_id,\n",
    "            [],\n",
    "            example.word_to_char_start,\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            []\n",
    "        )\n",
    "\n",
    "        for column, col_type, _ in example.column_meta:\n",
    "            # get query tokens\n",
    "            tokens = []\n",
    "            word_to_subword = []\n",
    "            subword_to_word = []\n",
    "            for i, query_token in enumerate(example.tokens):\n",
    "                if self.config[\"base_class\"] == \"roberta\":\n",
    "                    sub_tokens = self.tokenizer.tokenize(query_token, add_prefix_space=True)\n",
    "                else:\n",
    "                    sub_tokens = self.tokenizer.tokenize(query_token)\n",
    "                cur_pos = len(tokens)\n",
    "                if len(sub_tokens) > 0:\n",
    "                    word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "                    tokens.extend(sub_tokens)\n",
    "                    subword_to_word.extend([i] * len(sub_tokens))\n",
    "\n",
    "            if self.config[\"base_class\"] == \"roberta\":\n",
    "                tokenize_result = self.tokenizer.encode_plus(\n",
    "                    col_type + \" \" + column,\n",
    "                    tokens,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_total_length,\n",
    "                    truncation=True,\n",
    "                    add_prefix_space=True\n",
    "                )\n",
    "            else:\n",
    "                tokenize_result = self.tokenizer.encode_plus(\n",
    "                    col_type + \" \" + column,\n",
    "                    tokens,\n",
    "                    max_length=max_total_length,\n",
    "                    truncation_strategy=\"longest_first\",\n",
    "                    pad_to_max_length=True,\n",
    "                )\n",
    "\n",
    "            input_ids = tokenize_result[\"input_ids\"]\n",
    "            input_mask = tokenize_result[\"attention_mask\"]\n",
    "\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            column_token_length = 0\n",
    "            for i, token_id in enumerate(input_ids):\n",
    "                if token_id == self.tokenizer.sep_token_id:\n",
    "                    column_token_length = i + 1\n",
    "                    break\n",
    "            segment_ids = [0] * max_total_length\n",
    "            for i in range(column_token_length, max_total_length):\n",
    "                if input_mask[i] == 0:\n",
    "                    break\n",
    "                segment_ids[i] = 1\n",
    "\n",
    "            subword_to_word = [0] * column_token_length + subword_to_word\n",
    "            word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "\n",
    "            assert len(input_ids) == max_total_length\n",
    "            assert len(input_mask) == max_total_length\n",
    "            assert len(segment_ids) == max_total_length\n",
    "\n",
    "            input_feature.tokens.append(tokens)\n",
    "            input_feature.word_to_subword.append(word_to_subword)\n",
    "            input_feature.subword_to_word.append(subword_to_word)\n",
    "            input_feature.input_ids.append(input_ids)\n",
    "            input_feature.input_mask.append(input_mask)\n",
    "            input_feature.segment_ids.append(segment_ids)\n",
    "\n",
    "        return input_feature\n",
    "\n",
    "    def fill_label_feature(self, example: SQLExample, input_feature: InputFeature, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        columns = [c[0] for c in example.column_meta]\n",
    "        col_num = len(columns)\n",
    "        input_feature.columns = columns\n",
    "\n",
    "        input_feature.agg = [0] * col_num\n",
    "        input_feature.agg[example.select] = example.agg\n",
    "        input_feature.where_num = [len(example.conditions)] * col_num\n",
    "\n",
    "        input_feature.select = [0] * len(columns)\n",
    "        input_feature.select[example.select] = 1\n",
    "\n",
    "        input_feature.where = [0] * len(columns)\n",
    "        input_feature.op = [0] * len(columns)\n",
    "        input_feature.value_start = [0] * len(columns)\n",
    "        input_feature.value_end = [0] * len(columns)\n",
    "\n",
    "        for colidx, op, _ in example.conditions:\n",
    "            input_feature.where[colidx] = 1\n",
    "            input_feature.op[colidx] = op\n",
    "        for colidx, column_meta in enumerate(example.column_meta):\n",
    "            if column_meta[-1] == None:\n",
    "                continue\n",
    "            se = example.value_start_end[column_meta[-1]]\n",
    "            try:\n",
    "                s = input_feature.word_to_subword[colidx][se[0]][0]\n",
    "                input_feature.value_start[colidx] = s\n",
    "                e = input_feature.word_to_subword[colidx][se[1]-1][1]-1\n",
    "                input_feature.value_end[colidx] = e\n",
    "\n",
    "                assert s < max_total_length and input_feature.input_mask[colidx][s] == 1\n",
    "                assert e < max_total_length and input_feature.input_mask[colidx][e] == 1\n",
    "\n",
    "            except:\n",
    "                print(\"value span is out of range\")\n",
    "                return False\n",
    "\n",
    "        # feature_sq = input_feature.output_SQ(return_str=False)\n",
    "        # example_sq = example.output_SQ(return_str=False)\n",
    "        # if feature_sq != example_sq:\n",
    "        #     print(example.qid, feature_sq, example_sq)\n",
    "        return True\n",
    "\n",
    "    def load_data(self, data_paths, config, include_label=False):\n",
    "        model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "        if include_label:\n",
    "            for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "                model_inputs[k] = []\n",
    "\n",
    "        pos = []\n",
    "        input_features = []\n",
    "        for data_path in data_paths.split(\"|\"):\n",
    "            cnt = 0\n",
    "            print(data_path)\n",
    "            for line in tqdm(open(data_path, encoding=\"utf8\")):\n",
    "                example = SQLExample.load_from_json(line)\n",
    "                if not example.valid and include_label == True:\n",
    "                    continue\n",
    "\n",
    "                input_feature = self.get_input_feature(example, config)\n",
    "                if include_label:\n",
    "                    success = self.fill_label_feature(example, input_feature, config)\n",
    "                    if not success:\n",
    "                        continue\n",
    "\n",
    "                # sq = input_feature.output_SQ()\n",
    "                input_features.append(input_feature)\n",
    "\n",
    "                cur_start = len(model_inputs[\"input_ids\"])\n",
    "                cur_sample_num = len(input_feature.input_ids)\n",
    "                pos.append((cur_start, cur_start + cur_sample_num))\n",
    "\n",
    "                model_inputs[\"input_ids\"].extend(input_feature.input_ids)\n",
    "                model_inputs[\"input_mask\"].extend(input_feature.input_mask)\n",
    "                model_inputs[\"segment_ids\"].extend(input_feature.segment_ids)\n",
    "                if include_label:\n",
    "                    model_inputs[\"agg\"].extend(input_feature.agg)\n",
    "                    model_inputs[\"select\"].extend(input_feature.select)\n",
    "                    model_inputs[\"where_num\"].extend(input_feature.where_num)\n",
    "                    model_inputs[\"where\"].extend(input_feature.where)\n",
    "                    model_inputs[\"op\"].extend(input_feature.op)\n",
    "                    model_inputs[\"value_start\"].extend(input_feature.value_start)\n",
    "                    model_inputs[\"value_end\"].extend(input_feature.value_end)\n",
    "\n",
    "                cnt += 1\n",
    "                if cnt % 5000 == 0:\n",
    "                    print(cnt)\n",
    "\n",
    "                if \"DEBUG\" in config and cnt > 100:\n",
    "                    break\n",
    "\n",
    "        for k in model_inputs:\n",
    "            model_inputs[k] = np.array(model_inputs[k], dtype=np.int64)\n",
    "\n",
    "        return input_features, model_inputs, pos\n",
    "\n",
    "class SQLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_paths, config, featurizer, include_label=False):\n",
    "        self.config = config\n",
    "        self.featurizer = featurizer\n",
    "        self.input_features, self.model_inputs, self.pos = self.featurizer.load_data(data_paths, config, include_label)\n",
    "\n",
    "        print(\"{0} loaded. Data shapes:\".format(data_paths))\n",
    "        for k, v in self.model_inputs.items():\n",
    "            print(k, v.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.model_inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5169581a-d538-4526-9467-7193a7674c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = Path().absolute().parent.parent / \"wikisqldata\"\n",
    "train_data_path = data_path / \"hydra_train.jsonl\"\n",
    "config = {\n",
    "    \"model_type\":\"pytorch\",\n",
    "\n",
    "    \"SAVE\":1,\n",
    "    \"train_data_path\": str(data_path / \"hydra_train.jsonl\"),\n",
    "    \"dev_data_path\": str(data_path / \"hydra_dev.jsonl\"),\n",
    "    \"test_data_path\": str(data_path / \"wikitest.jsonl\"),\n",
    "\n",
    "    \"base_class\":\"bert\",\n",
    "    \"base_name\":\"base\",\n",
    "    \"max_total_length\":96,\n",
    "    \"where_column_num\":4,\n",
    "    \"op_num\":4,\n",
    "    \"agg_num\":6,\n",
    "\n",
    "    \"drop_rate\":0.2,\n",
    "    \"learning_rate\":3e-5,\n",
    "    \"decay\":0.01,\n",
    "    \"epochs\":5,\n",
    "    \"batch_size\":256,\n",
    "    \"num_warmup_steps\":400,\n",
    "}\n",
    "# model = create_base_model(config)\n",
    "# tokenizer = create_tokenizer(config)\n",
    "featurizer = HydraFeaturizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "17b90266-03a0-454f-9f5e-858dcaaffccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Desktop\\Codes\\wikisqldata\\hydra_train.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2024bc2ba944368064646fe842b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "value span is out of range\n",
      "value span is out of range\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n"
     ]
    }
   ],
   "source": [
    "input_features, model_inputs, pos = featurizer.load_data(config[\"train_data_path\"], config, include_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "545e691d-e277-43cb-acfe-b83094eaa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig(object):\n",
    "    def __init__(self, config):\n",
    "        model_path_dict = {\n",
    "            \"kobert\": \"monologg/kobert\",\n",
    "            \"koelectra\": \"monologg/koelectra-base-v3-discriminator\"\n",
    "        }\n",
    "        cls_dict = {\n",
    "            \"kobert\": (KoBertTokenizer, BertModel),\n",
    "            \"koelectra\": (ElectraTokenizer, ElectraModel)\n",
    "        }\n",
    "        self.model_type = config[\"model_type\"]\n",
    "        self.special_tkns_path = config[\"special_tkns_path\"]\n",
    "        self.model_path = model_path_dict[self.model_type]\n",
    "        self.tokenizer_cls, self.model_cls = cls_dict[self.model_type]\n",
    "        \n",
    "        \n",
    "    def get_transfomers_model(self, tokenizer):\n",
    "        model = self.model_cls.from_pretrained(self.model_path)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        return model\n",
    "\n",
    "    def get_transfomers_tokenizer(self):\n",
    "        special_tkns = self.get_special_tokens()\n",
    "        tokenizer = self.tokenizer_cls.from_pretrained(\n",
    "            self.model_path, \n",
    "            add_special_tokens=True, \n",
    "            additional_special_tokens=special_tkns\n",
    "        )\n",
    "        return tokenizer\n",
    "    \n",
    "    def get_special_tokens(self):\n",
    "        with open(self.special_tkns_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "            special_tkns = [line.strip() for line in file.readlines()]\n",
    "        \n",
    "        special_tkns += [\"[STRING]\", \"[REAL]\", \"[INT]\"]\n",
    "        return special_tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8fb760dd-73db-41f5-9a88-deea5eb7a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeature(object):\n",
    "    def __init__(self,\n",
    "                 qid,\n",
    "                 question,\n",
    "                 table_id,\n",
    "                 tokens,\n",
    "                 word_to_char_start,\n",
    "                 word_to_subword,\n",
    "                 subword_to_word,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids):\n",
    "        self.qid = qid\n",
    "        self.question = question\n",
    "        self.table_id = table_id\n",
    "        self.tokens = tokens\n",
    "        self.word_to_char_start = word_to_char_start\n",
    "        self.word_to_subword = word_to_subword\n",
    "        self.subword_to_word = subword_to_word\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "        self.columns = None\n",
    "        self.agg = None\n",
    "        self.select = None\n",
    "        self.where_num = None\n",
    "        self.where = None\n",
    "        self.op = None\n",
    "        self.value_start = None\n",
    "        self.value_end = None\n",
    "        \n",
    "        self.keys = [\n",
    "            \"question\", \"table_id\", \"tokens\", \"word_to_char_start\", \"word_to_subword\", \"subword_to_word\", \"input_ids\", \"input_mask\", \"segment_ids\",\n",
    "            \"columns\", \"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"\n",
    "        ]\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for k in self.keys:\n",
    "            s += f\"{k}: \"\n",
    "#             s_res = self.__dict__[k]\n",
    "#             if k == \"tokens\":\n",
    "#                 s_res = [[tkn for tkn in s if tkn != \"[PAD]\"] for s in s_res ]\n",
    "#             if k == \"input_ids\":\n",
    "#                 s_res = [[tkn for tkn in s if tkn != 0] for s in s_res ]\n",
    "            s += f\"{self.__dict__[k]}\\n\"\n",
    "        return s\n",
    "    \n",
    "    def output_SQ(self, agg = None, sel = None, conditions = None, return_str=True):\n",
    "        agg_ops = ['NA', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "        cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "        if agg is None and sel is None and conditions is None:\n",
    "            sel = np.argmax(self.select)\n",
    "            agg = self.agg[sel]\n",
    "            conditions = []\n",
    "            for i in range(len(self.where)):\n",
    "                if self.where[i] == 0:\n",
    "                    continue\n",
    "                conditions.append((i, self.op[i], self.value_start[i], self.value_end[i]))\n",
    "\n",
    "        agg_text = agg_ops[agg]\n",
    "        select_text = self.columns[sel]\n",
    "        cond_texts = []\n",
    "        for wc, op, vs, ve in conditions:\n",
    "            column_text = self.columns[wc]\n",
    "            op_text = cond_ops[op]\n",
    "            word_start, word_end = self.subword_to_word[wc][vs], self.subword_to_word[wc][ve]\n",
    "            char_start = self.word_to_char_start[word_start]\n",
    "            char_end = len(self.question) if word_end + 1 >= len(self.word_to_char_start) else self.word_to_char_start[word_end + 1]\n",
    "            value_span_text = self.question[char_start:char_end]\n",
    "            cond_texts.append(column_text + op_text + value_span_text.rstrip())\n",
    "\n",
    "        if return_str:\n",
    "            sq = agg_text + \", \" + select_text + \", \" + \" AND \".join(cond_texts)\n",
    "        else:\n",
    "            sq = (agg_text, select_text, set(cond_texts))\n",
    "\n",
    "        return sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "37e0cfd3-65d2-4807-ae7b-c30b58987575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydraFeaturizer(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = TransformerConfig(config).get_transfomers_tokenizer()\n",
    "        self.colType2token = {\n",
    "            \"string\": \"[STRING]\",\n",
    "            \"real\": \"[REAL]\",\n",
    "            \"integer\": \"[INT]\" \n",
    "        }\n",
    "\n",
    "    def get_input_feature(self, example: SQLExample, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        input_feature = InputFeature(\n",
    "            example.qid,\n",
    "            example.question,\n",
    "            example.table_id,\n",
    "            [],\n",
    "            example.word_to_char_start,\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            []\n",
    "        )\n",
    "\n",
    "        for column, col_type, _ in example.column_meta:\n",
    "            # get query tokens\n",
    "            tokens = []\n",
    "            word_to_subword = []\n",
    "            subword_to_word = []\n",
    "            for i, query_token in enumerate(example.tokens):\n",
    "                sub_tokens = self.tokenizer.tokenize(query_token)\n",
    "                cur_pos = len(tokens)\n",
    "                if len(sub_tokens) > 0:\n",
    "                    word_to_subword += [(cur_pos, cur_pos + len(sub_tokens))]\n",
    "                    tokens.extend(sub_tokens)\n",
    "                    subword_to_word.extend([i] * len(sub_tokens))\n",
    "            \n",
    "            tokenize_result = self.tokenizer(\n",
    "                self.colType2token[col_type] + \" \" + column,\n",
    "                tokens,\n",
    "                max_length=max_total_length,\n",
    "                truncation=\"longest_first\",\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            input_ids = tokenize_result[\"input_ids\"]\n",
    "            input_mask = tokenize_result[\"attention_mask\"]\n",
    "\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            column_token_length = 0\n",
    "            for i, token_id in enumerate(input_ids):\n",
    "                if token_id == self.tokenizer.sep_token_id:\n",
    "                    column_token_length = i + 1\n",
    "                    break\n",
    "            segment_ids = [0] * max_total_length\n",
    "            for i in range(column_token_length, max_total_length):\n",
    "                if input_mask[i] == 0:\n",
    "                    break\n",
    "                segment_ids[i] = 1\n",
    "\n",
    "            subword_to_word = [0] * column_token_length + subword_to_word\n",
    "            word_to_subword = [(pos[0]+column_token_length, pos[1]+column_token_length) for pos in word_to_subword]\n",
    "\n",
    "            assert len(input_ids) == max_total_length\n",
    "            assert len(input_mask) == max_total_length\n",
    "            assert len(segment_ids) == max_total_length\n",
    "\n",
    "            input_feature.tokens.append(tokens)\n",
    "            input_feature.word_to_subword.append(word_to_subword)\n",
    "            input_feature.subword_to_word.append(subword_to_word)\n",
    "            input_feature.input_ids.append(input_ids)\n",
    "            input_feature.input_mask.append(input_mask)\n",
    "            input_feature.segment_ids.append(segment_ids)\n",
    "\n",
    "        return input_feature\n",
    "\n",
    "    def fill_label_feature(self, example: SQLExample, input_feature: InputFeature, config):\n",
    "        max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "        columns = [c[0] for c in example.column_meta]\n",
    "        col_num = len(columns)\n",
    "        input_feature.columns = columns\n",
    "\n",
    "        input_feature.agg = [0] * col_num\n",
    "        input_feature.agg[example.select] = example.agg\n",
    "        input_feature.where_num = [len(example.conditions)] * col_num\n",
    "\n",
    "        input_feature.select = [0] * len(columns)\n",
    "        input_feature.select[example.select] = 1\n",
    "\n",
    "        input_feature.where = [0] * len(columns)\n",
    "        input_feature.op = [0] * len(columns)\n",
    "        input_feature.value_start = [0] * len(columns)\n",
    "        input_feature.value_end = [0] * len(columns)\n",
    "\n",
    "        for colidx, op, _ in example.conditions:\n",
    "            input_feature.where[colidx] = 1\n",
    "            input_feature.op[colidx] = op\n",
    "        for colidx, column_meta in enumerate(example.column_meta):\n",
    "            if column_meta[-1] == None:\n",
    "                continue\n",
    "            se = example.value_start_end[column_meta[-1]]\n",
    "            try:\n",
    "                s = input_feature.word_to_subword[colidx][se[0]][0]\n",
    "                input_feature.value_start[colidx] = s\n",
    "                e = input_feature.word_to_subword[colidx][se[1]-1][1]-1\n",
    "                input_feature.value_end[colidx] = e\n",
    "\n",
    "                assert s < max_total_length and input_feature.input_mask[colidx][s] == 1\n",
    "                assert e < max_total_length and input_feature.input_mask[colidx][e] == 1\n",
    "\n",
    "            except:\n",
    "                print(\"value span is out of range\")\n",
    "                return False\n",
    "        \n",
    "        \n",
    "        feature_sq = input_feature.output_SQ(return_str=False)\n",
    "        example_sq = example.output_SQ(return_str=False)\n",
    "        if feature_sq != example_sq:\n",
    "            print(example.qid, feature_sq, example_sq)\n",
    "        return True\n",
    "\n",
    "    def load_data(self, data_paths, config, include_label=False):\n",
    "        model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "        if include_label:\n",
    "            for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "                model_inputs[k] = []\n",
    "\n",
    "        pos = []\n",
    "        input_features = []\n",
    "        for data_path in data_paths.split(\"|\"):\n",
    "            cnt = 0\n",
    "            print(data_path)\n",
    "            for line in tqdm(open(data_path, encoding=\"utf8\")):\n",
    "                example = SQLExample.load_from_json(line)\n",
    "                if not example.valid and include_label == True:\n",
    "                    continue\n",
    "\n",
    "                input_feature = self.get_input_feature(example, config)\n",
    "                if include_label:\n",
    "                    success = self.fill_label_feature(example, input_feature, config)\n",
    "                    if not success:\n",
    "                        continue\n",
    "\n",
    "                # sq = input_feature.output_SQ()\n",
    "                input_features.append(input_feature)\n",
    "\n",
    "                cur_start = len(model_inputs[\"input_ids\"])\n",
    "                cur_sample_num = len(input_feature.input_ids)\n",
    "                pos.append((cur_start, cur_start + cur_sample_num))\n",
    "\n",
    "                model_inputs[\"input_ids\"].extend(input_feature.input_ids)\n",
    "                model_inputs[\"input_mask\"].extend(input_feature.input_mask)\n",
    "                model_inputs[\"segment_ids\"].extend(input_feature.segment_ids)\n",
    "                if include_label:\n",
    "                    model_inputs[\"agg\"].extend(input_feature.agg)\n",
    "                    model_inputs[\"select\"].extend(input_feature.select)\n",
    "                    model_inputs[\"where_num\"].extend(input_feature.where_num)\n",
    "                    model_inputs[\"where\"].extend(input_feature.where)\n",
    "                    model_inputs[\"op\"].extend(input_feature.op)\n",
    "                    model_inputs[\"value_start\"].extend(input_feature.value_start)\n",
    "                    model_inputs[\"value_end\"].extend(input_feature.value_end)\n",
    "\n",
    "                cnt += 1\n",
    "                if cnt % 5000 == 0:\n",
    "                    print(cnt)\n",
    "                \n",
    "                if cnt > 100:\n",
    "                    break\n",
    "                \n",
    "        for k in model_inputs:\n",
    "            model_inputs[k] = np.array(model_inputs[k], dtype=np.int64)\n",
    "\n",
    "        return input_features, model_inputs, pos\n",
    "\n",
    "class SQLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_paths, config, featurizer, include_label=False):\n",
    "        self.config = config\n",
    "        self.featurizer = featurizer\n",
    "        self.input_features, self.model_inputs, self.pos = self.featurizer.load_data(data_paths, config, include_label)\n",
    "\n",
    "        print(\"{0} loaded. Data shapes:\".format(data_paths))\n",
    "        for k, v in self.model_inputs.items():\n",
    "            print(k, v.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.model_inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.model_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b055176a-3229-4e9d-9f59-c9f6d28efab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Desktop\\Codes\\Text2SQL\\src\\data\\train\\hydra_train.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad790d72987147f4bec270ea330cc5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "65 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "67 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'bsns_year=2019', 'account_nm='})\n",
      "69 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'bsns_year=2019', 'account_nm='})\n",
      "71 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'bsns_year=2020', 'account_nm='})\n",
      "73 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'bsns_year=2020', 'account_nm='})\n",
      "75 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "77 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "79 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "81 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "83 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "85 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "87 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "89 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "91 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'bsns_year=2019', 'account_nm='})\n",
      "93 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'bsns_year=2019', 'account_nm='})\n",
      "95 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'bsns_year=2020', 'account_nm='})\n",
      "97 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'bsns_year=2020', 'account_nm='})\n",
      "99 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "101 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "103 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "105 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "107 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "109 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "113 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "115 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "117 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "119 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "121 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "123 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "125 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "127 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "129 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "131 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "133 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "135 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "137 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "139 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "141 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "143 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "145 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "147 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "149 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "151 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "153 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "155 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "157 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "159 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "163 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "165 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "167 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "169 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "171 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "173 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n",
      "175 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "177 ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='}) ('NA', 'thstrm_amount', {'thstrm_nm= 58 ', 'account_nm='})\n",
      "179 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "181 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 59 '})\n",
      "183 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "185 ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '}) ('NA', 'thstrm_amount', {'account_nm=', 'thstrm_nm= 60 '})\n",
      "187 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "189 ('NA', 'thstrm_amount', {'account_nm=2018', 'bsns_year=2018'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2018'})\n",
      "191 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "193 ('NA', 'thstrm_amount', {'account_nm=2019', 'bsns_year=2019'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2019'})\n",
      "195 ('NA', 'thstrm_amount', {'account_nm=2020', 'bsns_year=2020'}) ('NA', 'thstrm_amount', {'account_nm=', 'bsns_year=2020'})\n"
     ]
    }
   ],
   "source": [
    "phase = \"train\"\n",
    "\n",
    "data_path = Path().absolute().parent / \"src\" / \"data\"\n",
    "config = {\n",
    "    \"model_type\": \"kobert\",\n",
    "    \n",
    "    \"SAVE\": 1,\n",
    "    \"train_data_path\": str(data_path / \"train\" / \"hydra_train.jsonl\"),\n",
    "    # \"dev_data_path\": str(data_path / \"hydra_dev.jsonl\"),\n",
    "    \"test_data_path\": str(data_path  / \"test\" / \"hydra_test.jsonl\"),\n",
    "    \"special_tkns_path\": str(data_path / \"company_codes.txt\"),\n",
    "    \n",
    "    \"max_total_length\": 96,\n",
    "    \"where_column_num\": 4,\n",
    "    \"op_num\": 4,\n",
    "    \"agg_num\": 6,\n",
    "\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"decay\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_warmup_steps\": 400,\n",
    "}\n",
    "featurizer = HydraFeaturizer(config)\n",
    "input_features, model_inputs, pos = featurizer.load_data(config[\"train_data_path\"], config, include_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "42466f0d-d623-41bb-9adc-3dab0a002444",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = config[\"train_data_path\"]\n",
    "include_label=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "67aba1f6-850f-4c3d-a2f4-c4c0fddd0eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Desktop\\Codes\\Text2SQL\\src\\data\\train\\hydra_train.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a257a850b8af479ab7c4006ce43b6478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_inputs = {k: [] for k in [\"input_ids\", \"input_mask\", \"segment_ids\"]}\n",
    "if include_label:\n",
    "    for k in [\"agg\", \"select\", \"where_num\", \"where\", \"op\", \"value_start\", \"value_end\"]:\n",
    "        model_inputs[k] = []\n",
    "\n",
    "pos = []\n",
    "input_features = []\n",
    "for data_path in data_paths.split(\"|\"):\n",
    "    cnt = 0\n",
    "    print(data_path)\n",
    "    for line in tqdm(open(data_path, encoding=\"utf8\")):\n",
    "        example = SQLExample.load_from_json(line)\n",
    "        if not example.valid and include_label == True:\n",
    "            continue\n",
    "\n",
    "        input_feature = featurizer.get_input_feature(example, config)\n",
    "        if example.qid == 63:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "292c5b25-874e-4d94-993c-242ac6b8d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid',\n",
       " 'question',\n",
       " 'table_id',\n",
       " 'column_meta',\n",
       " 'agg',\n",
       " 'select',\n",
       " 'conditions',\n",
       " 'tokens',\n",
       " 'char_to_word',\n",
       " 'word_to_char_start',\n",
       " 'value_start_end',\n",
       " 'valid']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "dafff4b1-71d1-48e9-9f6d-ec61d7cc8504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000040 2018  ?'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "43f2e76b-2fac-4d88-818b-4bf678957812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0, '2018'], [10, 0, '']]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae3c24-459d-4eaa-9ac5-7e596dca997f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f3509afc-8688-4459-b35a-0a7ba983979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2018': [2, 3], '': [2, 3]}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.value_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1a81a2e7-b1d8-46a5-bc7e-fe7aed54bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_total_length = int(config[\"max_total_length\"])\n",
    "\n",
    "columns = [c[0] for c in example.column_meta]\n",
    "col_num = len(columns)\n",
    "input_feature.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bf692446-e259-4cef-8297-a64f5b1f8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287fef6-72ef-4aaa-8b0c-a486b52adc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_feature.agg = [0] * col_num\n",
    "input_feature.agg[example.select] = example.agg\n",
    "input_feature.where_num = [len(example.conditions)] * col_num\n",
    "\n",
    "input_feature.select = [0] * len(columns)\n",
    "input_feature.select[example.select] = 1\n",
    "\n",
    "input_feature.where = [0] * len(columns)\n",
    "input_feature.op = [0] * len(columns)\n",
    "input_feature.value_start = [0] * len(columns)\n",
    "input_feature.value_end = [0] * len(columns)\n",
    "\n",
    "for colidx, op, _ in example.conditions:\n",
    "    input_feature.where[colidx] = 1\n",
    "    input_feature.op[colidx] = op\n",
    "for colidx, column_meta in enumerate(example.column_meta):\n",
    "    if column_meta[-1] == None:\n",
    "        continue\n",
    "    se = example.value_start_end[column_meta[-1]]\n",
    "    try:\n",
    "        s = input_feature.word_to_subword[colidx][se[0]][0]\n",
    "        input_feature.value_start[colidx] = s\n",
    "        e = input_feature.word_to_subword[colidx][se[1]-1][1]-1\n",
    "        input_feature.value_end[colidx] = e\n",
    "\n",
    "        assert s < max_total_length and input_feature.input_mask[colidx][s] == 1\n",
    "        assert e < max_total_length and input_feature.input_mask[colidx][e] == 1\n",
    "\n",
    "    except:\n",
    "        print(\"value span is out of range\")\n",
    "        return False\n",
    "\n",
    "\n",
    "feature_sq = input_feature.output_SQ(return_str=False)\n",
    "example_sq = example.output_SQ(return_str=False)\n",
    "if feature_sq != example_sq:\n",
    "    print(example.qid, feature_sq, example_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b4e48-cc82-4e8f-aff4-a95028df3d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
